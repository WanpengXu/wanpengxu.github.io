<!DOCTYPE html>
<html lang="default">

<head>
    <meta charset="UTF-8">
<meta name="viewport"
      content="width=device-width, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">

    <meta name="baidu-site-verification" content="code-dlFXc2180h" />


    <meta name="google-site-verification" content="qlTU_n7RT-JIFfLGMsgqxNsTEkVlsWnOfvoPzzHzSR0" />
    <!-- <meta name="google-site-verification" content="wNw6ZIFTz_hFtiJ2w108gXZcLcg3e8tLeSGIC36Wn_M" /> -->


    <meta name="author" content="许">





<title>Mu Li. Dive into Deep Learning | 须臾所学之野</title>



    <link rel="icon" href="/favicon.png">




    <!-- stylesheets list from _config.yml -->
    
    <link rel="stylesheet" href="/css/style.css">
    



    <!-- scripts list from _config.yml -->
    
    <script src="/js/script.js"></script>
    
    <script src="/js/tocbot.min.js"></script>
    






    
    
        <script>
    MathJax = {
        tex: {
            // 行内公式标志
            inlineMath: [
                ['$', '$']
            ],
            // 块级公式标志
            displayMath: [
                ['$$', '$$']
            ],
            // 下面两个主要是支持渲染某些公式，可以自己了解
            processEnvironments: true,
            processRefs: true,
        },
        options: {
            // 跳过渲染的标签
            skipHtmlTags: ['noscript', 'style', 'textarea', 'pre', 'code'],
            // 跳过mathjax处理的元素的类名，任何元素指定一个类 tex2jax_ignore 将被跳过，多个累=类名'class1|class2'
            ignoreHtmlClass: 'tex2jax_ignore',
        },
        // 这里可能是因为我的MathJax2仍有残留，导致行间公式被渲染成了type="math/tex"，所以要用这种2、3版本混合查找方式进行渲染
        // 这样可能效率低，有机会再改。
        options: {
            renderActions: {
                /* add a new named action not to override the original 'find' action */
                find_script_mathtex: [10, function (doc) {
                    for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
                        const display = !!node.type.match(/; *mode=display/);
                        const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
                        const text = document.createTextNode('');
                        node.parentNode.replaceChild(text, node);
                        math.start = {
                            node: text,
                            delim: '',
                            n: 0
                        };
                        math.end = {
                            node: text,
                            delim: '',
                            n: 0
                        };
                        doc.math.push(math);
                    }
                }, '']
            }
        },
        svg: {
            fontCache: 'global',
        },
    };
</script>

<script type="text/javascript" id="MathJax-script" async
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>

<!-- <script type="text/javascript" id="MathJax-script" async
    src="https://cdn.bootcdn.net/ajax/libs/mathjax/3.1.2/es5/tex-mml-chtml.min.js">
</script> -->
    



<meta name="generator" content="Hexo 6.3.0"></head>

<body>
    <script>
        // this function is used to check current theme before page loaded.
        (() => {
            const currentTheme = window.localStorage && window.localStorage.getItem('theme') || '';
            const isDark = currentTheme === 'dark';
            const pagebody = document.getElementsByTagName('body')[0]
            if (isDark) {
                pagebody.classList.add('dark-theme');
                // mobile
                document.getElementById("mobile-toggle-theme").innerText = "· Dark"
            } else {
                pagebody.classList.remove('dark-theme');
                // mobile
                document.getElementById("mobile-toggle-theme").innerText = "· Light"
            }
        })();
    </script>

    <div class="wrapper">
        <header>
    <nav class="navbar">
        <div class="container">
            <div class="navbar-header header-logo"><a href="/">Xuwp&#39;s Blog</a></div>
            <div class="menu navbar-right">
                
                    <a class="menu-item" href="/archives">Posts</a>
                
                    <a class="menu-item" href="/category">Categories</a>
                
                    <a class="menu-item" href="/tag">Tags</a>
                
                    <a class="menu-item" href="/about">About</a>
                
                    <a class="menu-item" href="/link">Links</a>
                
                    <a class="menu-item" href="https://xuwp.top/cv-en/">Resume</a>
                
                <input id="switch_default" type="checkbox" class="switch_default">
                <label for="switch_default" class="toggleBtn"></label>
            </div>
        </div>
    </nav>

    
    <nav class="navbar-mobile" id="nav-mobile">
        <div class="container">
            <div class="navbar-header">
                <div>
                    <a href="/">Xuwp&#39;s Blog</a><a id="mobile-toggle-theme">·&nbsp;Light</a>
                </div>
                <div class="menu-toggle" onclick="mobileBtn()">&#9776; Menu</div>
            </div>
            <div class="menu" id="mobile-menu">
                
                    <a class="menu-item" href="/archives">Posts</a>
                
                    <a class="menu-item" href="/category">Categories</a>
                
                    <a class="menu-item" href="/tag">Tags</a>
                
                    <a class="menu-item" href="/about">About</a>
                
                    <a class="menu-item" href="/link">Links</a>
                
                    <a class="menu-item" href="https://xuwp.top/cv-en/">Resume</a>
                
            </div>
        </div>
    </nav>

</header>
<script>
    var mobileBtn = function f() {
        var toggleMenu = document.getElementsByClassName("menu-toggle")[0];
        var mobileMenu = document.getElementById("mobile-menu");
        if(toggleMenu.classList.contains("active")){
           toggleMenu.classList.remove("active")
            mobileMenu.classList.remove("active")
        }else{
            toggleMenu.classList.add("active")
            mobileMenu.classList.add("active")
        }
    }
</script>
            <div class="main">
                <div class="container">
    
    
        <div class="post-toc">
    <div class="tocbot-list">
    </div>
    <div class="tocbot-list-menu">
        <a class="tocbot-toc-expand" onclick="expand_toc()">Expand all</a>
        <a onclick="go_top()">Back to top</a>
        <a onclick="go_bottom()">Go to bottom</a>
    </div>
</div>

<script>
    var tocbot_timer;
    var DEPTH_MAX = 6;    // 为 6 时展开所有
    var tocbot_default_config = {
        tocSelector: '.tocbot-list',
        contentSelector: '.post-content',
        headingSelector: 'h1, h2, h3, h4, h5',
        orderedList: false,
        scrollSmooth: true,
        onClick: extend_click,
    };

    function extend_click() {
        clearTimeout(tocbot_timer);
        tocbot_timer = setTimeout(function () {
            tocbot.refresh(obj_merge(tocbot_default_config, { hasInnerContainers: true }));
        }, 420); // 这个值是由 tocbot 源码里定义的 scrollSmoothDuration 得来的
    }

    document.ready(function () {
        tocbot.init(obj_merge(tocbot_default_config, { collapseDepth: 1 }));
    });

    function expandToc() {
        var b = document.querySelector('.tocbot-toc-expand');
        var expanded = b.getAttribute('data-expanded');
        expanded ? b.removeAttribute('data-expanded') : b.setAttribute('data-expanded', true);
        tocbot.refresh(obj_merge(tocbot_default_config, { collapseDepth: expanded ? 1 : DEPTH_MAX }));
        b.innerText = expanded ? 'Expand all' : 'Collapse all';
    }

    function go_top() {
        window.scrollTo(0, 0);
    }

    function go_bottom() {
        window.scrollTo(0, document.body.scrollHeight);
    }

    function obj_merge(target, source) {
        for (var item in source) {
            if (source.hasOwnProperty(item)) {
                target[item] = source[item];
            }
        }
        return target;
    }
</script>
    

    
    <article class="post-wrap">
        <header class="post-header">
            <h1 class="post-title">Mu Li. Dive into Deep Learning</h1>
            
                <div class="post-meta">
                    
                        Author: <a itemprop="author" rel="author" href="/">许</a>
                    

                    
                        <span class="post-time">
                        Date: <a href="#">October 15, 2022&nbsp;&nbsp;21:16:49</a>
                        </span>
                    
                    
                        <span class="post-category">
                    Category:
                            
                                <a href="/categories/AI/">AI</a>
                            
                        </span>
                    
                </div>
            
        </header>

        <div class="post-content">
            <blockquote>
<p>课程地址：<a target="_blank" rel="noopener" href="https://courses.d2l.ai/zh-v2/">https://courses.d2l.ai/zh-v2/</a></p>
<p>课程回放：<a target="_blank" rel="noopener" href="https://b23.tv/Fc52WcK">https://b23.tv/Fc52WcK</a></p>
<p>教材地址：<a target="_blank" rel="noopener" href="https://zh-v2.d2l.ai/">https://zh-v2.d2l.ai/</a></p>
<p>教材讨论：<a target="_blank" rel="noopener" href="https://discuss.d2l.ai/">https://discuss.d2l.ai/</a></p>
</blockquote>
<h1 id="Preliminaries"><a href="#Preliminaries" class="headerlink" title="Preliminaries"></a>Preliminaries</h1><h2 id="线性代数"><a href="#线性代数" class="headerlink" title="线性代数"></a>线性代数</h2><p><img src="https://testingcf.jsdelivr.net/gh/WanpengXu/myPicGo/img/202210152058672.png" alt=""></p>
<p>Frobenius Norm，将矩阵拉成向量，求向量长度。</p>
<p>其他的一般不用。</p>
<p><img src="https://testingcf.jsdelivr.net/gh/WanpengXu/myPicGo/img/202210122124918.png" alt=""></p>
<p>generalizes to：v. 推广为</p>
<p><img src="https://testingcf.jsdelivr.net/gh/WanpengXu/myPicGo/img/202210122135966.png" alt=""></p>
<p>矩阵作用于特征向量时，特征向量方向不变。</p>
<p><strong>按轴求和</strong></p>
<p>什么是按轴求和、降低维度？</p>
<p>给你一张染了色的纸，把它竖着或横着挤压成一个棍，这就是降维；它的颜色会变深，这就是求和。</p>
<p>pytorch中用于按轴求和的方法是<code>torch.Tensor.sum</code>，其调用<code>torch.sum</code>函数</p>
<p>pytorch docs给出的说明</p>
<p><img src="https://testingcf.jsdelivr.net/gh/WanpengXu/myPicGo/img/202210132022552.png" alt=""></p>
<p>返回输入张量input在给定维度dim上每行的和，如果dim是一个维度的列表，把它们全部降低。</p>
<p>这里的维度是怎么定义的？</p>
<p>以正常遍历的维度作为维度0，每往里1层，其维度比上一层多1。</p>
<p>比如这里有一个二维的tensor（三种常见的创建方式）</p>
<p><img src="https://testingcf.jsdelivr.net/gh/WanpengXu/myPicGo/img/202210132035252.png" alt=""></p>
<p>其正常的遍历<code>for i in A</code>，每个<code>i</code>都是一个一维tensor，这个维度就是dim 0。</p>
<p><img src="https://testingcf.jsdelivr.net/gh/WanpengXu/myPicGo/img/202210132037422.png" alt=""></p>
<p>在这个维度上求和，即把每个i相加，其大致流程为</p>
<p><img src="https://testingcf.jsdelivr.net/gh/WanpengXu/myPicGo/img/202210132118728.png" alt=""></p>
<p>类似地，在dim 1上求和</p>
<p><img src="https://testingcf.jsdelivr.net/gh/WanpengXu/myPicGo/img/202210132118032.png" alt=""></p>
<p>如果keepdim为True，那么输出tensor和输入tensor具有相同的维度数量（维数），但被降维的那些维度大小为1。</p>
<p><img src="https://testingcf.jsdelivr.net/gh/WanpengXu/myPicGo/img/202210132126351.png" alt="image-20221013212619231"></p>
<p>保持相同的维数有什么用？</p>
<p>广播。</p>
<p>广播就是当两个tensor相加时，如果shape不相同，但<strong>维数相同</strong>且有些维度的数量为1，可以复制这些维度上的小tensor，使其shape在这个维度上的数量与较大者相等。</p>
<p><img src="https://testingcf.jsdelivr.net/gh/WanpengXu/myPicGo/img/202210132131018.png" alt=""></p>
<h1 id="Linear-Networks"><a href="#Linear-Networks" class="headerlink" title="Linear Networks"></a>Linear Networks</h1><h2 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h2><h3 id="QA"><a href="#QA" class="headerlink" title="QA"></a>QA</h3><p>Q：损失为什么要求平均？</p>
<p>A：除以n的好处是，不论批量多大，梯度的值都差不多（都是平均值），与学习率$\eta$解耦。</p>
<hr>
<p>Q：batchsize是否会影响模型结果？</p>
<p>A：事实上，batchsize越小，模型结果越好，这一点比较反直觉。因为SGD理论上是给模型带来噪音，batchsize越小，噪音越大，但噪音对神经网络是一件好事，因为深度神经网络比较复杂，有噪音可以不容易“跑偏”，更鲁棒，泛化能力更强。</p>
<hr>
<p>Q：随机梯度下降中的随机？</p>
<p>A：随机，指的是元素是随机的，即随机采样。</p>
<hr>
<p>Q：为什么机器学习优化算法采用梯度下降这种一阶导算法，而不采用收敛更快的牛顿法等二阶导算法？</p>
<p>A：首先牛顿法用不了，只能用近似的牛顿法。机器学习中有两个模型，统计模型（Loss）、优化模型（Optim），这两个东西都是错的（可能是想说和现实规则不一样？），所以我们不关心收敛速度，而关心收敛到什么地方，是否泛化能力更强。</p>
<hr>
<p>Q：detach的作用？</p>
<p>A：将其从计算图中剥离，不再计算其梯度，某些pytorch需要先进行这步操作再转为numpy。</p>
<hr>
<p>Q：样本大小不是batchsize的整数倍怎么办？</p>
<p>A：做法一：剩下多少就拿多少，做法二：丢弃，做法三：从下一个epoch补缺少的的过来</p>
<h2 id="Softmax-回归"><a href="#Softmax-回归" class="headerlink" title="Softmax 回归"></a>Softmax 回归</h2><p>Softmax回归中exp的作用是将输入域映射到$(0, +\inf)$，其本质上是将预测的值转为概率，分类问题给出的标签本就是一种概率，衡量两个概率间的区别使用交叉熵（CE）</p>
<p>蓝：$l$</p>
<p>绿：$l$的似然函数</p>
<p>橙：$l$的梯度</p>
<p>L2 Loss</p>
<p><img src="https://testingcf.jsdelivr.net/gh/WanpengXu/myPicGo/img/202210151008974.png" alt=""></p>
<p>L1 Loss</p>
<p><img src="https://testingcf.jsdelivr.net/gh/WanpengXu/myPicGo/img/202210151008979.png" alt=""></p>
<p>当预测值与真实值较远时，梯度永远是常数，权重更新不会太大，更稳定；</p>
<p>当预测值与真实值较近时，因为sub gradient是[-1, 1]间的任意值，在优化末期时不稳定。</p>
<p>Huber’s Robust Loss</p>
<p>L1和L2的缝合形式，避免L1的间断点</p>
<script type="math/tex; mode=display">
l(y, y^{\prime}) = \begin{cases}
\left\vert y - y^{\prime}\right\vert - \frac{1}{2} & if \left\vert y - y^{\prime}\right\vert > 1\\
\frac{1}{2}\left(y - y^{\prime}\right)^2 & otherwise
\end{cases}</script><p><img src="https://testingcf.jsdelivr.net/gh/WanpengXu/myPicGo/img/202210151008693.png" alt=""></p>
<h3 id="QA-1"><a href="#QA-1" class="headerlink" title="QA"></a>QA</h3><p>Q：soft label？</p>
<p>A：softmax回归很难用指数逼近0或1这种极端数值，所以将正确的类记为$0.9$，不正确的类记为$\frac{0.1}{n-1}$。</p>
<p>更多：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/410491474">从<em>Label</em> Smoothing和Knowledge Distillation理解<em>Soft Label</em></a>（陀飞轮）</p>
<p><img src="https://testingcf.jsdelivr.net/gh/WanpengXu/myPicGo/img/202210152022225.png" alt=""></p>
<p><img src="https://testingcf.jsdelivr.net/gh/WanpengXu/myPicGo/img/202210152021026.jpeg" alt=""></p>
<hr>
<p>Q：batchsize的大小为什么没有影响训练速度？</p>
<p>A：batchsize不影响计算量，只影响并行度，进而影响执行的效率。不影响的原因可能是模型很小或是在CPU上训练。</p>
<hr>
<p>Q：测试时为什么要设net.eval()？</p>
<p>A：将网络设置为评估模式后，pytorch不会计算梯度，进而不会计算与梯度相关的操作，可以提升效率、不影响训练过程。</p>
<hr>
<p>Q：optimizer是怎么获取Model的参数的？</p>
<p>A：初始化时传递的，<code>optimizer = torch.optim.SGD(net.parameters(), lr=0.1)</code></p>
<h1 id="MultiLayer-Perceptrons"><a href="#MultiLayer-Perceptrons" class="headerlink" title="MultiLayer Perceptrons"></a>MultiLayer Perceptrons</h1><h2 id="感知机"><a href="#感知机" class="headerlink" title="感知机"></a>感知机</h2><h3 id="QA-2"><a href="#QA-2" class="headerlink" title="QA"></a>QA</h3><p>Q：单层神经网络有万有逼近性，为什么神经网络趋向于增加隐藏层的层数（深度）而不是神经元的个数（宽度）？</p>
<p>A：理论上，单层感知机具有万有逼近性，可以拟合任何函数；实际上，优化算法解不了。</p>
<p><img src="https://testingcf.jsdelivr.net/gh/WanpengXu/myPicGo//img/202212032025502.png" style="zoom:33%;" /></p>
<p>两种神经网络，左侧为“浅度学习”的神经网络，右侧为深度学习的神经网络。理论上，二者的拟合能力相同；但实际上，左侧的网络特别容易overfitting，因为所有神经元都是并行的，不容易迭代到理论解。</p>
<p>直观的例子：</p>
<p><img src="https://testingcf.jsdelivr.net/gh/WanpengXu/myPicGo//img/202212032025154.png" style="zoom:33%;" /></p>
<p>让每层学出部分特征，如第一层学个耳朵/嘴，第二层学个头等等。</p>
<h2 id="过拟合和欠拟合"><a href="#过拟合和欠拟合" class="headerlink" title="过拟合和欠拟合"></a>过拟合和欠拟合</h2><h3 id="introduction"><a href="#introduction" class="headerlink" title="introduction"></a>introduction</h3><p><img src="https://testingcf.jsdelivr.net/gh/WanpengXu/myPicGo//img/202212032037721.png" alt=""></p>
<p><img src="https://testingcf.jsdelivr.net/gh/WanpengXu/myPicGo//img/202212032039687.png" alt=""></p>
<p>模型容量：参数数量+参数值的范围</p>
<p>理论依据：VC维，在深度学习中很难计算</p>
<h3 id="QA-3"><a href="#QA-3" class="headerlink" title="QA"></a>QA</h3><p>Q：SVM相对于神经网络的缺点</p>
<p>A：1. SVM使用kernel，很难达到百万级</p>
<ol>
<li>可调整的参数少</li>
<li>神经网络更灵活，可编程性强</li>
</ol>
<hr>
<p>Q：K折交叉验证在深度学习中是否常用？</p>
<p>A：K折交叉验证确实在深度学习中用的不多。K折交叉验证主要解决机器学习中数据集太小的情况，深度学习的数据集很大，所以很少用。</p>
<h2 id="权重衰减"><a href="#权重衰减" class="headerlink" title="权重衰减"></a>权重衰减</h2><h3 id="introduction-1"><a href="#introduction-1" class="headerlink" title="introduction"></a>introduction</h3><p>一种处理过拟合的方法</p>
<p>模型过拟合，是因为其复杂度太高，即<strong>模型容量太大</strong>，所以我们有两种方法处理过拟合。</p>
<ol>
<li>减少参数数量 </li>
<li><strong>限制参数值的范围</strong>，如$|\boldsymbol{w}|^2 \leq \theta$</li>
</ol>
<p>在模型结构上做出创新不是一件容易的事，所以我们应该先在现有的模型上找到更优的模型参数。</p>
<p>因此，我们可以使用罚函数法限制模型参数的范围，即</p>
<p>形式化原问题</p>
<script type="math/tex; mode=display">
\begin{aligned}
\underset{\boldsymbol{w}}{\min}\ &\ell(\boldsymbol{w}, b)\\
\text{s.t.}\ &\|\boldsymbol{w}\|^2 \leq \theta
\end{aligned}</script><p>应用罚函数法</p>
<script type="math/tex; mode=display">
\begin{aligned}
\underset{\boldsymbol{w}}{\min}\ &\ell(\boldsymbol{w}, b) + \frac{\lambda}{2}\|\boldsymbol{w}\|^2
\end{aligned}</script><p>反向传播更新参数时</p>
<script type="math/tex; mode=display">
\begin{aligned}
\frac{\partial}{\partial \boldsymbol{w}}\left(\ell(\boldsymbol{w}, b)+\frac{\lambda}{2}\|\boldsymbol{w}\|^2\right) &= \frac{\partial \ell(\boldsymbol{w}, b)}{\partial \boldsymbol{w}}+\lambda \boldsymbol{w} \\
\boldsymbol{w}_{t+1} &\leftarrow \boldsymbol{w}_{t} - \eta \frac{\partial}{\partial \boldsymbol{w}_t}\left(\ell(\boldsymbol{w}_t, b_t) + \frac{\lambda}{2}\|\boldsymbol{w}_t\|^2\right) \\
\boldsymbol{w}_{t+1} &\leftarrow (1-\eta\lambda)\boldsymbol{w}_t - \eta \frac{\partial \ell(\boldsymbol{w}_t, b_t)}{\partial \boldsymbol{w}_t}
\end{aligned}</script><p>未增加“罚”（penalty）时，权重迭代公式为</p>
<script type="math/tex; mode=display">
\boldsymbol{w}_{t+1} \leftarrow \boldsymbol{w}_t - \eta \frac{\partial \ell(\boldsymbol{w}_t, b_t)}{\partial \boldsymbol{w}_t}</script><p>通常，$\eta\lambda&lt;1$。因此，权重$\boldsymbol{w}_t$<strong>在梯度下降前会衰减一次</strong>，所以罚函数法在深度学习中就称为权重衰减。</p>
<p>Note. 一般不对$b$做权重衰减，因为效果不大。</p>
<h3 id="QA-4"><a href="#QA-4" class="headerlink" title="QA"></a>QA</h3><p>Q：为什么限制参数值的范围可以降低复杂度？</p>
<p>A：当参数的范围很大时，可能会出现下图的“陡峭”情况。（想象一下$y=\text{big number}\cdot \sin(x)$的泰勒展开式，次数为1的项和次数为7的项，它们的范围就很大）</p>
<p><img src="https://testingcf.jsdelivr.net/gh/WanpengXu/myPicGo/img/202212052109290.png" alt=""></p>
<p>所以限制参数的范围，即限制拟合函数必须“平滑”（类似PhotoShop的钢笔工具，把锚点相互离得很近），所以就减少了复杂性。</p>
<hr>
<p>Q：L2 norm的符号？</p>
<p>A：L2 norm（二范数）的符号应该是$|\boldsymbol{w}|_2$，表示$\sqrt{\sum_{i = 1}^{m}w_i^2}$。所以$|w|_2^2 = \sum_{i = 1}^{m}w_i^2$，即分量的平方和。对于向量来说，二范数就是默认的范数。所以对向量，有$|w|_2 = |w|$，$|w|_2^2 = |w|^2$。</p>
<hr>
<p>Q：假如真正的$\boldsymbol{w}$就是比较大，那加入参数衰减令参数减小，会有反作用吗？</p>
<p>A：因为我们获取到的数据是真实数据的一部分，所以肯定会有噪音，$\lambda$的作用就是处理这些噪音；</p>
<p>假设没有噪音，那么就不会train到奇怪的地方，就不需要参数衰减了；</p>
<p>（我个人觉得是有的，但是与他所对抗噪音带来的好处比，可以忽略它的反作用）</p>
<h2 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h2><h3 id="motivation"><a href="#motivation" class="headerlink" title="motivation"></a>motivation</h3><p>一个好的模型要对输入数据的扰动“鲁棒”。</p>
<p>就像下面这张High School Graduation dropout rate（高中毕业辍学率）一样，淘汰掉一部分输入的学生，可以让输出的结果更优。</p>
<p><img src="https://testingcf.jsdelivr.net/gh/WanpengXu/myPicGo/img/202212061953464.png" style="zoom: 50%;" /></p>
<h3 id="process"><a href="#process" class="headerlink" title="process"></a>process</h3><p>在层间“无偏差地”加入噪音，等价于对数据进行一个Tikhonov正则。</p>
<p>“无偏差”，即对$\boldsymbol{x}$加入噪音，形成$\boldsymbol{x}^\prime$，使得</p>
<script type="math/tex; mode=display">
\mathbb{E}(\boldsymbol{x}^\prime) = \boldsymbol{x}</script><p><strong>Note.</strong> 这里是element-wise相等，不是说“一个期望等于一个向量”，而是说“$x^\prime$的每个元素的期望与$\boldsymbol{x}$的对应元素相等”（重载的等于号）</p>
<p>Dropout对每个元素做如下扰动</p>
<script type="math/tex; mode=display">
x_i^{\prime}= 
\begin{cases}
0 & \text { with probablity } p \\
\frac{x_i}{1-p} & \text { otherise }
\end{cases}</script><p>使得</p>
<script type="math/tex; mode=display">
\mathbb{E}(x^\prime_i) = p \times 0 + (1-p) \cdot \frac{x_i}{1-p} = x_i</script><p>即</p>
<script type="math/tex; mode=display">
\mathbb{E}(\boldsymbol{x}^\prime) = \boldsymbol{x}</script><p>通常在层间使用Dropout，如</p>
<p><img src="https://testingcf.jsdelivr.net/gh/WanpengXu/myPicGo/img/202212062227812.jpg" alt=""></p>
<p>注意.</p>
<p>droupout只在train时生效，使$h \gets \operatorname{dropout}(h)$</p>
<p>在test/inference时关闭，即$h \gets h$，可以保证输出的确定性。</p>
<h3 id="QA-5"><a href="#QA-5" class="headerlink" title="QA"></a>QA</h3><p>Q：dropout随即丢弃，如何保证结果的正确性、可重复性？</p>
<p>A：首先，机器学习没有“正确性”，只有“准确性”。</p>
<p>神经网络的“可重复性”一直是一件困难的事情，但可以固定一些参数以减小“随机性”：</p>
<ol>
<li>固定random seed</li>
<li>固定神经网络的初始权重</li>
<li>禁用NVDIA CUDA中的DNN Library。DNN Library会带来50%~80%的加速，但是受计算机体系结构的限制，并行化导致相加顺序变化，所以每次矩阵运算的结果是不同的，</li>
</ol>
<p>但是其实没必要保证可重复性，只要精度在一个范围内就行。“随机性”不是一个坏事情，也可以让神经网络更稳定、平滑。如果神经网络在各种随机下仍然能够收敛，那么以后训练时也可以收敛。</p>
<hr>
<p>Q：BN和dropout会同时使用吗？</p>
<p>A：BatchNorm是给卷积层用的，而dropout作用于全连接层，二者没有什么相关性。</p>
<hr>
<p>Q：dropout是否会让loss曲线不平滑？</p>
<p>A：正常情况下，如果每个batch后都计算一次loss的话，是平滑的，但这样很“贵”，所以一般都是10个batch后计算一次loss并作图，这时可能是不平滑的。但我们对loss曲线不care，不平滑就不平滑，因为曲线的最后肯定是平滑的，否则代表无法收敛。</p>
<hr>
<p>Q：dropout每个epoch随机选择子网络，最后做平均。这种做法是不是类似于投票的思想？</p>
<p>A：Geoffrey Hinton最初设计dropout时是这样认为的，但有几篇paper认为”Dropout is a regularization”，大家通过实验认为它更像一个正则项。所以Hinton在2017年提出了Capsule Networks，但是这个网络还没有work。</p>
<hr>
<p>Q：dropout已被Google申请专利，产品开发时可以使用吗？</p>
<p>A：Google不仅申请了dropout，也给RNN和transformer等许多技术申请了专利。我在Amazon用dropout，我们的律师也没有说不可以用，但也没有说可以用。（好搞笑啊哈哈哈哈）</p>
<hr>
<p>Q：dropout和weight decay都属于正则，哪个更常用呢？</p>
<p>A：一般weight decay更常用，dropout主要对全连接层使用，而weight decay对CNN、transformer等都可以使用。weight decay的lambda不太好调参。dropout的probability更好调参，因为它很直观，一般只有三种经验值：0.1、0.5、0.9。</p>
<p>什么时候使用dropout呢？假如有一个单隐层MLP，隐层的大小为64，当没有使用dropout时，训练结果没有那么过拟合。那么接下来就可以尝试大小为128的隐层+<code>dropout(p=0.5)</code>。</p>
<p>总得来说，深度学习是希望模型够强，然后通过正则来保证它不过拟合（学偏）。</p>
<p>回到这个问题，dropout使用的多还是因为比较好调参，例如模型特别大、隐藏层特别复杂，就可以<code>dropout(p=0.9)</code>，丢弃90%层间数据，成为强回归器；反之，则<code>dropout(p=0.1)</code>，成为弱回归器。</p>
<hr>
<p>Q：在同样的lr下，使用dropout是否会使得参数收敛变慢，需要调高lr吗？</p>
<p>A：是有可能减慢收敛的，因为参数梯度更新的更少了。但是没有哪种经验说需要调高lr，因为lr对期望和方差更敏感，而dropout不改变期望。</p>
<h2 id="数值稳定性"><a href="#数值稳定性" class="headerlink" title="数值稳定性"></a>数值稳定性</h2><h3 id="introduction-2"><a href="#introduction-2" class="headerlink" title="introduction"></a>introduction</h3><p>考虑如下的d层神经网络</p>
<script type="math/tex; mode=display">
\begin{cases}
\boldsymbol{h}^t=f_t\left(\boldsymbol{h}^{t-1}\right) \\
y=\ell \circ f_d \circ \ldots \circ f_1(\boldsymbol{x})
\end{cases}</script><p>计算损失$\ell$关于参数$\boldsymbol{W}_t$的梯度时，有</p>
<script type="math/tex; mode=display">
\begin{aligned}
\frac{\partial \ell}{\partial \boldsymbol{W}^t}
&=\frac{\partial \ell}{\partial \boldsymbol{h}^d} \frac{\partial \boldsymbol{h}^d}{\partial \boldsymbol{h}^{d-1}} \ldots \frac{\partial \boldsymbol{h}^{t+1}}{\partial \boldsymbol{h}^t} \frac{\partial \boldsymbol{h}^t}{\partial \boldsymbol{W}^t} \\
&=\frac{\partial \ell}{\partial \boldsymbol{h}^d} \cdot \prod_{i=t}^{d-1}\frac{\partial \boldsymbol{h}^{i+1}}{\partial \boldsymbol{h}^{i}} \cdot \frac{\partial \boldsymbol{h}^t}{\partial \boldsymbol{W}^t}
\end{aligned}</script><p>其中，因向量对向量的微分为矩阵，所以需计算包含$d-1-t+1=d-t$次矩阵乘法。</p>
<p>当神经网络的深度增加时，大量的矩阵乘法会带来两个问题：</p>
<ul>
<li>梯度爆炸：如$1.5^{100} \approx 4 \times 10^7$</li>
<li>梯度消失，如$0.8^{100} \approx 2 \times 10^{-10}$</li>
</ul>
<h3 id="example"><a href="#example" class="headerlink" title="example"></a>example</h3><p>假如有以下MLP（偏移并入$\boldsymbol{W}$中）</p>
<script type="math/tex; mode=display">
\boldsymbol{h}^t=f_t\left(\boldsymbol{h}^{t-1}\right)=\sigma\left(\boldsymbol{W}^t \boldsymbol{h}^{t-1}\right)</script><p>每层的输出对输入的微分为</p>
<script type="math/tex; mode=display">
\frac{\partial \boldsymbol{h}^t}{\partial \boldsymbol{h}^{t-1}}
=\operatorname{diag}\left(\sigma^{\prime}\left(\boldsymbol{W}^t \boldsymbol{h}^{t-1}\right)\right)W^t</script><p>Note. 这里不应该有转置，每层的微分是这样得到的：事实上，按分子布局，有$\frac{\partial A\cdot\boldsymbol{x}}{\partial \boldsymbol{x}}=A$。所以，即使加入sigmoid函数，也应该是一个$m \times n$的矩阵。按嵌套求导规则大概可以得到$\sigma^{\prime}\left(\boldsymbol{W}^t \boldsymbol{h}^{t-1}\right)W^t$，但是这个结果的矩阵布局是$(m \times 1) \times (m \times n)$，不符合求导规则，所以要让前面的向量对角化（这里的$\operatorname{diag}(\cdot)$）以成为成$(m \times m) \times (m \times n)=m \times n$。</p>
<p>$\ell$对$\boldsymbol{W}_t$微分需要的矩阵乘积为</p>
<script type="math/tex; mode=display">
\prod_{i=t}^{d-1} \frac{\partial \boldsymbol{h}^{i+1}}{\partial \boldsymbol{h}^i}
=\prod_{i=t}^{d-1} \operatorname{diag}\left(\sigma^{\prime}\left(\boldsymbol{W}^i \boldsymbol{h}^{i-1}\right)\right)W^i</script><h4 id="梯度爆炸"><a href="#梯度爆炸" class="headerlink" title="梯度爆炸"></a>梯度爆炸</h4><p>当$\sigma(\cdot)=\operatorname{ReLU}(\cdot)$时</p>
<script type="math/tex; mode=display">
\sigma(x)=
\begin{cases}
x &,x \geq 0 \\
0 &,\text{otherwise}
\end{cases} \\

\sigma^{\prime}(x)=
\begin{cases}
1 &,x \geq 0 \\
0 &,\text{otherwise}
\end{cases}</script><p>$\operatorname{diag}\left(\sigma^{\prime}\left(\boldsymbol{W}^i \boldsymbol{h}^{i-1}\right)\right)$中仅有对角线元素，且为0或1。这时，其与$W^i$相乘，会使$W^i$的某些行清零，其他行保留原来的值（将左边矩阵的每行转置后依次放在右边矩阵的列上，列向量为onehot则保留右侧矩阵的行，列向量为零向量则清零右侧矩阵的行）。这样，$\prod_{i=t}^{d-1} \frac{\partial \boldsymbol{h}^{i+1}}{\partial \boldsymbol{h}^i}$中的一些元素会来自$\prod_{i=t}^{d-1}W^i$。当网络加深时，$d-t$就会很大，如果$W^i$中的元素大于1，累乘会使最终结果很大，即梯度爆炸。</p>
<p><strong>梯度爆炸带来的问题</strong></p>
<ul>
<li><p>累乘值会超出值域（当超过值域时成为infinity）</p>
<ul>
<li><p>对16位浮点数（float16，半精度浮点数）尤为严重</p>
<p><img src="https://testingcf.jsdelivr.net/gh/WanpengXu/myPicGo/img/202212201954933.png" alt=""></p>
<p>可见，最小正规数（下溢阈值）$\approx 6e-5$，最大值$\approx6e4$，数值区间很小。</p>
</li>
</ul>
</li>
<li><p>对学习率敏感</p>
<ul>
<li>学习率太大$\to$大参数值$\to$更大的梯度</li>
<li>学习率太小$\to$训练无进展</li>
<li>需要在训练过程动态调整学习率</li>
</ul>
</li>
</ul>
<h4 id="梯度消失"><a href="#梯度消失" class="headerlink" title="梯度消失"></a>梯度消失</h4><p>当$\sigma(\cdot)=\operatorname{logistic}(\cdot)$时</p>
<script type="math/tex; mode=display">
\begin{aligned}
\sigma(x) &= \frac{1}{1+\exp(-x)} \\
\sigma^{\prime}(x) &= \sigma(x)(1-\sigma(x))
\end{aligned}</script><p><img src="https://testingcf.jsdelivr.net/gh/WanpengXu/myPicGo/img/202212202128620.png" alt=""></p>
<p>当输入值很大时，激活函数的梯度值很小，即$\operatorname{diag}\left(\sigma^{\prime}\left(\boldsymbol{W}^i \boldsymbol{h}^{i-1}\right)\right)$中仅有对角线元素，且值很小。当网络加深时，$d-t$就会很大，如果$\boldsymbol{W}^i \boldsymbol{h}^{i-1}$的每个元素很大，$\sigma^{\prime}\left(\boldsymbol{W}^i \boldsymbol{h}^{i-1}\right)$中的每个元素就会很小，累乘会使最终结果很小，即梯度消失。</p>
<p><strong>梯度消失带来的问题</strong></p>
<ul>
<li>梯度值变成0<ul>
<li>对16位浮点数尤为严重</li>
</ul>
</li>
<li>训练没有进展<ul>
<li>不论学习率大小</li>
</ul>
</li>
<li>对于底层部尤为严重<ul>
<li>仅仅顶层部（靠近输出的层）训练的较好</li>
<li>神经网络无法加深</li>
</ul>
</li>
</ul>
<h3 id="模型初始化和激活函数"><a href="#模型初始化和激活函数" class="headerlink" title="模型初始化和激活函数"></a>模型初始化和激活函数</h3><p>如何让训练更加稳定，增加数值稳定性呢？</p>
<p>目标：让梯度值在合理的范围内，如[1e-6, 1e3]</p>
<p>方法：</p>
<ul>
<li><strong>将乘法变为加法。</strong>如：ResNet, LSTM</li>
<li><strong>归一化。</strong>如：权重归一化、梯度裁剪（clipping）</li>
<li><strong>合理的权重初始化和激活函数。</strong></li>
</ul>
<p>这里讲解“合理的权重初始化和激活函数”</p>
<h4 id="motivation：让层间的方差为同一个常数"><a href="#motivation：让层间的方差为同一个常数" class="headerlink" title="motivation：让层间的方差为同一个常数"></a>motivation：让层间的方差为同一个常数</h4><p>将每层的输出和梯度看作随机变量，让所有层间的均值和方差都保持一致。</p>
<p>即设计神经网络，使其对$\forall i, t$，有</p>
<p>正向传播时</p>
<script type="math/tex; mode=display">
\begin{aligned}
\mathbb{E}\left[h_i^t\right] & =0 \\
\operatorname{Var}\left[h_i^t\right] & =a
\end{aligned}</script><p>反向传播时</p>
<script type="math/tex; mode=display">
\begin{aligned}
\mathbb{E}\left[\frac{\partial \ell}{\partial h_i^t}\right] &= 0 \quad \\
\operatorname{Var}\left[\frac{\partial \ell}{\partial h_i^t}\right] &= b
\end{aligned}</script><p>其中，$a, b$为常数。</p>
<p>怎么达到这个要求呢？</p>
<h4 id="权重初始化"><a href="#权重初始化" class="headerlink" title="权重初始化"></a>权重初始化</h4><p>对$h^t$层，</p>
<p>假设其权重$w_{ij}^t$都是从同一分布中独立抽取的。设该分布的均值和方差为$\mu=0, \sigma_t^2$（不一定是高斯/正态分布）。</p>
<p>假设其输入$h_i^{t-1}$也服从一个分布，设该分布的均值和方差为$\mu=0, \operatorname{Var}[h_i^{t-1}]$，且独立于$w_{ij}^t$。</p>
<p>假设没有激活函数，即$\boldsymbol{h}^t = \boldsymbol{W}^t\boldsymbol{h}^{t-1}, \boldsymbol{W}^t\in\mathbb{R}^{n_t \times n_{t-1}}$</p>
<p>正向传播时，经推导，有</p>
<script type="math/tex; mode=display">
\begin{aligned}
\mathbb{E}[h_i^t] &= 0 \\
\operatorname{Var}[h_i^t] &= n_{t-1} \sigma_t^2 \operatorname{Var}[h_i^{t-1}]
\end{aligned}</script><p>反向传播时，经推导，有</p>
<script type="math/tex; mode=display">
\begin{aligned}
\mathbb{E}\left[\frac{\partial \ell}{\partial h_i^{t-1}}\right] &= 0 \\
\operatorname{Var}\left[\frac{\partial \ell}{\partial h_i^{t-1}}\right] &= n_t \sigma_t^2 \operatorname{Var}\left[\frac{\partial \ell}{\partial h_j^t}\right]
\end{aligned}</script><p>为了使得层间的方差保持一致，那么必须有</p>
<script type="math/tex; mode=display">
\begin{aligned}
n_{t-1} \sigma_t^2 &= 1 \\
n_t \sigma_t^2 &= 1
\end{aligned}</script><p>但这个条件难以满足，但我们可以满足两式平均得到的条件</p>
<script type="math/tex; mode=display">
\frac{n_{t-1} + n_t}{2} \sigma_t^2 = 1</script><p>即</p>
<script type="math/tex; mode=display">
\sigma_t^2 = \frac{2}{n_{t-1} + n_t}</script><p>这就是<strong>Xavier初始化</strong>的基础：<strong>通过该层的输入数量和输出数量确定权重分布的方差</strong>。</p>
<p>若采用正态分布，有</p>
<script type="math/tex; mode=display">
N(0, \frac{2}{n_\text{in} + n_\text{out}})</script><p>若采用均匀分布，有</p>
<script type="math/tex; mode=display">
U(-\sqrt{\frac{6}{n_\text{in} + n_\text{out}}}, \sqrt{\frac{6}{n_\text{in} + n_\text{out}}})</script><p>Note. 为保持均值为0，$U(a, b)$中$a=b$；为保证方差为$\frac{2}{n_\text{in} + n_\text{out}}$，$U(a, a)$的方差$\frac{a^2}{3} = \frac{2}{n_\text{in} + n_\text{out}}$。</p>
<p>Xavier初始化是很常用的一种初始化方法。</p>
<h4 id="合适的激活函数"><a href="#合适的激活函数" class="headerlink" title="合适的激活函数"></a>合适的激活函数</h4><p>假设激活函数是线性的（事实上不使用，网络不会产生非线性、万有逼近能力），即$\sigma(x) = \alpha x + \beta$</p>
<p>设</p>
<script type="math/tex; mode=display">
\boldsymbol{h}^t = \sigma(\boldsymbol{h}^\text{temp}), \boldsymbol{h}^\text{temp} = \boldsymbol{W}^t\boldsymbol{h}^{t-1}</script><p>同时，有</p>
<script type="math/tex; mode=display">
\frac{\partial{\ell}}{\partial{\boldsymbol{h}^{t-1}}} = \alpha \frac{\partial{\ell}}{\partial{\boldsymbol{h}^\text{temp}}}\frac{\partial\boldsymbol{h}^\text{temp}}{\partial\boldsymbol{h}^{t-1}}, \frac{\partial\boldsymbol{h}^\text{temp}}{\partial\boldsymbol{h}^{t-1}} = \boldsymbol{W}^t</script><p>正向传播时，经推导，有</p>
<script type="math/tex; mode=display">
\begin{aligned}
\mathbb{E}[h_i^t] &= \beta \\
\operatorname{Var}[h_i^t] &= \alpha^2 \operatorname{Var}[h_i^\text{temp}]
\end{aligned}</script><p>为满足motivation，需要</p>
<script type="math/tex; mode=display">
\begin{aligned}
\beta  &= 0 \\
\alpha &= 1
\end{aligned}</script><p>反向传播时，经推导，有</p>
<script type="math/tex; mode=display">
\begin{aligned}
\mathbb{E}\left[\frac{\partial \ell}{\partial h_i^{t-1}}\right] & =0 \\
\operatorname{Var}\left[\frac{\partial \ell}{\partial h_i^{t-1}}\right] & =\alpha^2 \operatorname{Var}\left[\frac{\partial \ell}{\partial h_i^\text{temp}}\right]
\end{aligned}</script><p>为满足motivation，需要</p>
<script type="math/tex; mode=display">
\alpha = 1</script><p>即$\sigma(x) = x$时，满足motivation，训练更稳定。</p>
<p>为此，我们使用泰勒展开检查常用激活函数</p>
<script type="math/tex; mode=display">
\begin{aligned}
\operatorname{logistic}(x) & =\frac{1}{2}+\frac{x}{4}-\frac{x^3}{48}+O\left(x^5\right) \\
\tanh (x) & =0+x-\frac{x^3}{3}+O\left(x^5\right) \\
\operatorname{ReLU}(x) & =0+x \quad \text { for } x \geq 0
\end{aligned}</script><p>其中，$\tanh(x), \operatorname{ReLU}(x)$在零点附近近似$f(x) = x$，这也从数值稳定性角度解释了这二者为何效果比较好。</p>
<p>为使得logistic函数在零点附近也近似$f(x) = x$，我们对其进行调整，即</p>
<script type="math/tex; mode=display">
\operatorname{scaled logistic}(x) = 4 \times \operatorname{logistic}(x) - 2</script><p>这样得到的scaled logistic函数，在零点附近时即为<strong>identity function</strong> / $f(x) = x$，可以解决掉logistic函数存在的问题。</p>
<h3 id="QA-6"><a href="#QA-6" class="headerlink" title="QA"></a>QA</h3><p>Q：nan, inf产生的原因？解决方法？</p>
<p>A：inf就是今天讲到的原因（lr太大，权重初始值太大），nan（Not A Number）一般是因为除零，如梯度值较小时将其除零；调小学习率，调小方差，直到正确计算出值，然后再调大使得训练有进展。</p>
<hr>

        </div>

        
            <section class="post-copyright">
                
                    <p class="copyright-item">
                        <span>Author:</span>
                        <span>许</span>
                    </p>
                
                
                    <p class="copyright-item">
                        <span>Permalink:</span>
                        <span><a href="https://xuwp.top/Dive-into-Deep-Learning.html">https://xuwp.top/Dive-into-Deep-Learning.html</a></span>
                    </p>
                
                
                    <p class="copyright-item">
                        <span>License:</span>
                        <span>Copyright (c) 2022 <a target="_blank" rel="noopener" href="http://creativecommons.org/licenses/by-nc/4.0/">CC-BY-NC-4.0</a> LICENSE</span>
                    </p>
                
                

            </section>
        
        <section class="post-tags">
            <div>
                <span>Tag(s):</span>
                <span class="tag">
                    
                    
                        <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"># 深度学习</a>
                    
                        
                </span>
            </div>
            <div>
                <a href="javascript:window.history.back();">back</a>
                <span>· </span>
                <a href="/">home</a>
            </div>
        </section>
        <section class="post-nav">
            
                <a class="prev" rel="prev" href="/Zhihua-Zhou-Machine-Learning-Prelim.html">周志华. 机器学习初步(2022秋)</a>
            
            
            <a class="next" rel="next" href="/MACHINE-LEARNING-2021-SPRING-HOMEWORKS.html">Hung-yi Lee. Machine Learning 2021 HWs</a>
            
        </section>


    </article>
</div>

            </div>
            <footer id="footer" class="footer">
    <div class="copyright">
        <span>Stay hungry. Stay foolish.</span>
    </div>
</footer>

    </div>
</body>

</html>