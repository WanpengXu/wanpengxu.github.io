<!DOCTYPE html>
<html lang="default">

<head>
    <meta charset="UTF-8">
<meta name="viewport"
      content="width=device-width, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">

    <meta name="baidu-site-verification" content="code-dlFXc2180h" />


    <!-- <meta name="google-site-verification" content="qlTU_n7RT-JIFfLGMsgqxNsTEkVlsWnOfvoPzzHzSR0" /> -->
    <meta name="google-site-verification" content="wNw6ZIFTz_hFtiJ2w108gXZcLcg3e8tLeSGIC36Wn_M" />


    <meta name="author" content="许">





<title>Mu Li. Dive into Deep Learning | 须臾所学之野</title>



    <link rel="icon" href="/favicon.png">




    <!-- stylesheets list from _config.yml -->
    
    <link rel="stylesheet" href="/css/style.css">
    



    <!-- scripts list from _config.yml -->
    
    <script src="/js/script.js"></script>
    
    <script src="/js/tocbot.min.js"></script>
    






    
    
        <script>
    MathJax = {
        tex: {
            // 行内公式标志
            inlineMath: [
                ['$', '$']
            ],
            // 块级公式标志
            displayMath: [
                ['$$', '$$']
            ],
            // 下面两个主要是支持渲染某些公式，可以自己了解
            processEnvironments: true,
            processRefs: true,
        },
        options: {
            // 跳过渲染的标签
            skipHtmlTags: ['noscript', 'style', 'textarea', 'pre', 'code'],
            // 跳过mathjax处理的元素的类名，任何元素指定一个类 tex2jax_ignore 将被跳过，多个累=类名'class1|class2'
            ignoreHtmlClass: 'tex2jax_ignore',
        },
        // 这里可能是因为我的MathJax2仍有残留，导致行间公式被渲染成了type="math/tex"，所以要用这种2、3版本混合查找方式进行渲染
        // 这样可能效率低，有机会再改。
        options: {
            renderActions: {
                /* add a new named action not to override the original 'find' action */
                find_script_mathtex: [10, function (doc) {
                    for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
                        const display = !!node.type.match(/; *mode=display/);
                        const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
                        const text = document.createTextNode('');
                        node.parentNode.replaceChild(text, node);
                        math.start = {
                            node: text,
                            delim: '',
                            n: 0
                        };
                        math.end = {
                            node: text,
                            delim: '',
                            n: 0
                        };
                        doc.math.push(math);
                    }
                }, '']
            }
        },
        svg: {
            fontCache: 'global',
        },
    };
</script>

<script type="text/javascript" id="MathJax-script" async
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>

<!-- <script type="text/javascript" id="MathJax-script" async
    src="https://cdn.bootcdn.net/ajax/libs/mathjax/3.1.2/es5/tex-mml-chtml.min.js">
</script> -->
    



<meta name="generator" content="Hexo 6.3.0"></head>

<body>
    <script>
        // this function is used to check current theme before page loaded.
        (() => {
            const currentTheme = window.localStorage && window.localStorage.getItem('theme') || '';
            const isDark = currentTheme === 'dark';
            const pagebody = document.getElementsByTagName('body')[0]
            if (isDark) {
                pagebody.classList.add('dark-theme');
                // mobile
                document.getElementById("mobile-toggle-theme").innerText = "· Dark"
            } else {
                pagebody.classList.remove('dark-theme');
                // mobile
                document.getElementById("mobile-toggle-theme").innerText = "· Light"
            }
        })();
    </script>

    <div class="wrapper">
        <header>
    <nav class="navbar">
        <div class="container">
            <div class="navbar-header header-logo"><a href="/">Xuwp&#39;s Blog</a></div>
            <div class="menu navbar-right">
                
                    <a class="menu-item" href="/archives">Posts</a>
                
                    <a class="menu-item" href="/category">Categories</a>
                
                    <a class="menu-item" href="/tag">Tags</a>
                
                    <a class="menu-item" href="/about">About</a>
                
                    <a class="menu-item" href="/link">Links</a>
                
                    <a class="menu-item" href="https://xuwp.top/cv-en/">Resume</a>
                
                <input id="switch_default" type="checkbox" class="switch_default">
                <label for="switch_default" class="toggleBtn"></label>
            </div>
        </div>
    </nav>

    
    <nav class="navbar-mobile" id="nav-mobile">
        <div class="container">
            <div class="navbar-header">
                <div>
                    <a href="/">Xuwp&#39;s Blog</a><a id="mobile-toggle-theme">·&nbsp;Light</a>
                </div>
                <div class="menu-toggle" onclick="mobileBtn()">&#9776; Menu</div>
            </div>
            <div class="menu" id="mobile-menu">
                
                    <a class="menu-item" href="/archives">Posts</a>
                
                    <a class="menu-item" href="/category">Categories</a>
                
                    <a class="menu-item" href="/tag">Tags</a>
                
                    <a class="menu-item" href="/about">About</a>
                
                    <a class="menu-item" href="/link">Links</a>
                
                    <a class="menu-item" href="https://xuwp.top/cv-en/">Resume</a>
                
            </div>
        </div>
    </nav>

</header>
<script>
    var mobileBtn = function f() {
        var toggleMenu = document.getElementsByClassName("menu-toggle")[0];
        var mobileMenu = document.getElementById("mobile-menu");
        if(toggleMenu.classList.contains("active")){
           toggleMenu.classList.remove("active")
            mobileMenu.classList.remove("active")
        }else{
            toggleMenu.classList.add("active")
            mobileMenu.classList.add("active")
        }
    }
</script>
            <div class="main">
                <div class="container">
    
    
        <div class="post-toc">
    <div class="tocbot-list">
    </div>
    <div class="tocbot-list-menu">
        <a class="tocbot-toc-expand" onclick="expand_toc()">Expand all</a>
        <a onclick="go_top()">Back to top</a>
        <a onclick="go_bottom()">Go to bottom</a>
    </div>
</div>

<script>
    var tocbot_timer;
    var DEPTH_MAX = 6;    // 为 6 时展开所有
    var tocbot_default_config = {
        tocSelector: '.tocbot-list',
        contentSelector: '.post-content',
        headingSelector: 'h1, h2, h3, h4, h5',
        orderedList: false,
        scrollSmooth: true,
        onClick: extend_click,
    };

    function extend_click() {
        clearTimeout(tocbot_timer);
        tocbot_timer = setTimeout(function () {
            tocbot.refresh(obj_merge(tocbot_default_config, { hasInnerContainers: true }));
        }, 420); // 这个值是由 tocbot 源码里定义的 scrollSmoothDuration 得来的
    }

    document.ready(function () {
        tocbot.init(obj_merge(tocbot_default_config, { collapseDepth: 1 }));
    });

    function expandToc() {
        var b = document.querySelector('.tocbot-toc-expand');
        var expanded = b.getAttribute('data-expanded');
        expanded ? b.removeAttribute('data-expanded') : b.setAttribute('data-expanded', true);
        tocbot.refresh(obj_merge(tocbot_default_config, { collapseDepth: expanded ? 1 : DEPTH_MAX }));
        b.innerText = expanded ? 'Expand all' : 'Collapse all';
    }

    function go_top() {
        window.scrollTo(0, 0);
    }

    function go_bottom() {
        window.scrollTo(0, document.body.scrollHeight);
    }

    function obj_merge(target, source) {
        for (var item in source) {
            if (source.hasOwnProperty(item)) {
                target[item] = source[item];
            }
        }
        return target;
    }
</script>
    

    
    <article class="post-wrap">
        <header class="post-header">
            <h1 class="post-title">Mu Li. Dive into Deep Learning</h1>
            
                <div class="post-meta">
                    
                        Author: <a itemprop="author" rel="author" href="/">许</a>
                    

                    
                        <span class="post-time">
                        Date: <a href="#">October 15, 2022&nbsp;&nbsp;21:16:49</a>
                        </span>
                    
                    
                        <span class="post-category">
                    Category:
                            
                                <a href="/categories/AI/">AI</a>
                            
                        </span>
                    
                </div>
            
        </header>

        <div class="post-content">
            <blockquote>
<p>课程地址：<a target="_blank" rel="noopener" href="https://courses.d2l.ai/zh-v2/">https://courses.d2l.ai/zh-v2/</a></p>
<p>课程回放：<a target="_blank" rel="noopener" href="https://space.bilibili.com/1567748478/channel/seriesdetail?sid=358497">【完结】动手学深度学习 PyTorch版</a></p>
<p>教材地址：<a target="_blank" rel="noopener" href="https://zh-v2.d2l.ai/">https://zh-v2.d2l.ai/</a></p>
<p>教材讨论：<a target="_blank" rel="noopener" href="https://discuss.d2l.ai/">https://discuss.d2l.ai/</a></p>
</blockquote>
<h1 id="Preliminaries"><a href="#Preliminaries" class="headerlink" title="Preliminaries"></a>Preliminaries</h1><h2 id="线性代数"><a href="#线性代数" class="headerlink" title="线性代数"></a>线性代数</h2><p><img src="https://testingcf.jsdelivr.net/gh/WanpengXu/myPicGo/img/202210152058672.png" alt=""></p>
<p>Frobenius Norm，将矩阵拉成向量，求向量长度。</p>
<p>其他的一般不用。</p>
<p><img src="https://testingcf.jsdelivr.net/gh/WanpengXu/myPicGo/img/202210122124918.png" alt=""></p>
<p>generalizes to：v. 推广为</p>
<p><img src="https://testingcf.jsdelivr.net/gh/WanpengXu/myPicGo/img/202210122135966.png" alt=""></p>
<p>矩阵作用于特征向量时，特征向量方向不变。</p>
<p><strong>按轴求和</strong></p>
<p>什么是按轴求和、降低维度？</p>
<p>给你一张染了色的纸，把它竖着或横着挤压成一个棍，这就是降维；它的颜色会变深，这就是求和。</p>
<p>pytorch中用于按轴求和的方法是<code>torch.Tensor.sum</code>，其调用<code>torch.sum</code>函数</p>
<p>pytorch docs给出的说明</p>
<p><img src="https://testingcf.jsdelivr.net/gh/WanpengXu/myPicGo/img/202210132022552.png" alt=""></p>
<p>返回输入张量input在给定维度dim上每行的和，如果dim是一个维度的列表，把它们全部降低。</p>
<p>这里的维度是怎么定义的？</p>
<p>以正常遍历的维度作为维度0，每往里1层，其维度比上一层多1。</p>
<p>比如这里有一个二维的tensor（三种常见的创建方式）</p>
<p><img src="https://testingcf.jsdelivr.net/gh/WanpengXu/myPicGo/img/202210132035252.png" alt=""></p>
<p>其正常的遍历<code>for i in A</code>，每个<code>i</code>都是一个一维tensor，这个维度就是dim 0。</p>
<p><img src="https://testingcf.jsdelivr.net/gh/WanpengXu/myPicGo/img/202210132037422.png" alt=""></p>
<p>在这个维度上求和，即把每个i相加，其大致流程为</p>
<p><img src="https://testingcf.jsdelivr.net/gh/WanpengXu/myPicGo/img/202210132118728.png" alt=""></p>
<p>类似地，在dim 1上求和</p>
<p><img src="https://testingcf.jsdelivr.net/gh/WanpengXu/myPicGo/img/202210132118032.png" alt=""></p>
<p>如果keepdim为True，那么输出tensor和输入tensor具有相同的维度数量（维数），但被降维的那些维度大小为1。</p>
<p><img src="https://testingcf.jsdelivr.net/gh/WanpengXu/myPicGo/img/202210132126351.png" alt="image-20221013212619231"></p>
<p>保持相同的维数有什么用？</p>
<p>广播。</p>
<p>广播就是当两个tensor相加时，如果shape不相同，但<strong>维数相同</strong>且有些维度的数量为1，可以复制这些维度上的小tensor，使其shape在这个维度上的数量与较大者相等。</p>
<p><img src="https://testingcf.jsdelivr.net/gh/WanpengXu/myPicGo/img/202210132131018.png" alt=""></p>
<h1 id="Linear-networks"><a href="#Linear-networks" class="headerlink" title="Linear-networks"></a>Linear-networks</h1><h2 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h2><h3 id="QA"><a href="#QA" class="headerlink" title="QA"></a>QA</h3><p>Q：损失为什么要求平均？</p>
<p>A：除以n的好处是，不论批量多大，梯度的值都差不多（都是平均值），与学习率$\eta$解耦。</p>
<p>Q：batchsize是否会影响模型结果？</p>
<p>A：事实上，batchsize越小，模型结果越好，这一点比较反直觉。因为SGD理论上是给模型带来噪音，batchsize越小，噪音越大，但噪音对神经网络是一件好事，因为深度神经网络比较复杂，有噪音可以不容易“跑偏”，更鲁棒，泛化能力更强。</p>
<p>Q：随机梯度下降中的随机？</p>
<p>A：随机，指的是元素是随机的，即随机采样。</p>
<p>Q：为什么机器学习优化算法采用梯度下降这种一阶导算法，而不采用收敛更快的牛顿法等二阶导算法？</p>
<p>A：首先牛顿法用不了，只能用近似的牛顿法。机器学习中有两个模型，统计模型（Loss）、优化模型（Optim），这两个东西都是错的（可能是想说和现实规则不一样？），所以我们不关心收敛速度，而关心收敛到什么地方，是否泛化能力更强。</p>
<p>Q：detach的作用？</p>
<p>A：将其从计算图中剥离，不再计算其梯度，某些pytorch需要先进行这步操作再转为numpy。</p>
<p>Q：样本大小不是batchsize的整数倍怎么办？</p>
<p>A：做法一：剩下多少就拿多少，做法二：丢弃，做法三：从下一个epoch补缺少的的过来</p>
<h2 id="Softmax-回归"><a href="#Softmax-回归" class="headerlink" title="Softmax 回归"></a>Softmax 回归</h2><p>Softmax回归中exp的作用是将输入域映射到$(0, +\inf)$，其本质上是将预测的值转为概率，分类问题给出的标签本就是一种概率，衡量两个概率间的区别使用交叉熵（CE）</p>
<p>蓝：$l$</p>
<p>绿：$l$的似然函数</p>
<p>橙：$l$的梯度</p>
<p>L2 Loss</p>
<p><img src="https://testingcf.jsdelivr.net/gh/WanpengXu/myPicGo/img/202210151008974.png" alt=""></p>
<p>L1 Loss</p>
<p><img src="https://testingcf.jsdelivr.net/gh/WanpengXu/myPicGo/img/202210151008979.png" alt=""></p>
<p>当预测值与真实值较远时，梯度永远是常数，权重更新不会太大，更稳定；</p>
<p>当预测值与真实值较近时，因为sub gradient是[-1, 1]间的任意值，在优化末期时不稳定。</p>
<p>Huber’s Robust Loss</p>
<p>L1和L2的缝合形式，避免L1的间断点</p>
<script type="math/tex; mode=display">
l(y, y^{\prime}) = \begin{cases}
\left\vert y - y^{\prime}\right\vert - \frac{1}{2} & if \left\vert y - y^{\prime}\right\vert > 1\\
\frac{1}{2}\left(y - y^{\prime}\right)^2 & otherwise
\end{cases}</script><p><img src="https://testingcf.jsdelivr.net/gh/WanpengXu/myPicGo/img/202210151008693.png" alt=""></p>
<h3 id="QA-1"><a href="#QA-1" class="headerlink" title="QA"></a>QA</h3><p>Q：soft label？</p>
<p>A：softmax回归很难用指数逼近0或1这种极端数值，所以将正确的类记为$0.9$，不正确的类记为$\frac{0.1}{n-1}$。</p>
<p>更多：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/410491474">从<em>Label</em> Smoothing和Knowledge Distillation理解<em>Soft Label</em></a>（陀飞轮）</p>
<p><img src="https://testingcf.jsdelivr.net/gh/WanpengXu/myPicGo/img/202210152022225.png" alt=""></p>
<p><img src="https://testingcf.jsdelivr.net/gh/WanpengXu/myPicGo/img/202210152021026.jpeg" alt=""></p>
<p>Q：batchsize的大小为什么没有影响训练速度？</p>
<p>A：batchsize不影响计算量，只影响并行度，进而影响执行的效率。不影响的原因可能是模型很小或是在CPU上训练。</p>
<p>Q：测试时为什么要设net.eval()？</p>
<p>A：将网络设置为评估模式后，pytorch不会计算梯度，进而不会计算与梯度相关的操作，可以提升效率、不影响训练过程。</p>
<p>Q：optimizer是怎么获取Model的参数的？</p>
<p>A：初始化时传递的，<code>optimizer = torch.optim.SGD(net.parameters(), lr=0.1)</code></p>
<h1 id="multilayer-perceptrons"><a href="#multilayer-perceptrons" class="headerlink" title="multilayer-perceptrons"></a>multilayer-perceptrons</h1><h2 id="感知机"><a href="#感知机" class="headerlink" title="感知机"></a>感知机</h2><h3 id="QA-2"><a href="#QA-2" class="headerlink" title="QA"></a>QA</h3><p>Q：单层神经网络有万有逼近性，为什么神经网络趋向于增加隐藏层的层数（深度）而不是神经元的个数（宽度）？</p>
<p>A：理论上，单层感知机具有万有逼近性，可以拟合任何函数；实际上，优化算法解不了。</p>
<p><img src="https://testingcf.jsdelivr.net/gh/WanpengXu/myPicGo//img/202212032025502.png" style="zoom:33%;" /></p>
<p>两种神经网络，左侧为“浅度学习”的神经网络，右侧为深度学习的神经网络。理论上，二者的拟合能力相同；但实际上，左侧的网络特别容易overfitting，因为所有神经元都是并行的，不容易迭代到理论解。</p>
<p>直观的例子：</p>
<p><img src="https://testingcf.jsdelivr.net/gh/WanpengXu/myPicGo//img/202212032025154.png" style="zoom:33%;" /></p>
<p>让每层学出部分特征，如第一层学个耳朵/嘴，第二层学个头等等。</p>
<h2 id="过拟合和欠拟合"><a href="#过拟合和欠拟合" class="headerlink" title="过拟合和欠拟合"></a>过拟合和欠拟合</h2><p><img src="https://testingcf.jsdelivr.net/gh/WanpengXu/myPicGo//img/202212032037721.png" alt=""></p>
<p><img src="https://testingcf.jsdelivr.net/gh/WanpengXu/myPicGo//img/202212032039687.png" alt=""></p>
<p>模型容量：参数数量+参数值的范围</p>
<p>理论依据：VC维，在深度学习中很难计算</p>
<h3 id="QA-3"><a href="#QA-3" class="headerlink" title="QA"></a>QA</h3><p>Q：SVM相对于神经网络的缺点</p>
<p>A：1. SVM使用kernel，很难达到百万级</p>
<ol>
<li>可调整的参数少</li>
<li>神经网络更灵活，可编程性强</li>
</ol>
<p>Q：K折交叉验证在深度学习中是否常用？</p>
<p>A：K折交叉验证确实在深度学习中用的不多。K折交叉验证主要解决机器学习中数据集太小的情况，深度学习的数据集很大，所以很少用。</p>
<h2 id="权重衰减"><a href="#权重衰减" class="headerlink" title="权重衰减"></a>权重衰减</h2><p>一种处理过拟合的方法</p>
<p>模型过拟合，是因为其复杂度太高，即<strong>模型容量太大</strong>，所以我们有两种方法处理过拟合。</p>
<ol>
<li>减少参数数量 </li>
<li><strong>限制参数值的范围</strong>，如$|\boldsymbol{w}|^2 \leq \theta$</li>
</ol>
<p>在模型结构上做出创新不是一件容易的事，所以我们应该先在现有的模型上找到更优的模型参数。</p>
<p>因此，我们可以使用罚函数法限制模型参数的范围，即</p>
<p>形式化原问题</p>
<script type="math/tex; mode=display">
\begin{aligned}
\underset{\boldsymbol{w}}{\min}\ &\ell(\boldsymbol{w}, b)\\
\text{s.t.}\ &\|\boldsymbol{w}\|^2 \leq \theta
\end{aligned}</script><p>应用罚函数法</p>
<script type="math/tex; mode=display">
\begin{aligned}
\underset{\boldsymbol{w}}{\min}\ &\ell(\boldsymbol{w}, b) + \frac{\lambda}{2}\|\boldsymbol{w}\|^2
\end{aligned}</script><p>反向传播更新参数时</p>
<script type="math/tex; mode=display">
\begin{aligned}
\frac{\partial}{\partial \boldsymbol{w}}\left(\ell(\boldsymbol{w}, b)+\frac{\lambda}{2}\|\boldsymbol{w}\|^2\right) &= \frac{\partial \ell(\boldsymbol{w}, b)}{\partial \boldsymbol{w}}+\lambda \boldsymbol{w} \\
\boldsymbol{w}_{t+1} &\leftarrow \boldsymbol{w}_{t} - \eta \frac{\partial}{\partial \boldsymbol{w}_t}\left(\ell(\boldsymbol{w}_t, b_t) + \frac{\lambda}{2}\|\boldsymbol{w}_t\|^2\right) \\
\boldsymbol{w}_{t+1} &\leftarrow (1-\eta\lambda)\boldsymbol{w}_t - \eta \frac{\partial \ell(\boldsymbol{w}_t, b_t)}{\partial \boldsymbol{w}_t}
\end{aligned}</script><p>未增加“罚”（penalty）时，权重迭代公式为</p>
<script type="math/tex; mode=display">
\boldsymbol{w}_{t+1} \leftarrow \boldsymbol{w}_t - \eta \frac{\partial \ell(\boldsymbol{w}_t, b_t)}{\partial \boldsymbol{w}_t}</script><p>通常，$\eta\lambda&lt;1$。因此，权重$\boldsymbol{w}_t$<strong>在梯度下降前会衰减一次</strong>，所以罚函数法在深度学习中就称为权重衰减。</p>
<p>Note. 一般不对$b$做权重衰减，因为效果不大。</p>
<h3 id="QA-4"><a href="#QA-4" class="headerlink" title="QA"></a>QA</h3><p>Q：为什么限制参数值的范围可以降低复杂度？</p>
<p>A：当参数的范围很大时，可能会出现下图的“陡峭”情况。（想象一下$y=\text{big number}\cdot \sin(x)$的泰勒展开式，次数为1的项和次数为7的项，它们的范围就很大）</p>
<p><img src="https://testingcf.jsdelivr.net/gh/WanpengXu/myPicGo/img/202212052109290.png" alt=""></p>
<p>所以限制参数的范围，即限制拟合函数必须“平滑”（类似PhotoShop的钢笔工具，把锚点相互离得很近），所以就减少了复杂性。</p>
<p>Q：L2 norm的符号？</p>
<p>A：L2 norm（二范数）的符号应该是$|\boldsymbol{w}|_2$，表示$\sqrt{\sum_{i = 1}^{m}w_i^2}$。所以$|w|_2^2 = \sum_{i = 1}^{m}w_i^2$，即分量的平方和。对于向量来说，二范数就是默认的范数。所以对向量，有$|w|_2 = |w|$，$|w|_2^2 = |w|^2$。</p>
<p>Q：假如真正的$\boldsymbol{w}$就是比较大，那加入参数衰减令参数减小，会有反作用吗？</p>
<p>A：因为我们获取到的数据是真实数据的一部分，所以肯定会有噪音，$\lambda$的作用就是处理这些噪音；</p>
<p>假设没有噪音，那么就不会train到奇怪的地方，就不需要参数衰减了；</p>
<p>（我个人觉得是有的，但是与他所对抗噪音带来的好处比，可以忽略它的反作用）</p>
<h2 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h2><h3 id="motivation"><a href="#motivation" class="headerlink" title="motivation"></a>motivation</h3><p>一个好的模型要对输入数据的扰动“鲁棒”。</p>
<p>就像下面这张High School Graduation dropout rate（高中毕业辍学率）一样，淘汰掉一部分输入的学生，可以让输出的结果更优。</p>
<p><img src="https://testingcf.jsdelivr.net/gh/WanpengXu/myPicGo/img/202212061953464.png" style="zoom: 50%;" /></p>
<h3 id="process"><a href="#process" class="headerlink" title="process"></a>process</h3><p>在层间“无偏差地”加入噪音，等价于对数据进行一个Tikhonov正则。</p>
<p>“无偏差”，即对$\boldsymbol{x}$加入噪音，形成$\boldsymbol{x}^\prime$，使得</p>
<script type="math/tex; mode=display">
\mathbb{E}(\boldsymbol{x}^\prime) = \boldsymbol{x}</script><p><strong>Note.</strong> 这里是element-wise相等，不是说“一个期望等于一个向量”，而是说“$x^\prime$的每个元素的期望与$\boldsymbol{x}$的对应元素相等”（重载的等于号）</p>
<p>Dropout对每个元素做如下扰动</p>
<script type="math/tex; mode=display">
x_i^{\prime}= 
\begin{cases}
0 & \text { with probablity } p \\
\frac{x_i}{1-p} & \text { otherise }
\end{cases}</script><p>使得</p>
<script type="math/tex; mode=display">
\mathbb{E}(x^\prime_i) = p \times 0 + (1-p) \cdot \frac{x_i}{1-p} = x_i</script><p>即</p>
<script type="math/tex; mode=display">
\mathbb{E}(\boldsymbol{x}^\prime) = \boldsymbol{x}</script><p>通常在层间使用Dropout，如</p>
<p><img src="https://testingcf.jsdelivr.net/gh/WanpengXu/myPicGo/img/202212062120483.jpg" alt=""></p>
<p>注意.</p>
<p>droupout只在train时生效，使$h \gets \operatorname{dropout}(h)$</p>
<p>在test/inference时关闭，即$h \gets h$，可以保证输出的确定性。</p>
<h3 id="QA-5"><a href="#QA-5" class="headerlink" title="QA"></a>QA</h3>
        </div>

        
            <section class="post-copyright">
                
                    <p class="copyright-item">
                        <span>Author:</span>
                        <span>许</span>
                    </p>
                
                
                    <p class="copyright-item">
                        <span>Permalink:</span>
                        <span><a href="https://xuwp.top/Dive-into-Deep-Learning.html">https://xuwp.top/Dive-into-Deep-Learning.html</a></span>
                    </p>
                
                
                    <p class="copyright-item">
                        <span>License:</span>
                        <span>Copyright (c) 2022 <a target="_blank" rel="noopener" href="http://creativecommons.org/licenses/by-nc/4.0/">CC-BY-NC-4.0</a> LICENSE</span>
                    </p>
                
                

            </section>
        
        <section class="post-tags">
            <div>
                <span>Tag(s):</span>
                <span class="tag">
                    
                    
                        <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"># 深度学习</a>
                    
                        
                </span>
            </div>
            <div>
                <a href="javascript:window.history.back();">back</a>
                <span>· </span>
                <a href="/">home</a>
            </div>
        </section>
        <section class="post-nav">
            
                <a class="prev" rel="prev" href="/Zhihua-Zhou-Machine-Learning-Prelim.html">周志华. 机器学习初步(2022秋)</a>
            
            
            <a class="next" rel="next" href="/MACHINE-LEARNING-2021-SPRING-HOMEWORKS.html">Hung-yi Lee. Machine Learning 2021 HWs</a>
            
        </section>


    </article>
</div>

            </div>
            <footer id="footer" class="footer">
    <div class="copyright">
        <span>Stay hungry. Stay foolish.</span>
    </div>
</footer>

    </div>
</body>

</html>