<!DOCTYPE html>
<html lang="default">

<head>
    <meta charset="UTF-8">
<meta name="viewport"
      content="width=device-width, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">

    <meta name="baidu-site-verification" content="code-dlFXc2180h" />


    <!-- <meta name="google-site-verification" content="qlTU_n7RT-JIFfLGMsgqxNsTEkVlsWnOfvoPzzHzSR0" /> -->
    <meta name="google-site-verification" content="wNw6ZIFTz_hFtiJ2w108gXZcLcg3e8tLeSGIC36Wn_M" />


    <meta name="author" content="许">





<title>机器学习基础 | 须臾所学之野</title>



    <link rel="icon" href="/favicon.png">




    <!-- stylesheets list from _config.yml -->
    
    <link rel="stylesheet" href="/css/style.css">
    



    <!-- scripts list from _config.yml -->
    
    <script src="/js/script.js"></script>
    
    <script src="/js/tocbot.min.js"></script>
    






    
    
        <script>
    MathJax = {
        tex: {
            // 行内公式标志
            inlineMath: [
                ['$', '$']
            ],
            // 块级公式标志
            displayMath: [
                ['$$', '$$']
            ],
            // 下面两个主要是支持渲染某些公式，可以自己了解
            processEnvironments: true,
            processRefs: true,
        },
        options: {
            // 跳过渲染的标签
            skipHtmlTags: ['noscript', 'style', 'textarea', 'pre', 'code'],
            // 跳过mathjax处理的元素的类名，任何元素指定一个类 tex2jax_ignore 将被跳过，多个累=类名'class1|class2'
            ignoreHtmlClass: 'tex2jax_ignore',
        },
        // 这里可能是因为我的MathJax2仍有残留，导致行间公式被渲染成了type="math/tex"，所以要用这种2、3版本混合查找方式进行渲染
        // 这样可能效率低，有机会再改。
        options: {
            renderActions: {
                /* add a new named action not to override the original 'find' action */
                find_script_mathtex: [10, function (doc) {
                    for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
                        const display = !!node.type.match(/; *mode=display/);
                        const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
                        const text = document.createTextNode('');
                        node.parentNode.replaceChild(text, node);
                        math.start = {
                            node: text,
                            delim: '',
                            n: 0
                        };
                        math.end = {
                            node: text,
                            delim: '',
                            n: 0
                        };
                        doc.math.push(math);
                    }
                }, '']
            }
        },
        svg: {
            fontCache: 'global',
        },
    };
</script>

<script type="text/javascript" id="MathJax-script" async
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>

<!-- <script type="text/javascript" id="MathJax-script" async
    src="https://cdn.bootcdn.net/ajax/libs/mathjax/3.1.2/es5/tex-mml-chtml.min.js">
</script> -->
    



<meta name="generator" content="Hexo 6.3.0"></head>

<body>
    <script>
        // this function is used to check current theme before page loaded.
        (() => {
            const currentTheme = window.localStorage && window.localStorage.getItem('theme') || '';
            const isDark = currentTheme === 'dark';
            const pagebody = document.getElementsByTagName('body')[0]
            if (isDark) {
                pagebody.classList.add('dark-theme');
                // mobile
                document.getElementById("mobile-toggle-theme").innerText = "· Dark"
            } else {
                pagebody.classList.remove('dark-theme');
                // mobile
                document.getElementById("mobile-toggle-theme").innerText = "· Light"
            }
        })();
    </script>

    <div class="wrapper">
        <header>
    <nav class="navbar">
        <div class="container">
            <div class="navbar-header header-logo"><a href="/">Xuwp&#39;s Blog</a></div>
            <div class="menu navbar-right">
                
                    <a class="menu-item" href="/archives">Posts</a>
                
                    <a class="menu-item" href="/category">Categories</a>
                
                    <a class="menu-item" href="/tag">Tags</a>
                
                    <a class="menu-item" href="/about">About</a>
                
                    <a class="menu-item" href="/link">Links</a>
                
                    <a class="menu-item" href="https://xuwp.top/cv-en/">Resume</a>
                
                <input id="switch_default" type="checkbox" class="switch_default">
                <label for="switch_default" class="toggleBtn"></label>
            </div>
        </div>
    </nav>

    
    <nav class="navbar-mobile" id="nav-mobile">
        <div class="container">
            <div class="navbar-header">
                <div>
                    <a href="/">Xuwp&#39;s Blog</a><a id="mobile-toggle-theme">·&nbsp;Light</a>
                </div>
                <div class="menu-toggle" onclick="mobileBtn()">&#9776; Menu</div>
            </div>
            <div class="menu" id="mobile-menu">
                
                    <a class="menu-item" href="/archives">Posts</a>
                
                    <a class="menu-item" href="/category">Categories</a>
                
                    <a class="menu-item" href="/tag">Tags</a>
                
                    <a class="menu-item" href="/about">About</a>
                
                    <a class="menu-item" href="/link">Links</a>
                
                    <a class="menu-item" href="https://xuwp.top/cv-en/">Resume</a>
                
            </div>
        </div>
    </nav>

</header>
<script>
    var mobileBtn = function f() {
        var toggleMenu = document.getElementsByClassName("menu-toggle")[0];
        var mobileMenu = document.getElementById("mobile-menu");
        if(toggleMenu.classList.contains("active")){
           toggleMenu.classList.remove("active")
            mobileMenu.classList.remove("active")
        }else{
            toggleMenu.classList.add("active")
            mobileMenu.classList.add("active")
        }
    }
</script>
            <div class="main">
                <div class="container">
    
    
        <div class="post-toc">
    <div class="tocbot-list">
    </div>
    <div class="tocbot-list-menu">
        <a class="tocbot-toc-expand" onclick="expand_toc()">Expand all</a>
        <a onclick="go_top()">Back to top</a>
        <a onclick="go_bottom()">Go to bottom</a>
    </div>
</div>

<script>
    var tocbot_timer;
    var DEPTH_MAX = 6;    // 为 6 时展开所有
    var tocbot_default_config = {
        tocSelector: '.tocbot-list',
        contentSelector: '.post-content',
        headingSelector: 'h1, h2, h3, h4, h5',
        orderedList: false,
        scrollSmooth: true,
        onClick: extend_click,
    };

    function extend_click() {
        clearTimeout(tocbot_timer);
        tocbot_timer = setTimeout(function () {
            tocbot.refresh(obj_merge(tocbot_default_config, { hasInnerContainers: true }));
        }, 420); // 这个值是由 tocbot 源码里定义的 scrollSmoothDuration 得来的
    }

    document.ready(function () {
        tocbot.init(obj_merge(tocbot_default_config, { collapseDepth: 1 }));
    });

    function expandToc() {
        var b = document.querySelector('.tocbot-toc-expand');
        var expanded = b.getAttribute('data-expanded');
        expanded ? b.removeAttribute('data-expanded') : b.setAttribute('data-expanded', true);
        tocbot.refresh(obj_merge(tocbot_default_config, { collapseDepth: expanded ? 1 : DEPTH_MAX }));
        b.innerText = expanded ? 'Expand all' : 'Collapse all';
    }

    function go_top() {
        window.scrollTo(0, 0);
    }

    function go_bottom() {
        window.scrollTo(0, document.body.scrollHeight);
    }

    function obj_merge(target, source) {
        for (var item in source) {
            if (source.hasOwnProperty(item)) {
                target[item] = source[item];
            }
        }
        return target;
    }
</script>
    

    
    <article class="post-wrap">
        <header class="post-header">
            <h1 class="post-title">机器学习基础</h1>
            
                <div class="post-meta">
                    
                        Author: <a itemprop="author" rel="author" href="/">许</a>
                    

                    
                        <span class="post-time">
                        Date: <a href="#">April 14, 2022&nbsp;&nbsp;1:25:07</a>
                        </span>
                    
                    
                        <span class="post-category">
                    Category:
                            
                                <a href="/categories/AI/">AI</a>
                            
                        </span>
                    
                </div>
            
        </header>

        <div class="post-content">
            <h1 id="BP神经网络"><a href="#BP神经网络" class="headerlink" title="BP神经网络"></a>BP神经网络</h1><h2 id="神经元模型"><a href="#神经元模型" class="headerlink" title="神经元模型"></a>神经元模型</h2><p>神经网络中最基本的成分是神经元模型。神经元模型源自“仿生”的思想，是对生物神经元的一种模仿。</p>
<p>首先，看一下生物学上的神经元。</p>
<p><img src="https://testingcf.jsdelivr.net/gh/WanpengXu/myPicGo/img/202204140130164.png" alt=""></p>
<p>只观察主要部分，我们可以发现生物学上的神经元由三部分组成：细胞体、细胞核、突触。突触可以分为树突和轴突，树突是神经元的输入通道，轴突用于传递自神经元发出的冲动。当神经递质含量满足要求时，神经元便会兴奋、传递冲动。</p>
<p>对生物学上的神经元进行简单的抽象，便得到了M-P神经元模型。</p>
<p><img src="https://testingcf.jsdelivr.net/gh/WanpengXu/myPicGo/img/202204140140970.png" alt=""></p>
<p>在这个模型中，神经元按权重接收来自其他 $n$ 个神经元传递的输入信号，这些总输入值与神经元的阈值进行比较后，通过“激活函数”处理以产生输出，即。<script type="math/tex">y=f(\sum\limits_{i=1}^{n}w_ix_i-\theta)=f(\boldsymbol{w}^{\mathrm{T}}\boldsymbol{x}-\theta)</script></p>
<p>如何选定激活函数呢？典型的激活函数有以下两种：</p>
<p><img src="https://testingcf.jsdelivr.net/gh/WanpengXu/myPicGo/img/202204140141559.png" alt=""></p>
<p>因阶跃函数具有不连续、不平滑的特性，因此常采用$S$型函数，$Sigmoid$函数是最常用的$S$型函数，需要神经元的输出区间为$[-1,1]$时，$S$型函数可以选为<strong>双曲正切函数</strong></p>
<script type="math/tex; mode=display">
f(x)=\frac{1-e^{-\alpha x}}{1+e^{-\alpha x}}</script><p>$Sigmoid$函数也存在缺点，当输入的绝对值大于某个阈值时会过快进入饱和状态，使梯度趋于0，模型收敛缓慢。</p>
<p>$ReLU$函数在一些现代神经网络中也很常用，它是一个很简单的分段线性函数</p>
<script type="math/tex; mode=display">
f(x)=\left\{
\begin{aligned}
& 0 &,x<0\\
& x &,x\geq 0\\
\end{aligned}
\right.</script><p>尽管形式简单，但其在卷积神经网络中效果很好。</p>
<h2 id="感知机与多层网络"><a href="#感知机与多层网络" class="headerlink" title="感知机与多层网络"></a>感知机与多层网络</h2><p>感知机（Perceptron）：两层神经元，用于实现“与”、“或”、“非”运算。</p>
<p>注意：<strong>感知机不能实现异或运算！</strong></p>
<p><img src="https://testingcf.jsdelivr.net/gh/WanpengXu/myPicGo/img/202204140157116.png" alt=""></p>
<p>对于上图所示的感知机，选用阶跃函数，有：</p>
<script type="math/tex; mode=display">
y=\operatorname{sgn}(\boldsymbol{w}^{\mathrm{T}}\boldsymbol{x}-\theta)=\left\{\begin{array}{rcl}
1,& {\boldsymbol{w}^{\mathrm{T}}\boldsymbol{x} -\theta\geq 0}\\
0,& {\boldsymbol{w}^{\mathrm{T}}\boldsymbol{x} -\theta < 0}\\
\end{array} \right.</script><p>对二维空间进行简单划分即可分别得到与、或、非运算的激活函数，他们不是唯一的。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">$x_1$</th>
<th style="text-align:center">$x_2$</th>
<th style="text-align:center">$y=f(1⋅x_1+1⋅x_2-2)$</th>
<th style="text-align:center">$y=f(1⋅x_1+1⋅x_2-0.5)$</th>
<th style="text-align:center">$y=f(-0.6⋅x_1+0⋅x_2+0.5)$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">1</td>
</tr>
<tr>
<td style="text-align:center">0</td>
<td style="text-align:center">1</td>
<td style="text-align:center">0</td>
<td style="text-align:center">1</td>
<td style="text-align:center">1</td>
</tr>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">1</td>
<td style="text-align:center">0</td>
</tr>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">1</td>
<td style="text-align:center">1</td>
<td style="text-align:center">1</td>
<td style="text-align:center">0</td>
</tr>
</tbody>
</table>
</div>
<p>更一般地，我们可以给定数据集，让神经网络通过学习以确定权重<script type="math/tex">\omega_i(i=1,2,\dots,n)</script>和阈值$\theta$。若将阈值$\theta$视作固定输入为$-1.0$的哑结点所对应的连接权重$\omega_{n+1}$，则可将权重<script type="math/tex">\omega_i(i=1,2,\dots,n)</script>和阈值$\theta$的学习统一为权重<script type="math/tex">\omega_i(i=1,2,\dots,n+1)</script>的学习。</p>
<script type="math/tex; mode=display">
\begin{aligned}
\boldsymbol{w}^\mathrm{T}\boldsymbol{x_i}-\theta&=\sum \limits_{j=1}^n w_jx_j+x_{n+1}\cdot w_{n+1}\\
&=\sum \limits_{j=1}^{n+1}w_jx_j\\
&=\boldsymbol{w}^{\mathrm{T}}\boldsymbol{x_i}
 \end{aligned}</script><p><img src="https://testingcf.jsdelivr.net/gh/WanpengXu/myPicGo/img/202204171728083.png" alt=""></p>
<p>感知机的学习规则为：</p>
<script type="math/tex; mode=display">
\omega_i\ \gets\ \omega_i + \mathrm{\Delta}\omega_i,\   \mathrm{\Delta}\omega_i = \eta\left(y\ -\ \hat{y}\right)x_i</script><p>其中 $\eta \in (0,1)$，称为学习率，属于超参数。</p>
<p>学习过程很简单：若感知机对训练样例 $(\boldsymbol x,y)$ 中 $\boldsymbol x$ 预测的结果 $\hat{y}$ 等于 $y$ ，则感知机不发生变化。否则，感知机根据错误的程度（残差）进行权重调整。</p>
<p>接下来说明一下为什么两层神经元的感知机不能实现异或运算。首先来看一下与、或、非、异或在二维空间中应怎样被划分。</p>
<p><img src="https://testingcf.jsdelivr.net/gh/WanpengXu/myPicGo/img/202204140225846.png" alt=""></p>
<p>显然，异或问题需要两条直线才能将平面划分为两类模式，而与、或、非只需要一条直线即可。</p>
<p>对于一个有两类模式的空间，若存在一个超平面（$n$ 维欧氏空间中维度等于 $n-1$ 的线性子空间，eg. 平面中的直线）能将他们分开，则称这两类模式是线性可分的。若两类模式是线性可分的，则感知机的学习过程一定会收敛，得到权向量<script type="math/tex">\boldsymbol \omega=(\omega_1;\omega_2;\dots;\omega_{n+1})</script> .否则感知机会发生振荡，无法求得合适解。</p>
<p>要解决线性可分问题，需要使用多层神经网络。对于异或问题，我们只需要先对与、或问题中的直线使用线性规划</p>
<script type="math/tex; mode=display">
\left\{
\begin{aligned}
x_1+x_2-0.5\geq0 \\
x_1+x_2-1.5\leq0 \\
\end{aligned}
\right.</script><p>由简单的谓词逻辑：$P\oplus Q = \neg (P \wedge Q) \wedge (P \vee Q) $，接着对$y^{(1)}=f(x_1+x_2-0.5)$和$y^{(2)}=f(-x_1-x_2+1.5)$使用与问题的激活函数即可。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">$x_1$</th>
<th style="text-align:center">$x_2$</th>
<th style="text-align:center">$y^{(1)}=f(x_1+x_2-0.5)$</th>
<th style="text-align:center">$y^{(2)}=f(-x_1-x_2+1.5)$</th>
<th style="text-align:center">$y=y^{(1)}+y^{(2)}-1.5$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">1</td>
<td style="text-align:center">0</td>
</tr>
<tr>
<td style="text-align:center">0</td>
<td style="text-align:center">1</td>
<td style="text-align:center">1</td>
<td style="text-align:center">1</td>
<td style="text-align:center">1</td>
</tr>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">0</td>
<td style="text-align:center">1</td>
<td style="text-align:center">1</td>
<td style="text-align:center">1</td>
</tr>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">1</td>
<td style="text-align:center">1</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
</tr>
</tbody>
</table>
</div>
<p>最终得到二层感知机如下，输入层与输出层之间的其他神经元层称作隐层。</p>
<p><img src="https://testingcf.jsdelivr.net/gh/WanpengXu/myPicGo/img/202204140300967.png" alt=""></p>
<p>更一般地，常见的神经网络形如：</p>
<p><img src="https://testingcf.jsdelivr.net/gh/WanpengXu/myPicGo/img/202204140302432.png" alt=""></p>
<p>多层前馈神经网络（multi-layer feedforward neural networks）：仅在相邻层间全互连、且包含隐含层的神经网络</p>
<p>注：“前馈”并不意味着网络中信号不能向后传，而是指网络拓扑结构上不存在环或回路。</p>
<h2 id="误差反向传播算法"><a href="#误差反向传播算法" class="headerlink" title="误差反向传播算法"></a>误差反向传播算法</h2><p>欲训练一个多层前馈神经网络，我们需要更强大的算法，这其中最成功的神经网络算法即为误差反向传播算法（BackPropagation, BP ），而且它不仅可用于训练多层前馈神经网络，还可用于其他神经网络。通常，“BP网络”指使用BP算法训练的多层前馈神经网络。</p>
<p>下面我们来推导一下BP算法。</p>
<p>首先，给出训练集$D=\{(\boldsymbol x_1, \boldsymbol y_1 ), (\boldsymbol x_2, \boldsymbol y_2 ), …, (\boldsymbol x_m, \boldsymbol y_m )\}, \boldsymbol x_i∈\mathbb{R}^d, \boldsymbol y_i∈\mathbb{R}^l$</p>
<p>给出如下多层前馈神经网络</p>
<p><img src="https://testingcf.jsdelivr.net/gh/WanpengXu/myPicGo/img/202204140306715.png" alt=""></p>
<p>其变量意义标注如下表。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center"><strong>变量</strong></th>
<th style="text-align:center"><strong>意义</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">$d$</td>
<td style="text-align:center">输入神经元个数</td>
</tr>
<tr>
<td style="text-align:center">$l$</td>
<td style="text-align:center">输出神经元个数</td>
</tr>
<tr>
<td style="text-align:center">$q$</td>
<td style="text-align:center">隐层神经元个数</td>
</tr>
<tr>
<td style="text-align:center">$\theta_j$</td>
<td style="text-align:center">输出层第$j$个神经元的阈值</td>
</tr>
<tr>
<td style="text-align:center">$γ_h$</td>
<td style="text-align:center">隐层第$j$个神经元的阈值</td>
</tr>
<tr>
<td style="text-align:center">$v_{ih}$</td>
<td style="text-align:center">输出层第$i$个神经元与隐层第$h$个神经元间的连接权</td>
</tr>
<tr>
<td style="text-align:center">$ω_{hj}$</td>
<td style="text-align:center">隐层第$h$个神经元与输出层第$j$神经元间的连接权</td>
</tr>
<tr>
<td style="text-align:center">$\alpha_h$</td>
<td style="text-align:center">隐层第$h$个神经元接收到的输入</td>
</tr>
<tr>
<td style="text-align:center">$\beta_j$</td>
<td style="text-align:center">输出层第$j$个神经元接收到的输入</td>
</tr>
<tr>
<td style="text-align:center">$b_h$</td>
<td style="text-align:center">隐层第$h$个神经元的输出</td>
</tr>
</tbody>
</table>
</div>
<p>设其隐层和输出层神经元使用$Sigmoid$函数。</p>
<p>对训练例$(\boldsymbol x_k, \boldsymbol y_k )$，假定神经网络的输出为$y ̂_k=(y ̂_1^k,y ̂_2^k,…,y ̂_l^k)$，其中$y ̂_j^k=f(β_j-θ_j)$，</p>
<p>则网络在$(\boldsymbol x_k, \boldsymbol y_k )$上的均方误差为：</p>
<script type="math/tex; mode=display">
E_k=\frac{1}{2} \sum_{j=1}^{l}(\hat{y}_1^k-y_1^k )^2</script><p>需要确定的参数：</p>
<ul>
<li><p>输入层到隐层的$d×q$个权值</p>
</li>
<li><p>隐层到输出层的$q×l$个权值</p>
</li>
<li><p>隐层神经元的$q$个阈值</p>
</li>
<li><p>输出层神经元的$l$个阈值</p>
</li>
</ul>
<p>共$d×q+q×l+q+l=(d+l+1)×q+l$个</p>
<p>广义感知机学习规则：任意参数$v$的更新估计式</p>
<script type="math/tex; mode=display">
v←v+\Delta v</script><p>对于参数$\omega_{hj}$，以目标的负梯度方向对其进行调整：</p>
<script type="math/tex; mode=display">
\Delta \omega_{hj}=-\eta\frac{\partial E_k}{\partial \omega_{hj}}</script><p>其中</p>
<script type="math/tex; mode=display">
\frac{\partial E_k}{\partial \omega_{hj}}=\frac{\partial E_k}{\partial \hat{y}_j^k}\cdot\frac{\partial \hat{y}_j^k}{\partial \beta_j}\cdot\frac{\partial \beta_j}{\partial \omega_{hj}}</script><p>由$\beta_j$的定义，有：</p>
<script type="math/tex; mode=display">
\frac{\partial \beta_j}{\partial \omega_{hj}}=b_h</script><p>$Sigmoid$的导函数：</p>
<script type="math/tex; mode=display">
f^{'}(x)=f(x)(1-f(x))</script><script type="math/tex; mode=display">
\begin{aligned}
g_j&=-\frac{\partial {E_k}}{\partial{\hat{y}_j^k}} \cdot \frac{\partial{\hat{y}_j^k}}{\partial{\beta_j}}
\\&=-( \hat{y}_j^k-y_j^k ) f ^{\prime} (\beta_j-\theta_j)
\\&=\hat{y}_j^k(1-\hat{y}_j^k)(y_j^k-\hat{y}_j^k)
\end{aligned}</script><p>所以</p>
<script type="math/tex; mode=display">
\Delta \omega_{hj}=\eta g_j b_h</script><p>对于参数$\theta_j$：</p>
<script type="math/tex; mode=display">\Delta \theta_j = -\eta g_j</script><p>因为</p>
<script type="math/tex; mode=display">\Delta \theta_j = -\eta \cfrac{\partial E_k}{\partial \theta_j}</script><p>又</p>
<script type="math/tex; mode=display">
\begin{aligned}    
\cfrac{\partial E_k}{\partial \theta_j} &= \cfrac{\partial E_k}{\partial \hat{y}_j^k} \cdot\cfrac{\partial \hat{y}_j^k}{\partial \theta_j} \\
&= \cfrac{\partial E_k}{\partial \hat{y}_j^k} \cdot\cfrac{\partial [f(\beta_j-\theta_j)]}{\partial \theta_j} \\
&=\cfrac{\partial E_k}{\partial \hat{y}_j^k} \cdot f^{\prime}(\beta_j-\theta_j) \times (-1) \\
&=\cfrac{\partial E_k}{\partial \hat{y}_j^k} \cdot f\left(\beta_{j}-\theta_{j}\right)\times\left[1-f\left(\beta_{j}-\theta_{j}\right)\right]  \times (-1) \\
&=\cfrac{\partial E_k}{\partial \hat{y}_j^k} \cdot \hat{y}_j^k\left(1-\hat{y}_j^k\right)  \times (-1) \\
&=\cfrac{\partial\left[ \cfrac{1}{2} \sum\limits_{j=1}^{l}\left(\hat{y}_{j}^{k}-y_{j}^{k}\right)^{2}\right]}{\partial \hat{y}_{j}^{k}} \cdot \hat{y}_j^k\left(1-\hat{y}_j^k\right) \times (-1)  \\
&=\cfrac{1}{2}\times 2(\hat{y}_j^k-y_j^k)\times 1 \cdot\hat{y}_j^k\left(1-\hat{y}_j^k\right)  \times (-1) \\
&=(y_j^k-\hat{y}_j^k)\hat{y}_j^k\left(1-\hat{y}_j^k\right)  \\
&= g_j
\end{aligned}</script><p>所以</p>
<script type="math/tex; mode=display">\Delta \theta_j = -\eta \cfrac{\partial E_k}{\partial \theta_j}=-\eta g_j</script><p>对于参数$v_{ih}$：</p>
<script type="math/tex; mode=display">
\Delta v_{ih} = \eta e_h x_i</script><p>因为</p>
<script type="math/tex; mode=display">\Delta v_{ih} = -\eta \cfrac{\partial E_k}{\partial v_{ih}}</script><p>又</p>
<script type="math/tex; mode=display">
\begin{aligned}    
\cfrac{\partial E_k}{\partial v_{ih}} &= \sum_{j=1}^{l} \cfrac{\partial E_k}{\partial \hat{y}_j^k} \cdot \cfrac{\partial \hat{y}_j^k}{\partial \beta_j} \cdot \cfrac{\partial \beta_j}{\partial b_h} \cdot \cfrac{\partial b_h}{\partial \alpha_h} \cdot \cfrac{\partial \alpha_h}{\partial v_{ih}} \\
&= \sum_{j=1}^{l} \cfrac{\partial E_k}{\partial \hat{y}_j^k} \cdot \cfrac{\partial \hat{y}_j^k}{\partial \beta_j} \cdot \cfrac{\partial \beta_j}{\partial b_h} \cdot \cfrac{\partial b_h}{\partial \alpha_h} \cdot x_i \\ 
&= \sum_{j=1}^{l} \cfrac{\partial E_k}{\partial \hat{y}_j^k} \cdot \cfrac{\partial \hat{y}_j^k}{\partial \beta_j} \cdot \cfrac{\partial \beta_j}{\partial b_h} \cdot f^{\prime}(\alpha_h-\gamma_h) \cdot x_i \\
&= \sum_{j=1}^{l} \cfrac{\partial E_k}{\partial \hat{y}_j^k} \cdot \cfrac{\partial \hat{y}_j^k}{\partial \beta_j} \cdot w_{hj} \cdot f^{\prime}(\alpha_h-\gamma_h) \cdot x_i \\
&= \sum_{j=1}^{l} (-g_j) \cdot w_{hj} \cdot f^{\prime}(\alpha_h-\gamma_h) \cdot x_i \\
&= -f^{\prime}(\alpha_h-\gamma_h) \cdot \sum_{j=1}^{l} g_j \cdot w_{hj}  \cdot x_i\\
&= -b_h(1-b_h) \cdot \sum_{j=1}^{l} g_j \cdot w_{hj}  \cdot x_i \\
&= -e_h \cdot x_i
\end{aligned}</script><p>所以</p>
<script type="math/tex; mode=display">\Delta v_{ih} =-\eta \cfrac{\partial E_k}{\partial v_{ih}} =\eta e_h x_i</script><p>对于$\Delta \gamma_h$</p>
<script type="math/tex; mode=display">\Delta \gamma_h= -\eta e_h</script><p>因为</p>
<script type="math/tex; mode=display">\Delta \gamma_h = -\eta \cfrac{\partial E_k}{\partial \gamma_h}</script><p>又</p>
<script type="math/tex; mode=display">
\begin{aligned}    
\cfrac{\partial E_k}{\partial \gamma_h} &= \sum_{j=1}^{l} \cfrac{\partial E_k}{\partial \hat{y}_j^k} \cdot \cfrac{\partial \hat{y}_j^k}{\partial \beta_j} \cdot \cfrac{\partial \beta_j}{\partial b_h} \cdot \cfrac{\partial b_h}{\partial \gamma_h} \\
&= \sum_{j=1}^{l} \cfrac{\partial E_k}{\partial \hat{y}_j^k} \cdot \cfrac{\partial \hat{y}_j^k}{\partial \beta_j} \cdot \cfrac{\partial \beta_j}{\partial b_h} \cdot f^{\prime}(\alpha_h-\gamma_h) \cdot (-1) \\
&= -\sum_{j=1}^{l} \cfrac{\partial E_k}{\partial \hat{y}_j^k} \cdot \cfrac{\partial \hat{y}_j^k}{\partial \beta_j} \cdot w_{hj} \cdot f^{\prime}(\alpha_h-\gamma_h)\\
&= -\sum_{j=1}^{l} \cfrac{\partial E_k}{\partial \hat{y}_j^k} \cdot \cfrac{\partial \hat{y}_j^k}{\partial \beta_j} \cdot w_{hj} \cdot b_h(1-b_h)\\
&= \sum_{j=1}^{l}g_j\cdot w_{hj} \cdot b_h(1-b_h)\\
&=e_h
\end{aligned}</script><p>所以</p>
<script type="math/tex; mode=display">\Delta \gamma_h=-\eta\cfrac{\partial E_k}{\partial \gamma_h} = -\eta e_h</script><p>综上：</p>
<script type="math/tex; mode=display">
\left\{
\begin{aligned}
& \Delta \omega_{hj}=\eta g_j b_h \\
& \Delta \theta_j = -\eta \cfrac{\partial E_k}{\partial \theta_j}=-\eta g_j \\
& \Delta v_{ih} = \eta e_h x_i \\
& \Delta \gamma_h= -\eta e_h \\
\end{aligned}
\right. \\
e_h=b_h(1-b_h)\sum_{j=1}^l w_{hj}g_j</script><p>需要精细调节时，可令(1)与(2)使用$\eta_1$，(3)与(4)使用$\eta_2$</p>
<p>由此，我们可以给出标准BP算法：</p>
<p><img src="https://testingcf.jsdelivr.net/gh/WanpengXu/myPicGo/img/202204141117290.png" alt=""></p>
<p>值得注意的是，BP算法的<strong>最终目标</strong>应为最小化训练集$D$上的累计误差</p>
<script type="math/tex; mode=display">
E=\frac{1}{m} \sum_{k=1}^mE_k</script><p>但刚才的标准BP算法基于最小化均方误差$E_k$推导。</p>
<h2 id="累计误差反向传播算法"><a href="#累计误差反向传播算法" class="headerlink" title="累计误差反向传播算法"></a>累计误差反向传播算法</h2><p>标准BP算法由单个<strong>均方误差</strong>$E_k$推导而来，所以每次更新针对单个样例，更新频繁。</p>
<p>同时，同一数据集中不同样例的反向更新可能出现“抵消”现象。</p>
<p>若类似地，对之前<strong>最终目标</strong>中提到的<strong>累积误差</strong>推导更新规则，便得到了更新频率更低的累计误差反向传播算法。</p>
<p> 标准 BP 算法</p>
<script type="math/tex; mode=display">
E_k=\frac{1}{2} \sum_{j=1}^{l}(\hat{y}_1^k-y_1^k )^2</script><ul>
<li><p>每次针对单个训练样例更新权值与阈值</p>
</li>
<li><p>参数更新频繁，不同样例可能抵消，需要多次迭代</p>
</li>
</ul>
<p>累计BP算法</p>
<script type="math/tex; mode=display">
E=\frac{1}{m} \sum_{k=1}^mE_k =\frac{1}{2m} \sum_{k=1}^m\sum_{j=1}^l(\hat{y}_1^k-y_1^k)^2</script><ul>
<li><p>其优化目标是最小化整个训练集上的累计误差</p>
</li>
<li><p>读取整个训练集一遍才对参数进行更新 , 参数更新</p>
</li>
</ul>
<h2 id="过拟合"><a href="#过拟合" class="headerlink" title="过拟合"></a>过拟合</h2><p>BP网络经常出现过拟合现象。</p>
<p>过拟合（overfitting）：训练误差持续降低，测试误差上升。即model对训练集性能很好，对测试集性能很差，也就是model的generalization差。</p>
<p><img src="https://testingcf.jsdelivr.net/gh/WanpengXu/myPicGo/img/202204141132268.png" alt=""></p>
<p>偏差–方差窘境：</p>
<p><img src="https://testingcf.jsdelivr.net/gh/WanpengXu/myPicGo/img/202204141130046.png" alt=""></p>
<p>缓解过拟合现象有几种策略：</p>
<ol>
<li><p>确保训练数据集和测试数据集是independent, identical distributed(i.i.d)的；</p>
</li>
<li><p>早停（ early stopping ）：若训练集误差降低，但验证集误差升高，则停止训练；</p>
</li>
<li><p><strong>正则化（</strong> <strong>regularization</strong> <strong>）</strong>：在误差目标函数中增加一个用于描述网络复杂度的部分。例如：</p>
</li>
</ol>
<script type="math/tex; mode=display">
E=\lambda\frac{1}{m} \sum_{k=1}^mE_k +(1-\lambda)\sum_i{\omega_i^2}</script><h2 id="全局最小与局部最小"><a href="#全局最小与局部最小" class="headerlink" title="全局最小与局部最小"></a>全局最小与局部最小</h2><p>局部极小：对<script type="math/tex">\forall(\boldsymbol \omega;\theta)\in \left\{∀(\boldsymbol \omega;\theta)\ |\ ‖(\boldsymbol \omega;\theta)-(\boldsymbol \omega^∗;\theta^∗)‖ ≤ \epsilon\right\}</script>，都有<script type="math/tex">E(\boldsymbol \omega;\theta)\geq E (\boldsymbol \omega^∗;\theta^∗)</script>，则<script type="math/tex">(\boldsymbol \omega^∗;\theta^∗ )</script>为局部极小解。即梯度为$0$且$E$小于某个邻点。</p>
<p>全局极小：对<script type="math/tex">\forall(\boldsymbol \omega;\theta)</script>，都有<script type="math/tex">E(\boldsymbol \omega;\theta)\geq E (\boldsymbol \omega^∗;\theta^∗)</script>则<script type="math/tex">(\boldsymbol \omega^∗;\theta^∗ )</script>为全局极小解。</p>
<p><img src="https://testingcf.jsdelivr.net/gh/WanpengXu/myPicGo/img/202204141137926.png" alt=""></p>
<p>因为BP算法的感知机更新规则基于梯度下降，所以当误差函数有多个局部极小时，则不能找到全局极小解，此时我们称参数寻优陷入了局部极小。</p>
<p>“跳出”局部极小的常见策略：</p>
<ul>
<li><p>不同的初始参数</p>
</li>
<li><p>模拟退火</p>
</li>
<li><p>随机扰动</p>
</li>
<li><p>遗传算法</p>
</li>
<li><p>……</p>
</li>
</ul>

        </div>

        
            <section class="post-copyright">
                
                    <p class="copyright-item">
                        <span>Author:</span>
                        <span>许</span>
                    </p>
                
                
                    <p class="copyright-item">
                        <span>Permalink:</span>
                        <span><a href="https://xuwp.top/Machine-Learning.html">https://xuwp.top/Machine-Learning.html</a></span>
                    </p>
                
                
                    <p class="copyright-item">
                        <span>License:</span>
                        <span>Copyright (c) 2022 <a target="_blank" rel="noopener" href="http://creativecommons.org/licenses/by-nc/4.0/">CC-BY-NC-4.0</a> LICENSE</span>
                    </p>
                
                

            </section>
        
        <section class="post-tags">
            <div>
                <span>Tag(s):</span>
                <span class="tag">
                    
                    
                        <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"># 机器学习</a>
                    
                        
                </span>
            </div>
            <div>
                <a href="javascript:window.history.back();">back</a>
                <span>· </span>
                <a href="/">home</a>
            </div>
        </section>
        <section class="post-nav">
            
                <a class="prev" rel="prev" href="/Operating-System-Security.html">操作系统安全/信息系统安全</a>
            
            
            <a class="next" rel="next" href="/Man-in-the-middle-Attack.html">中间人攻击——DHCP耗尽攻击</a>
            
        </section>


    </article>
</div>

            </div>
            <footer id="footer" class="footer">
    <div class="copyright">
        <span>Stay hungry. Stay foolish.</span>
    </div>
</footer>

    </div>
</body>

</html>