<!DOCTYPE html>
<html lang="default">

<head>
    <meta charset="UTF-8">
<meta name="viewport"
      content="width=device-width, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">

    <meta name="baidu-site-verification" content="code-dlFXc2180h" />


    <meta name="google-site-verification" content="qlTU_n7RT-JIFfLGMsgqxNsTEkVlsWnOfvoPzzHzSR0" />
    <!-- <meta name="google-site-verification" content="wNw6ZIFTz_hFtiJ2w108gXZcLcg3e8tLeSGIC36Wn_M" /> -->


    <meta name="author" content="许">





<title>Hung-yi Lee. Machine Learning 2021 HWs | 须臾所学之野</title>



    <link rel="icon" href="/favicon.png">




    <!-- stylesheets list from _config.yml -->
    
    <link rel="stylesheet" href="/css/style.css">
    



    <!-- scripts list from _config.yml -->
    
    <script src="/js/script.js"></script>
    
    <script src="/js/tocbot.min.js"></script>
    






    
    
        <script>
    MathJax = {
        tex: {
            // 行内公式标志
            inlineMath: [
                ['$', '$']
            ],
            // 块级公式标志
            displayMath: [
                ['$$', '$$']
            ],
            // 下面两个主要是支持渲染某些公式，可以自己了解
            processEnvironments: true,
            processRefs: true,
        },
        options: {
            // 跳过渲染的标签
            skipHtmlTags: ['noscript', 'style', 'textarea', 'pre', 'code'],
            // 跳过mathjax处理的元素的类名，任何元素指定一个类 tex2jax_ignore 将被跳过，多个累=类名'class1|class2'
            ignoreHtmlClass: 'tex2jax_ignore',
        },
        // 这里可能是因为我的MathJax2仍有残留，导致行间公式被渲染成了type="math/tex"，所以要用这种2、3版本混合查找方式进行渲染
        // 这样可能效率低，有机会再改。
        options: {
            renderActions: {
                /* add a new named action not to override the original 'find' action */
                find_script_mathtex: [10, function (doc) {
                    for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
                        const display = !!node.type.match(/; *mode=display/);
                        const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
                        const text = document.createTextNode('');
                        node.parentNode.replaceChild(text, node);
                        math.start = {
                            node: text,
                            delim: '',
                            n: 0
                        };
                        math.end = {
                            node: text,
                            delim: '',
                            n: 0
                        };
                        doc.math.push(math);
                    }
                }, '']
            }
        },
        svg: {
            fontCache: 'global',
        },
    };
</script>

<script type="text/javascript" id="MathJax-script" async
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>

<!-- <script type="text/javascript" id="MathJax-script" async
    src="https://cdn.bootcdn.net/ajax/libs/mathjax/3.1.2/es5/tex-mml-chtml.min.js">
</script> -->
    



<meta name="generator" content="Hexo 6.3.0"></head>

<body>
    <script>
        // this function is used to check current theme before page loaded.
        (() => {
            const currentTheme = window.localStorage && window.localStorage.getItem('theme') || '';
            const isDark = currentTheme === 'dark';
            const pagebody = document.getElementsByTagName('body')[0]
            if (isDark) {
                pagebody.classList.add('dark-theme');
                // mobile
                document.getElementById("mobile-toggle-theme").innerText = "· Dark"
            } else {
                pagebody.classList.remove('dark-theme');
                // mobile
                document.getElementById("mobile-toggle-theme").innerText = "· Light"
            }
        })();
    </script>

    <div class="wrapper">
        <header>
    <nav class="navbar">
        <div class="container">
            <div class="navbar-header header-logo"><a href="/">Xuwp&#39;s Blog</a></div>
            <div class="menu navbar-right">
                
                    <a class="menu-item" href="/archives">Posts</a>
                
                    <a class="menu-item" href="/category">Categories</a>
                
                    <a class="menu-item" href="/tag">Tags</a>
                
                    <a class="menu-item" href="/about">About</a>
                
                    <a class="menu-item" href="/link">Links</a>
                
                    <a class="menu-item" href="https://xuwp.top/cv-en/">Resume</a>
                
                <input id="switch_default" type="checkbox" class="switch_default">
                <label for="switch_default" class="toggleBtn"></label>
            </div>
        </div>
    </nav>

    
    <nav class="navbar-mobile" id="nav-mobile">
        <div class="container">
            <div class="navbar-header">
                <div>
                    <a href="/">Xuwp&#39;s Blog</a><a id="mobile-toggle-theme">·&nbsp;Light</a>
                </div>
                <div class="menu-toggle" onclick="mobileBtn()">&#9776; Menu</div>
            </div>
            <div class="menu" id="mobile-menu">
                
                    <a class="menu-item" href="/archives">Posts</a>
                
                    <a class="menu-item" href="/category">Categories</a>
                
                    <a class="menu-item" href="/tag">Tags</a>
                
                    <a class="menu-item" href="/about">About</a>
                
                    <a class="menu-item" href="/link">Links</a>
                
                    <a class="menu-item" href="https://xuwp.top/cv-en/">Resume</a>
                
            </div>
        </div>
    </nav>

</header>
<script>
    var mobileBtn = function f() {
        var toggleMenu = document.getElementsByClassName("menu-toggle")[0];
        var mobileMenu = document.getElementById("mobile-menu");
        if(toggleMenu.classList.contains("active")){
           toggleMenu.classList.remove("active")
            mobileMenu.classList.remove("active")
        }else{
            toggleMenu.classList.add("active")
            mobileMenu.classList.add("active")
        }
    }
</script>
            <div class="main">
                <div class="container">
    
    
        <div class="post-toc">
    <div class="tocbot-list">
    </div>
    <div class="tocbot-list-menu">
        <a class="tocbot-toc-expand" onclick="expand_toc()">Expand all</a>
        <a onclick="go_top()">Back to top</a>
        <a onclick="go_bottom()">Go to bottom</a>
    </div>
</div>

<script>
    var tocbot_timer;
    var DEPTH_MAX = 6;    // 为 6 时展开所有
    var tocbot_default_config = {
        tocSelector: '.tocbot-list',
        contentSelector: '.post-content',
        headingSelector: 'h1, h2, h3, h4, h5',
        orderedList: false,
        scrollSmooth: true,
        onClick: extend_click,
    };

    function extend_click() {
        clearTimeout(tocbot_timer);
        tocbot_timer = setTimeout(function () {
            tocbot.refresh(obj_merge(tocbot_default_config, { hasInnerContainers: true }));
        }, 420); // 这个值是由 tocbot 源码里定义的 scrollSmoothDuration 得来的
    }

    document.ready(function () {
        tocbot.init(obj_merge(tocbot_default_config, { collapseDepth: 1 }));
    });

    function expandToc() {
        var b = document.querySelector('.tocbot-toc-expand');
        var expanded = b.getAttribute('data-expanded');
        expanded ? b.removeAttribute('data-expanded') : b.setAttribute('data-expanded', true);
        tocbot.refresh(obj_merge(tocbot_default_config, { collapseDepth: expanded ? 1 : DEPTH_MAX }));
        b.innerText = expanded ? 'Expand all' : 'Collapse all';
    }

    function go_top() {
        window.scrollTo(0, 0);
    }

    function go_bottom() {
        window.scrollTo(0, document.body.scrollHeight);
    }

    function obj_merge(target, source) {
        for (var item in source) {
            if (source.hasOwnProperty(item)) {
                target[item] = source[item];
            }
        }
        return target;
    }
</script>
    

    
    <article class="post-wrap">
        <header class="post-header">
            <h1 class="post-title">Hung-yi Lee. Machine Learning 2021 HWs</h1>
            
                <div class="post-meta">
                    
                        Author: <a itemprop="author" rel="author" href="/">许</a>
                    

                    
                        <span class="post-time">
                        Date: <a href="#">July 30, 2022&nbsp;&nbsp;19:17:05</a>
                        </span>
                    
                    
                        <span class="post-category">
                    Category:
                            
                                <a href="/categories/AI/">AI</a>
                            
                        </span>
                    
                </div>
            
        </header>

        <div class="post-content">
            <p>李宏毅教授2021年机器学习课程的作业，详见<a target="_blank" rel="noopener" href="https://speech.ee.ntu.edu.tw/~hylee/ml/2021-spring.php">ML 2021 Spring</a></p>
<p>感觉NTU学生的英语都很好啊hh，所以这也是本人第一次练习使用双语书写注释及内容</p>
<h1 id="Homework-1-COVID-19-Cases-Prediction-Regression"><a href="#Homework-1-COVID-19-Cases-Prediction-Regression" class="headerlink" title="Homework 1: COVID-19 Cases Prediction (Regression)"></a><strong>Homework 1: COVID-19 Cases Prediction (Regression)</strong></h1><h2 id="Simple-Baseline"><a href="#Simple-Baseline" class="headerlink" title="Simple Baseline"></a><strong>Simple Baseline</strong></h2><p>只要跑通助教的代码就可以，经注释后的代码如下。</p>
<h3 id="Download-Data"><a href="#Download-Data" class="headerlink" title="Download Data"></a><strong>Download Data</strong></h3><p>If the Google drive links are dead, you can download data from <a target="_blank" rel="noopener" href="https://www.kaggle.com/c/ml2021spring-hw1/data">kaggle</a>, and upload data manually to the workspace.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">tr_path = <span class="string">&#x27;covid.train.csv&#x27;</span>   <span class="comment"># path to training data</span></span><br><span class="line">tt_path = <span class="string">&#x27;covid.test.csv&#x27;</span>    <span class="comment"># path to testing data</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Google Colab special command</span></span><br><span class="line">!gdown --<span class="built_in">id</span> <span class="string">&#x27;19CCyCgJrUxtvgZF53vnctJiOJ23T5mqF&#x27;</span> --output covid.train.csv</span><br><span class="line">!gdown --<span class="built_in">id</span> <span class="string">&#x27;1CE240jLm2npU-tdz81-oVKEF3T2yfT1O&#x27;</span> --output covid.test.csv</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">/usr/local/lib/python3.7/dist-packages/gdown/cli.py:131: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don&#x27;t need to pass it anymore to use a file ID.</span><br><span class="line">  category=FutureWarning,</span><br><span class="line">Downloading...</span><br><span class="line">From: https://drive.google.com/uc?id=19CCyCgJrUxtvgZF53vnctJiOJ23T5mqF</span><br><span class="line">To: /content/covid.train.csv</span><br><span class="line">100% 2.00M/2.00M [00:00&lt;00:00, 202MB/s]</span><br><span class="line">/usr/local/lib/python3.7/dist-packages/gdown/cli.py:131: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don&#x27;t need to pass it anymore to use a file ID.</span><br><span class="line">  category=FutureWarning,</span><br><span class="line">Downloading...</span><br><span class="line">From: https://drive.google.com/uc?id=1CE240jLm2npU-tdz81-oVKEF3T2yfT1O</span><br><span class="line">To: /content/covid.test.csv</span><br><span class="line">100% 651k/651k [00:00&lt;00:00, 129MB/s]</span><br></pre></td></tr></table></figure>
<h3 id="Import-Some-Packages"><a href="#Import-Some-Packages" class="headerlink" title="Import Some Packages"></a><strong>Import Some Packages</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># PyTorch</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset, DataLoader</span><br><span class="line"></span><br><span class="line"><span class="comment"># For data preprocess</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> csv</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="comment"># For plotting</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> matplotlib.pyplot <span class="keyword">import</span> figure</span><br><span class="line"></span><br><span class="line"><span class="comment"># set a random seed for reproducibility（可重复性）</span></span><br><span class="line">myseed = <span class="number">42069</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># if True, causes cuDNN to only use deterministic convolution algorithms.(确定性卷积算法)</span></span><br><span class="line"><span class="comment"># 确定性卷积算法 is, algorithms which, given the same input, and when run on the same software and hardware, always produce the same output</span></span><br><span class="line"><span class="comment"># Performance： nondeterministic algorithms &gt; deterministic algorithms (In most cases)</span></span><br><span class="line">torch.backends.cudnn.deterministic = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># if True, causes cuDNN to benchmark multiple convolution algorithms and select the fastest.</span></span><br><span class="line"><span class="comment"># 如果卷积网络结构不是动态变化的，即网络的输入 (batch size，图像的大小，输入的通道) 是固定的，设置为True。由于HW1并未涉及卷积运算，所以设置为False</span></span><br><span class="line">torch.backends.cudnn.benchmark = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># set the random seed for numpy, torch, torch.cuda</span></span><br><span class="line">np.random.seed(myseed)</span><br><span class="line">torch.manual_seed(myseed)</span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    torch.cuda.manual_seed_all(myseed)</span><br></pre></td></tr></table></figure>
<h3 id="Some-Utilities"><a href="#Some-Utilities" class="headerlink" title="Some Utilities"></a><strong>Some Utilities</strong></h3><p>You do not need to modify this part.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_device</span>():</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27; Get device (if GPU is available, use GPU) &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">&#x27;cuda&#x27;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&#x27;cpu&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">plot_learning_curve</span>(<span class="params">loss_record, title=<span class="string">&#x27;&#x27;</span></span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27; Plot learning curve of your DNN (train &amp; dev loss) &#x27;&#x27;&#x27;</span></span><br><span class="line">    total_steps = <span class="built_in">len</span>(loss_record[<span class="string">&#x27;train&#x27;</span>])</span><br><span class="line">    x_1 = <span class="built_in">range</span>(total_steps)</span><br><span class="line">    x_2 = x_1[::<span class="built_in">len</span>(loss_record[<span class="string">&#x27;train&#x27;</span>]) // <span class="built_in">len</span>(loss_record[<span class="string">&#x27;dev&#x27;</span>])]</span><br><span class="line">    figure(figsize=(<span class="number">6</span>, <span class="number">4</span>))              <span class="comment"># Set the weight, height of the figure(in feet英尺)</span></span><br><span class="line">    plt.plot(x_1, loss_record[<span class="string">&#x27;train&#x27;</span>], c=<span class="string">&#x27;tab:red&#x27;</span>, label=<span class="string">&#x27;train&#x27;</span>)</span><br><span class="line">    plt.plot(x_2, loss_record[<span class="string">&#x27;dev&#x27;</span>], c=<span class="string">&#x27;tab:cyan&#x27;</span>, label=<span class="string">&#x27;dev&#x27;</span>)</span><br><span class="line">    plt.ylim(<span class="number">0.0</span>, <span class="number">5.</span>)                   <span class="comment"># Limit the y-axis range to 0.0~5.0</span></span><br><span class="line">    plt.xlabel(<span class="string">&#x27;Training steps&#x27;</span>)</span><br><span class="line">    plt.ylabel(<span class="string">&#x27;MSE loss&#x27;</span>)</span><br><span class="line">    plt.title(<span class="string">&#x27;Learning curve of &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(title))</span><br><span class="line">    plt.legend()                        <span class="comment"># Place a legend（图例，就是每种线代表什么） on the Axes.</span></span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">plot_pred</span>(<span class="params">dv_set, model, device, lim=<span class="number">35.</span>, preds=<span class="literal">None</span>, targets=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27; Plot prediction of your DNN &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="keyword">if</span> preds <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">or</span> targets <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        model.<span class="built_in">eval</span>()                    <span class="comment"># Sets the module in evaluation mode.</span></span><br><span class="line">        preds, targets = [], []</span><br><span class="line">        <span class="keyword">for</span> x, y <span class="keyword">in</span> dv_set:</span><br><span class="line">            x, y = x.to(device), y.to(device)</span><br><span class="line">            <span class="keyword">with</span> torch.no_grad():       <span class="comment"># Context-manager that disabled gradient calculation.（无视梯度上下文，直接requires_grad=False，加速用）</span></span><br><span class="line">                pred = model(x)</span><br><span class="line">                preds.append(pred.detach().cpu())   <span class="comment"># Tensor.detach(): Returns a new Tensor, detached from the current graph. Tensor.cpu(): Returns a copy of this object in CPU memory.</span></span><br><span class="line">                targets.append(y.detach().cpu())</span><br><span class="line">        preds = torch.cat(preds, dim=<span class="number">0</span>).numpy()     <span class="comment"># Concatenates the given sequence of seq tensors in the given dimension. （和cpp中cat类似，这里就是list-&gt;Tensor-&gt;ndarray）</span></span><br><span class="line">        targets = torch.cat(targets, dim=<span class="number">0</span>).numpy()</span><br><span class="line"></span><br><span class="line">    figure(figsize=(<span class="number">5</span>, <span class="number">5</span>))</span><br><span class="line">    plt.scatter(targets, preds, c=<span class="string">&#x27;r&#x27;</span>, alpha=<span class="number">0.5</span>)   <span class="comment"># 散点图</span></span><br><span class="line">    plt.plot([-<span class="number">0.2</span>, lim], [-<span class="number">0.2</span>, lim], c=<span class="string">&#x27;b&#x27;</span>)</span><br><span class="line">    plt.xlim(-<span class="number">0.2</span>, lim)</span><br><span class="line">    plt.ylim(-<span class="number">0.2</span>, lim)</span><br><span class="line">    plt.xlabel(<span class="string">&#x27;ground truth value&#x27;</span>)</span><br><span class="line">    plt.ylabel(<span class="string">&#x27;predicted value&#x27;</span>)</span><br><span class="line">    plt.title(<span class="string">&#x27;Ground Truth v.s. Prediction&#x27;</span>)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>
<h3 id="Preprocess"><a href="#Preprocess" class="headerlink" title="Preprocess"></a><strong>Preprocess</strong></h3><p>We have three kinds of datasets:</p>
<ul>
<li><code>train</code>: for training</li>
<li><code>dev</code>: for validation</li>
<li><code>test</code>: for testing (w/o target value)</li>
</ul>
<h4 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a><strong>Dataset</strong></h4><p>The <code>COVID19Dataset</code> below does:</p>
<ul>
<li>read <code>.csv</code> files</li>
<li>extract features</li>
<li>split <code>covid.train.csv</code> into train/dev sets</li>
<li>normalize features</li>
</ul>
<p>Finishing <code>TODO</code> below might make you pass medium baseline.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">COVID19Dataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27; Dataset for loading and preprocessing the COVID19 dataset &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,</span></span><br><span class="line"><span class="params">                 path,</span></span><br><span class="line"><span class="params">                 mode=<span class="string">&#x27;train&#x27;</span>,</span></span><br><span class="line"><span class="params">                 target_only=<span class="literal">False</span></span>):</span><br><span class="line">        self.mode = mode</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Read data into numpy arrays</span></span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(path, <span class="string">&#x27;r&#x27;</span>) <span class="keyword">as</span> fp:</span><br><span class="line">            data = <span class="built_in">list</span>(csv.reader(fp))</span><br><span class="line">            data = np.array(data[<span class="number">1</span>:])[:, <span class="number">1</span>:].astype(<span class="built_in">float</span>)    <span class="comment"># Remove the 0th row and 0th column 获取数值数据</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 默认情况，选取除最后一列的全部列（0~92）作为train data</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> target_only:</span><br><span class="line">            feats = <span class="built_in">list</span>(<span class="built_in">range</span>(<span class="number">93</span>))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># <span class="doctag">TODO:</span> Using 40 states &amp; 2 tested_positive features (indices = 57 &amp; 75)</span></span><br><span class="line">            <span class="comment"># feats = list(range(40)) + [57] + [75]</span></span><br><span class="line">            <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 最后一列数据不同，训练集有最后一列（93th），测试集没有93th column</span></span><br><span class="line">        <span class="keyword">if</span> mode == <span class="string">&#x27;test&#x27;</span>:</span><br><span class="line">            <span class="comment"># Testing data</span></span><br><span class="line">            <span class="comment"># data: 893 x 93 (40 states + day 1 (18) + day 2 (18) + day 3 (17))</span></span><br><span class="line">            <span class="comment"># 测试集不含lable，只需操作data</span></span><br><span class="line">            data = data[:, feats]       <span class="comment"># all rows，&lt;feats&gt; columns</span></span><br><span class="line">            self.data = torch.FloatTensor(data)   <span class="comment"># ndarray -&gt; Tensor(Float32)</span></span><br><span class="line">        <span class="keyword">elif</span> mode <span class="keyword">in</span> [<span class="string">&#x27;train&#x27;</span>, <span class="string">&#x27;dev&#x27;</span>]:</span><br><span class="line">            <span class="comment"># Training data (train/dev sets)</span></span><br><span class="line">            <span class="comment"># data: 2700 x 94 (40 states + day 1 (18) + day 2 (18) + day 3 (18))</span></span><br><span class="line">            target = data[:, -<span class="number">1</span>]        <span class="comment"># all rows, 94th column</span></span><br><span class="line">            data = data[:, feats]       <span class="comment"># all rows, &lt;feats(default: 0th~93th)&gt; columns</span></span><br><span class="line">            </span><br><span class="line">            <span class="comment"># Splitting training data into train &amp; dev sets</span></span><br><span class="line">            <span class="comment"># len(train sets):len(dev sets) = 9:1 （每10个中9个作为train，1个作为dev）</span></span><br><span class="line">            <span class="keyword">if</span> mode == <span class="string">&#x27;train&#x27;</span>:</span><br><span class="line">                indices = [i <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(data)) <span class="keyword">if</span> i % <span class="number">10</span> != <span class="number">0</span>]</span><br><span class="line">            <span class="keyword">elif</span> mode == <span class="string">&#x27;dev&#x27;</span>:</span><br><span class="line">                indices = [i <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(data)) <span class="keyword">if</span> i % <span class="number">10</span> == <span class="number">0</span>]</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># Convert data into PyTorch tensors</span></span><br><span class="line">            self.data = torch.FloatTensor(data[indices])</span><br><span class="line">            self.target = torch.FloatTensor(target[indices])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Normalize features (you may remove this part to see what will happen)</span></span><br><span class="line">        <span class="comment"># 归一化特征（通常添加后可以提升模型训练的效果）</span></span><br><span class="line">        <span class="comment"># （第i维数据 - 第i维数据的平均值）/（第i维数据的标准差）</span></span><br><span class="line">        self.data[:, <span class="number">40</span>:] = \</span><br><span class="line">            (self.data[:, <span class="number">40</span>:] - self.data[:, <span class="number">40</span>:].mean(dim=<span class="number">0</span>, keepdim=<span class="literal">True</span>)) \</span><br><span class="line">            / self.data[:, <span class="number">40</span>:].std(dim=<span class="number">0</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        self.dim = self.data.shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Finished reading the &#123;&#125; set of COVID19 Dataset (&#123;&#125; samples found, each dim = &#123;&#125;)&#x27;</span></span><br><span class="line">              .<span class="built_in">format</span>(mode, <span class="built_in">len</span>(self.data), self.dim))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, index</span>):</span><br><span class="line">        <span class="comment"># Returns one sample at a time</span></span><br><span class="line">        <span class="keyword">if</span> self.mode <span class="keyword">in</span> [<span class="string">&#x27;train&#x27;</span>, <span class="string">&#x27;dev&#x27;</span>]:</span><br><span class="line">            <span class="comment"># For training</span></span><br><span class="line">            <span class="keyword">return</span> self.data[index], self.target[index]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># For testing (no target)</span></span><br><span class="line">            <span class="keyword">return</span> self.data[index]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="comment"># Returns the size of the dataset</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.data)</span><br></pre></td></tr></table></figure>
<h4 id="DataLoader"><a href="#DataLoader" class="headerlink" title="DataLoader"></a><strong>DataLoader</strong></h4><p>A <code>DataLoader</code> loads data from a given <code>Dataset</code> into batches.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">prep_dataloader</span>(<span class="params">path, mode, batch_size, n_jobs=<span class="number">0</span>, target_only=<span class="literal">False</span></span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27; Generates a dataset, then is put into a dataloader. &#x27;&#x27;&#x27;</span></span><br><span class="line">    dataset = COVID19Dataset(path, mode=mode, target_only=target_only)  <span class="comment"># Construct dataset</span></span><br><span class="line">    <span class="comment"># num_workers (int, optional) – how many subprocesses to use for data loading. 0 means that the data will be loaded in the main process. (default: 0)</span></span><br><span class="line">    <span class="comment"># pin_memory (bool, optional) – If True, the data loader will copy Tensors into device/CUDA pinned memory before returning them.</span></span><br><span class="line">    dataloader = DataLoader(</span><br><span class="line">        dataset, batch_size,</span><br><span class="line">        shuffle=(mode == <span class="string">&#x27;train&#x27;</span>), drop_last=<span class="literal">False</span>,</span><br><span class="line">        num_workers=n_jobs, pin_memory=<span class="literal">True</span>)                            <span class="comment"># Construct dataloader</span></span><br><span class="line">    <span class="keyword">return</span> dataloader</span><br></pre></td></tr></table></figure>
<h3 id="Deep-Neural-Network"><a href="#Deep-Neural-Network" class="headerlink" title="Deep Neural Network"></a><strong>Deep Neural Network</strong></h3><p><code>NeuralNet</code> is an <code>nn.Module</code> designed for regression.<br>The DNN consists of 2 fully-connected layers with ReLU activation.<br>This module also included a function <code>cal_loss</code> for calculating loss.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">NeuralNet</span>(nn.Module):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27; A simple fully-connected deep neural network &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_dim</span>):</span><br><span class="line">        <span class="built_in">super</span>(NeuralNet, self).__init__()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Define your neural network here</span></span><br><span class="line">        <span class="comment"># <span class="doctag">TODO:</span> How to modify this model to achieve better performance?</span></span><br><span class="line">        <span class="comment"># A sequential container.（顺序容器） Modules will be added to it in the order they are passed in the constructor. </span></span><br><span class="line">        self.net = nn.Sequential(</span><br><span class="line">            nn.Linear(input_dim, <span class="number">64</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(<span class="number">64</span>, <span class="number">1</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Mean squared error loss</span></span><br><span class="line">        <span class="comment"># reduction (string, optional) – Specifies the reduction to apply to the output: &#x27;none&#x27; | &#x27;mean&#x27; | &#x27;sum&#x27;.</span></span><br><span class="line">        <span class="comment"># &#x27;none&#x27;: no reduction will be applied, </span></span><br><span class="line">        <span class="comment"># &#x27;mean&#x27;: the sum of the output will be divided by the number of elements in the output, </span></span><br><span class="line">        <span class="comment"># &#x27;sum&#x27;: the output will be summed. (Default)</span></span><br><span class="line">        self.criterion = nn.MSELoss(reduction=<span class="string">&#x27;mean&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27; Given input of size (batch_size x input_dim), compute output of the network &#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="comment"># Returns a tensor with all the dimensions of input of size 1 removed.</span></span><br><span class="line">        <span class="keyword">return</span> self.net(x).squeeze(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">cal_loss</span>(<span class="params">self, pred, target</span>):</span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27; Calculate loss &#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="comment"># <span class="doctag">TODO:</span> you may implement L1/L2 regularization here</span></span><br><span class="line">        <span class="keyword">return</span> self.criterion(pred, target)</span><br></pre></td></tr></table></figure>
<h3 id="Train-Dev-Test"><a href="#Train-Dev-Test" class="headerlink" title="Train/Dev/Test"></a><strong>Train/Dev/Test</strong></h3><h4 id="Training"><a href="#Training" class="headerlink" title="Training"></a><strong>Training</strong></h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">tr_set, dv_set, model, config, device</span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27; DNN training &#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">    n_epochs = config[<span class="string">&#x27;n_epochs&#x27;</span>]  <span class="comment"># Maximum number of epochs</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Setup optimizer</span></span><br><span class="line">    optimizer = <span class="built_in">getattr</span>(torch.optim, config[<span class="string">&#x27;optimizer&#x27;</span>])(</span><br><span class="line">        model.parameters(), **config[<span class="string">&#x27;optim_hparas&#x27;</span>])</span><br><span class="line"></span><br><span class="line">    min_mse = <span class="number">1000.</span></span><br><span class="line">    loss_record = &#123;<span class="string">&#x27;train&#x27;</span>: [], <span class="string">&#x27;dev&#x27;</span>: []&#125;      <span class="comment"># for recording training loss</span></span><br><span class="line">    early_stop_cnt = <span class="number">0</span></span><br><span class="line">    epoch = <span class="number">0</span></span><br><span class="line">    <span class="keyword">while</span> epoch &lt; n_epochs:</span><br><span class="line">        model.train()                           <span class="comment"># set model to training mode</span></span><br><span class="line">        <span class="keyword">for</span> x, y <span class="keyword">in</span> tr_set:                     <span class="comment"># iterate through the dataloader</span></span><br><span class="line">            optimizer.zero_grad()               <span class="comment"># set gradient to zero</span></span><br><span class="line">            x, y = x.to(device), y.to(device)   <span class="comment"># move data to device (cpu/cuda)</span></span><br><span class="line">            pred = model(x)                     <span class="comment"># forward pass (compute output)</span></span><br><span class="line">            mse_loss = model.cal_loss(pred, y)  <span class="comment"># compute loss</span></span><br><span class="line">            mse_loss.backward()                 <span class="comment"># compute gradient (backpropagation)</span></span><br><span class="line">            optimizer.step()                    <span class="comment"># update model with optimizer</span></span><br><span class="line">            loss_record[<span class="string">&#x27;train&#x27;</span>].append(mse_loss.detach().cpu().item())</span><br><span class="line"></span><br><span class="line">        <span class="comment"># After each epoch, test your model on the validation (development) set.</span></span><br><span class="line">        dev_mse = dev(dv_set, model, device)</span><br><span class="line">        <span class="keyword">if</span> dev_mse &lt; min_mse:</span><br><span class="line">            <span class="comment"># Save model if your model improved</span></span><br><span class="line">            min_mse = dev_mse</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&#x27;Saving model (epoch = <span class="subst">&#123;epoch + <span class="number">1</span> : 4d&#125;</span>, loss = <span class="subst">&#123;min_mse : <span class="number">.4</span>f&#125;</span>)&#x27;</span>)</span><br><span class="line">            <span class="comment"># model.state_dict(): Returns a dictionary containing a whole state of the module.</span></span><br><span class="line">            torch.save(model.state_dict(), config[<span class="string">&#x27;save_path&#x27;</span>])  <span class="comment"># Save model to specified path &lt;config[&#x27;save_path&#x27;]&gt;</span></span><br><span class="line">            early_stop_cnt = <span class="number">0</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            early_stop_cnt += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        epoch += <span class="number">1</span></span><br><span class="line">        loss_record[<span class="string">&#x27;dev&#x27;</span>].append(dev_mse)</span><br><span class="line">        <span class="keyword">if</span> early_stop_cnt &gt; config[<span class="string">&#x27;early_stop&#x27;</span>]:</span><br><span class="line">            <span class="comment"># Stop training if your model stops improving for &quot;config[&#x27;early_stop&#x27;]&quot; epochs. 早停</span></span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Finished training after &#123;&#125; epochs&#x27;</span>.<span class="built_in">format</span>(epoch))</span><br><span class="line">    <span class="keyword">return</span> min_mse, loss_record</span><br></pre></td></tr></table></figure>
<h4 id="Validation"><a href="#Validation" class="headerlink" title="Validation"></a><strong>Validation</strong></h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">dev</span>(<span class="params">dv_set, model, device</span>):</span><br><span class="line">    model.<span class="built_in">eval</span>()                                <span class="comment"># set model to evalutation mode</span></span><br><span class="line">    total_loss = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> x, y <span class="keyword">in</span> dv_set:                         <span class="comment"># iterate through the dataloader</span></span><br><span class="line">        x, y = x.to(device), y.to(device)       <span class="comment"># move data to device (cpu/cuda)</span></span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():                   <span class="comment"># disable gradient calculation</span></span><br><span class="line">            pred = model(x)                     <span class="comment"># forward pass (compute output)</span></span><br><span class="line">            mse_loss = model.cal_loss(pred, y)  <span class="comment"># compute loss</span></span><br><span class="line">        total_loss += mse_loss.detach().cpu().item() * <span class="built_in">len</span>(x)  <span class="comment"># accumulate loss</span></span><br><span class="line">    total_loss = total_loss / <span class="built_in">len</span>(dv_set.dataset)              <span class="comment"># compute averaged loss</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> total_loss</span><br></pre></td></tr></table></figure>
<h4 id="Testing"><a href="#Testing" class="headerlink" title="Testing"></a><strong>Testing</strong></h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">test</span>(<span class="params">tt_set, model, device</span>):</span><br><span class="line">    model.<span class="built_in">eval</span>()                                <span class="comment"># set model to evalutation mode</span></span><br><span class="line">    preds = []</span><br><span class="line">    <span class="keyword">for</span> x <span class="keyword">in</span> tt_set:                            <span class="comment"># iterate through the dataloader</span></span><br><span class="line">        x = x.to(device)                        <span class="comment"># move data to device (cpu/cuda)</span></span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():                   <span class="comment"># disable gradient calculation</span></span><br><span class="line">            pred = model(x)                     <span class="comment"># forward pass (compute output)</span></span><br><span class="line">            preds.append(pred.detach().cpu())   <span class="comment"># collect prediction 记录预测值</span></span><br><span class="line">    preds = torch.cat(preds, dim=<span class="number">0</span>).numpy()     <span class="comment"># concatenate all predictions and convert to a numpy array (list-&gt;tensor-&gt;ndarray) </span></span><br><span class="line">    <span class="keyword">return</span> preds</span><br></pre></td></tr></table></figure>
<h3 id="Setup-Hyper-parameters"><a href="#Setup-Hyper-parameters" class="headerlink" title="Setup Hyper-parameters"></a><strong>Setup Hyper-parameters</strong></h3><p><code>config</code> contains hyper-parameters for training and the path to save your model.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">device = get_device()                 <span class="comment"># get the current available device (&#x27;cpu&#x27; or &#x27;cuda&#x27;)</span></span><br><span class="line">os.makedirs(<span class="string">&#x27;models&#x27;</span>, exist_ok=<span class="literal">True</span>)  <span class="comment"># The trained model will be saved to ./models/, exist_ok：只有在目录不存在时创建目录</span></span><br><span class="line">target_only = <span class="literal">False</span>                   <span class="comment"># <span class="doctag">TODO:</span> Using 40 states &amp; 2 tested_positive features</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># <span class="doctag">TODO:</span> How to tune these hyper-parameters to improve your model&#x27;s performance?</span></span><br><span class="line">config = &#123;</span><br><span class="line">    <span class="string">&#x27;n_epochs&#x27;</span>: <span class="number">3000</span>,                <span class="comment"># maximum number of epochs</span></span><br><span class="line">    <span class="string">&#x27;batch_size&#x27;</span>: <span class="number">270</span>,               <span class="comment"># mini-batch size for dataloader</span></span><br><span class="line">    <span class="string">&#x27;optimizer&#x27;</span>: <span class="string">&#x27;SGD&#x27;</span>,              <span class="comment"># optimization algorithm (optimizer in torch.optim)</span></span><br><span class="line">    <span class="string">&#x27;optim_hparas&#x27;</span>: &#123;                <span class="comment"># hyper-parameters for the optimizer (depends on which optimizer you are using)</span></span><br><span class="line">        <span class="string">&#x27;lr&#x27;</span>: <span class="number">0.001</span>,                 <span class="comment"># learning rate of SGD</span></span><br><span class="line">        <span class="string">&#x27;momentum&#x27;</span>: <span class="number">0.9</span>              <span class="comment"># momentum for SGD</span></span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="string">&#x27;early_stop&#x27;</span>: <span class="number">200</span>,               <span class="comment"># early stopping epochs (the number epochs since your model&#x27;s last improvement)</span></span><br><span class="line">    <span class="string">&#x27;save_path&#x27;</span>: <span class="string">&#x27;models/model.pth&#x27;</span>  <span class="comment"># your model will be saved here</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="Load-data-and-model"><a href="#Load-data-and-model" class="headerlink" title="Load data and model"></a><strong>Load data and model</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tr_set = prep_dataloader(tr_path, <span class="string">&#x27;train&#x27;</span>, config[<span class="string">&#x27;batch_size&#x27;</span>], target_only=target_only)</span><br><span class="line">dv_set = prep_dataloader(tr_path, <span class="string">&#x27;dev&#x27;</span>, config[<span class="string">&#x27;batch_size&#x27;</span>], target_only=target_only)</span><br><span class="line">tt_set = prep_dataloader(tt_path, <span class="string">&#x27;test&#x27;</span>, config[<span class="string">&#x27;batch_size&#x27;</span>], target_only=target_only)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Finished reading the train set of COVID19 Dataset (2430 samples found, each dim = 93)</span><br><span class="line">Finished reading the dev set of COVID19 Dataset (270 samples found, each dim = 93)</span><br><span class="line">Finished reading the test set of COVID19 Dataset (893 samples found, each dim = 93)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model = NeuralNet(tr_set.dataset.dim).to(device)  # Construct model and move to device</span><br></pre></td></tr></table></figure>
<h3 id="Start-Training"><a href="#Start-Training" class="headerlink" title="Start Training!"></a><strong>Start Training!</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model_loss, model_loss_record = train(tr_set, dv_set, model, config, device)</span><br></pre></td></tr></table></figure>
<pre><code>Saving model (epoch =    1, loss =  78.8524)
Saving model (epoch =    2, loss =  37.6170)
Saving model (epoch =    3, loss =  26.1203)
Saving model (epoch =    4, loss =  16.1862)
Saving model (epoch =    5, loss =  9.7153)
Saving model (epoch =    6, loss =  6.3701)
Saving model (epoch =    7, loss =  5.1802)
Saving model (epoch =    8, loss =  4.4255)
Saving model (epoch =    9, loss =  3.8009)
Saving model (epoch =   10, loss =  3.3691)
Saving model (epoch =   11, loss =  3.0943)
Saving model (epoch =   12, loss =  2.8176)
Saving model (epoch =   13, loss =  2.6274)
Saving model (epoch =   14, loss =  2.4542)
Saving model (epoch =   15, loss =  2.3012)
Saving model (epoch =   16, loss =  2.1766)
Saving model (epoch =   17, loss =  2.0641)
Saving model (epoch =   18, loss =  1.9399)
Saving model (epoch =   19, loss =  1.8978)
Saving model (epoch =   20, loss =  1.7950)
Saving model (epoch =   21, loss =  1.7164)
Saving model (epoch =   22, loss =  1.6455)
Saving model (epoch =   23, loss =  1.5912)
Saving model (epoch =   24, loss =  1.5599)
Saving model (epoch =   25, loss =  1.5197)
Saving model (epoch =   26, loss =  1.4698)
Saving model (epoch =   27, loss =  1.4189)
Saving model (epoch =   28, loss =  1.3992)
Saving model (epoch =   29, loss =  1.3696)
Saving model (epoch =   30, loss =  1.3442)
Saving model (epoch =   31, loss =  1.3231)
Saving model (epoch =   32, loss =  1.2834)
Saving model (epoch =   33, loss =  1.2804)
Saving model (epoch =   34, loss =  1.2471)
Saving model (epoch =   36, loss =  1.2414)
Saving model (epoch =   37, loss =  1.2138)
Saving model (epoch =   38, loss =  1.2083)
Saving model (epoch =   41, loss =  1.1591)
Saving model (epoch =   42, loss =  1.1484)
Saving model (epoch =   44, loss =  1.1209)
Saving model (epoch =   47, loss =  1.1122)
Saving model (epoch =   48, loss =  1.0937)
Saving model (epoch =   50, loss =  1.0842)
Saving model (epoch =   53, loss =  1.0655)
Saving model (epoch =   54, loss =  1.0613)
Saving model (epoch =   57, loss =  1.0524)
Saving model (epoch =   58, loss =  1.0394)
Saving model (epoch =   60, loss =  1.0267)
Saving model (epoch =   63, loss =  1.0248)
Saving model (epoch =   66, loss =  1.0099)
Saving model (epoch =   70, loss =  0.9829)
Saving model (epoch =   72, loss =  0.9817)
Saving model (epoch =   73, loss =  0.9743)
Saving model (epoch =   75, loss =  0.9671)
Saving model (epoch =   78, loss =  0.9643)
Saving model (epoch =   79, loss =  0.9597)
Saving model (epoch =   85, loss =  0.9549)
Saving model (epoch =   86, loss =  0.9535)
Saving model (epoch =   90, loss =  0.9467)
Saving model (epoch =   92, loss =  0.9432)
Saving model (epoch =   93, loss =  0.9231)
Saving model (epoch =   95, loss =  0.9127)
Saving model (epoch =  104, loss =  0.9117)
Saving model (epoch =  107, loss =  0.8994)
Saving model (epoch =  110, loss =  0.8935)
Saving model (epoch =  116, loss =  0.8882)
Saving model (epoch =  124, loss =  0.8872)
Saving model (epoch =  128, loss =  0.8724)
Saving model (epoch =  134, loss =  0.8722)
Saving model (epoch =  139, loss =  0.8677)
Saving model (epoch =  146, loss =  0.8654)
Saving model (epoch =  156, loss =  0.8642)
Saving model (epoch =  159, loss =  0.8528)
Saving model (epoch =  167, loss =  0.8494)
Saving model (epoch =  173, loss =  0.8492)
Saving model (epoch =  176, loss =  0.8461)
Saving model (epoch =  178, loss =  0.8403)
Saving model (epoch =  182, loss =  0.8375)
Saving model (epoch =  199, loss =  0.8295)
Saving model (epoch =  212, loss =  0.8273)
Saving model (epoch =  235, loss =  0.8252)
Saving model (epoch =  238, loss =  0.8233)
Saving model (epoch =  251, loss =  0.8211)
Saving model (epoch =  253, loss =  0.8205)
Saving model (epoch =  258, loss =  0.8175)
Saving model (epoch =  284, loss =  0.8143)
Saving model (epoch =  308, loss =  0.8136)
Saving model (epoch =  312, loss =  0.8075)
Saving model (epoch =  324, loss =  0.8045)
Saving model (epoch =  400, loss =  0.8040)
Saving model (epoch =  404, loss =  0.8010)
Saving model (epoch =  466, loss =  0.7998)
Saving model (epoch =  525, loss =  0.7993)
Saving model (epoch =  561, loss =  0.7945)
Saving model (epoch =  584, loss =  0.7903)
Saving model (epoch =  667, loss =  0.7896)
Saving model (epoch =  717, loss =  0.7823)
Saving model (epoch =  776, loss =  0.7812)
Saving model (epoch =  835, loss =  0.7797)
Saving model (epoch =  866, loss =  0.7771)
Saving model (epoch =  919, loss =  0.7770)
Saving model (epoch =  933, loss =  0.7748)
Saving model (epoch =  965, loss =  0.7705)
Saving model (epoch =  1027, loss =  0.7674)
Saving model (epoch =  1119, loss =  0.7647)
Saving model (epoch =  1140, loss =  0.7643)
Saving model (epoch =  1196, loss =  0.7620)
Saving model (epoch =  1234, loss =  0.7616)
Saving model (epoch =  1243, loss =  0.7582)
Finished training after 1444 epochs
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plot_learning_curve(model_loss_record, title=<span class="string">&#x27;deep model&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p><img src="https://testingcf.jsdelivr.net/gh/WanpengXu/myPicGo/img/202207301912583.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">del</span> model   <span class="comment"># delete model variable</span></span><br><span class="line">model = NeuralNet(tr_set.dataset.dim).to(device)</span><br><span class="line">ckpt = torch.load(config[<span class="string">&#x27;save_path&#x27;</span>], map_location=<span class="string">&#x27;cpu&#x27;</span>)  <span class="comment"># Load your best model</span></span><br><span class="line">model.load_state_dict(ckpt)</span><br><span class="line">plot_pred(dv_set, model, device)  <span class="comment"># Show prediction on the validation set</span></span><br></pre></td></tr></table></figure>
<p><img src="https://testingcf.jsdelivr.net/gh/WanpengXu/myPicGo/img/202207301913401.png" alt="png"></p>
<h3 id="Testing-1"><a href="#Testing-1" class="headerlink" title="Testing"></a><strong>Testing</strong></h3><p>The predictions of your model on testing set will be stored at <code>pred.csv</code>.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">save_pred</span>(<span class="params">preds, file</span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27; Save predictions to specified file &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Saving results to &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(file))</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(file, <span class="string">&#x27;w&#x27;</span>) <span class="keyword">as</span> fp:</span><br><span class="line">        writer = csv.writer(fp)</span><br><span class="line">        writer.writerow([<span class="string">&#x27;id&#x27;</span>, <span class="string">&#x27;tested_positive&#x27;</span>])</span><br><span class="line">        <span class="keyword">for</span> i, p <span class="keyword">in</span> <span class="built_in">enumerate</span>(preds):</span><br><span class="line">            writer.writerow([i, p])</span><br><span class="line"></span><br><span class="line">preds = test(tt_set, model, device)  <span class="comment"># predict COVID-19 cases with your model</span></span><br><span class="line">save_pred(preds, <span class="string">&#x27;pred.csv&#x27;</span>)         <span class="comment"># save prediction file to pred.csv</span></span><br></pre></td></tr></table></figure>
<pre><code>Saving results to pred.csv
</code></pre><h2 id="Medium-Baseline"><a href="#Medium-Baseline" class="headerlink" title="Medium Baseline"></a><strong>Medium Baseline</strong></h2><p>Simple Baseline Code中标注了一些TODO，TA说只要完成这些TODO也许就可以达到Medium Baseline，TODO一共有这些：</p>
<ol>
<li>TODO: Using 40 states &amp; 2 tested_positive features (indices = 57 &amp; 75)</li>
<li>TODO: How to modify this model to achieve better performance?</li>
<li>TODO: you may implement L1/L2 regularization here</li>
<li>TODO: How to tune these hyper-parameters to improve your model’s performance?</li>
</ol>
<p>其中最重要的是TODO1，按TA所说修改即可达到Medium Baseline，即：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">feats = <span class="built_in">list</span>(<span class="built_in">range</span>(<span class="number">40</span>)) + [<span class="number">57</span>] + [<span class="number">75</span>]</span><br></pre></td></tr></table></figure>
<p>查看一下这两列数据，发现列名为<code>tested_positive</code>和<code>tested_positive.1</code>，即第1天阳性和第2天阳性，我们要预测的就是93th列——<code>tested_positive2</code></p>
<p><img src="https://testingcf.jsdelivr.net/gh/WanpengXu/myPicGo/img/202207310014714.png" alt=""></p>
<h2 id="Strong-Baseline"><a href="#Strong-Baseline" class="headerlink" title="Strong Baseline"></a>Strong Baseline</h2><p>TA给了以下提示</p>
<ol>
<li>Feature selection (what other features are useful?)</li>
<li>DNN architecture (layers? dimension? activation function?)</li>
<li>Training (mini-batch? optimizer? learning rate?)</li>
<li>L2 regularization</li>
<li>There are some mistakes in the sample code, can you find them?</li>
</ol>
<p>总的来说就是从三部分优化：1. Data、2. Network Structure、3. Optimization</p>
<p>在本题中最重要的还是Feature selection。</p>
<p>关于1：</p>
<p>可以使用SelectKBest函数，也可以对tested_positive2分析相关性（方法2，还没整理）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line">data = pd.read_csv(tr_path).iloc[<span class="number">1</span>:, <span class="number">1</span>:]</span><br><span class="line">x = data.iloc[:, <span class="number">0</span>:<span class="number">93</span>]</span><br><span class="line">y = data.iloc[:, <span class="number">93</span>]</span><br><span class="line"><span class="comment"># min-max normalization 极差归一化</span></span><br><span class="line">x = (x - x.<span class="built_in">min</span>()) / (x.<span class="built_in">max</span>() - x.<span class="built_in">min</span>())</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> SelectKBest</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> f_regression</span><br><span class="line"></span><br><span class="line">best_features = SelectKBest(score_func=f_regression, k = <span class="number">5</span>)</span><br><span class="line">fit = best_features.fit(x, y)</span><br><span class="line">df_scores = pd.DataFrame(fit.scores_)</span><br><span class="line">df_columns = pd.DataFrame(x.columns)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Concat two dataframes for better visualization </span></span><br><span class="line">feature_scores = pd.concat([df_columns, df_scores], axis=<span class="number">1</span>)</span><br><span class="line">feature_scores.columns = [<span class="string">&#x27;Specs&#x27;</span>, <span class="string">&#x27;Score&#x27;</span>]         <span class="comment"># Naming the dataframe columns</span></span><br><span class="line">feature_scores.nlargest(<span class="number">15</span>, <span class="string">&#x27;Score&#x27;</span>)                <span class="comment"># Print 15 best features</span></span><br><span class="line"><span class="comment"># feature_scores.nlargest(15, &#x27;Score&#x27;).index.values</span></span><br></pre></td></tr></table></figure>
<p><img src="https://testingcf.jsdelivr.net/gh/WanpengXu/myPicGo/img/202207310059387.png" alt=""></p>
<p>最后一行Score太低，去掉</p>
<p>可以得到改进：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">feats = [<span class="number">75</span>, <span class="number">57</span>, <span class="number">42</span>, <span class="number">60</span>, <span class="number">78</span>, <span class="number">43</span>, <span class="number">61</span>, <span class="number">79</span>, <span class="number">40</span>, <span class="number">58</span>, <span class="number">76</span>, <span class="number">41</span>, <span class="number">59</span>, <span class="number">77</span>]</span><br></pre></td></tr></table></figure>
<p>关于2：</p>
<p>该作业中“浅的”网络效果更好（Less is More），与后续作业不同，使用如下结构：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">self.net = nn.Sequential(</span><br><span class="line">    nn.Linear(input_dim, <span class="number">64</span>),</span><br><span class="line">    nn.BatchNorm1d(<span class="number">64</span>),       	<span class="comment"># 使用BN，加速模型训练</span></span><br><span class="line">    nn.LeakyReLU(),           	<span class="comment"># 更换activation function</span></span><br><span class="line">    nn.Dropout(p=<span class="number">0.35</span>),        	<span class="comment"># 使用Dropout，减小过拟合，注意不能在BN之前</span></span><br><span class="line">    nn.Linear(<span class="number">64</span>, <span class="number">1</span>)</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>关于3：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">config = &#123;</span><br><span class="line">    <span class="string">&#x27;n_epochs&#x27;</span>: <span class="number">10000</span>,              <span class="comment"># 因为有early_stop，所以大一点没有影响</span></span><br><span class="line">    <span class="string">&#x27;batch_size&#x27;</span>: <span class="number">200</span>,              <span class="comment"># 微调batchsize</span></span><br><span class="line">    <span class="string">&#x27;optimizer&#x27;</span>: <span class="string">&#x27;Adam&#x27;</span>,            <span class="comment"># 使用Adam优化器</span></span><br><span class="line">    <span class="string">&#x27;optim_hparas&#x27;</span>: &#123;               <span class="comment"># 完全使用默认参数</span></span><br><span class="line">        <span class="string">&#x27;lr&#x27;</span>: <span class="number">0.001</span>,                 </span><br><span class="line">        <span class="comment">#&#x27;momentum&#x27;: 0.9,             </span></span><br><span class="line">        <span class="comment">#&#x27;weight_decay&#x27;: 5e-4,</span></span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="string">&#x27;early_stop&#x27;</span>: <span class="number">500</span>,              <span class="comment"># 由于最后训练使用了所有数据，大一点影响不大</span></span><br><span class="line">    <span class="string">&#x27;save_path&#x27;</span>: <span class="string">&#x27;models/model.pth&#x27;</span>  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>关于4：</p>
<p>TA说使用L2正则化，但其实效果提升不大</p>
<p>L1正则化的损失函数</p>
<script type="math/tex; mode=display">
L = L(W)+\lambda \sum_{i=1}^{n}{|\omega_i|}</script><p>L2正则化的损失函数</p>
<script type="math/tex; mode=display">
L = L(W)+\lambda \sum_{i=1}^{n}\omega_i^2</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">cal_loss</span>(<span class="params">self, pred, target</span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27; Calculate loss &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> you may implement L1/L2 regularization here</span></span><br><span class="line">    regularization_lambda = <span class="number">0.00075</span></span><br><span class="line">    regularization_loss = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> model.parameters():</span><br><span class="line">        <span class="comment"># regularization_loss += torch.sum(abs(param))    # L1 regularization_loss</span></span><br><span class="line">        regularization_loss += torch.<span class="built_in">sum</span>(param ** <span class="number">2</span>)    <span class="comment"># L2 regularization_loss</span></span><br><span class="line">    <span class="keyword">return</span> self.criterion(pred, target) + regularization_lambda * regularization_loss</span><br></pre></td></tr></table></figure>
<p>关于5：</p>
<p><img src="https://testingcf.jsdelivr.net/gh/WanpengXu/myPicGo/img/202207302330850.png" alt=""></p>
<p><img src="https://testingcf.jsdelivr.net/gh/WanpengXu/myPicGo/img/202207302331743.png" alt=""></p>
<p>改为</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">return</span> torch.sqrt(self.criterion(pred, target))</span><br></pre></td></tr></table></figure>
<p>最后过了public的strong baseline，private差0.00x</p>
<p><img src="https://testingcf.jsdelivr.net/gh/WanpengXu/myPicGo/img/202207310211151.png" alt=""></p>
<h1 id="Homework-2-TIMIT-framewise-phoneme-classification"><a href="#Homework-2-TIMIT-framewise-phoneme-classification" class="headerlink" title="Homework 2: TIMIT framewise phoneme (classification)"></a><strong>Homework 2: TIMIT framewise phoneme (classification)</strong></h1><h2 id="Phoneme-Classification-Simple-Baseline"><a href="#Phoneme-Classification-Simple-Baseline" class="headerlink" title="Phoneme Classification - Simple Baseline"></a>Phoneme Classification - Simple Baseline</h2><p><img src="https://testingcf.jsdelivr.net/gh/WanpengXu/myPicGo/img/202208010306532.png" alt=""></p>
<p>首先可以看到Evaluation metric就是acc，所以越高越好。</p>
<p>跑通TA’s sample code就可以双过simple baseline</p>
<p><img src="https://testingcf.jsdelivr.net/gh/WanpengXu/myPicGo/img/202208010253944.png" alt=""></p>
<p>ps：跑一次挺慢的，colab挂上GPU也要15min左右一次。</p>
<p>CODE:</p>
<iframe src="https://nbviewer.org/github/WanpengXu/wanpengxu.github.io/blob/main/ipynb/SHARE_MLSpring2021_HW2_1_ipynb.ipynb" width="100%" height="1150"></iframe>


<h2 id="Phoneme-Classification-Strong-Baseline"><a href="#Phoneme-Classification-Strong-Baseline" class="headerlink" title="Phoneme Classification - Strong Baseline"></a>Phoneme Classification - Strong Baseline</h2><p>TA在PPT里给了一些Hint。</p>
<ol>
<li>Model architecture (layers? dimension? activation function?)</li>
<li>Training (batch size? optimizer? learning rate? epoch?)</li>
<li>Tips (batch norm? dropout? regularization?)</li>
</ol>
<p>基本还是Data、Structure、Optimization。</p>
<p>1-百万级数据集，所以VAL_RATIO设为5%</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">VAL_RATIO = <span class="number">0.05</span></span><br></pre></td></tr></table></figure>
<p><img src="https://testingcf.jsdelivr.net/gh/WanpengXu/myPicGo/img/202208021053405.png" alt=""></p>
<p>2-使用RAdam调优</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.optim <span class="keyword">import</span> RAdam</span><br><span class="line"></span><br><span class="line"><span class="comment"># fix random seed for reproducibility 可重复性</span></span><br><span class="line">same_seeds(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># get device </span></span><br><span class="line">device = get_device()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;DEVICE: <span class="subst">&#123;device&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># training parameters</span></span><br><span class="line">num_epoch = <span class="number">100</span>               <span class="comment"># number of training epoch</span></span><br><span class="line">learning_rate = <span class="number">0.0001</span>       <span class="comment"># learning rate</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># the path where checkpoint saved</span></span><br><span class="line">model_path = <span class="string">&#x27;./model.ckpt&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># create model, define a loss function, and optimizer</span></span><br><span class="line">model = Classifier().to(device)</span><br><span class="line">criterion = nn.CrossEntropyLoss()       <span class="comment"># auto append soft-max to network</span></span><br><span class="line"><span class="comment"># optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)</span></span><br><span class="line">optimizer = RAdam(model.parameters(), lr=learning_rate)</span><br></pre></td></tr></table></figure>
<h2 id="Hessian-Matrix"><a href="#Hessian-Matrix" class="headerlink" title="Hessian Matrix"></a>Hessian Matrix</h2><h1 id="Homework-3-Convolutional-Neural-Network-Image-Classfication-CNN"><a href="#Homework-3-Convolutional-Neural-Network-Image-Classfication-CNN" class="headerlink" title="Homework 3: Convolutional Neural Network - Image Classfication(CNN)"></a>Homework 3: Convolutional Neural Network - Image Classfication(CNN)</h1><h2 id="Simple-Baseline-1"><a href="#Simple-Baseline-1" class="headerlink" title="Simple Baseline"></a>Simple Baseline</h2><p>直接跑通可以双过simple baseline（需要fix一个好的seed），含注释的原始代码如下：</p>
<iframe src="https://nbviewer.org/github/WanpengXu/wanpengxu.github.io/blob/main/ipynb/HW03_xuwp.ipynb" width="100%" height="1150"></iframe>

<p><img src="https://testingcf.jsdelivr.net/gh/WanpengXu/myPicGo/img/202208060310798.png" alt=""></p>
<p>TA’s Slide: Build a <strong>convolutional neural network</strong> using <strong>labeled images</strong> with provided codes.</p>
<p>意思是只要使用给的CNN就可以过simple baseline。</p>
<h2 id="Medium-Baseline-1"><a href="#Medium-Baseline-1" class="headerlink" title="Medium Baseline"></a>Medium Baseline</h2><p>TA’s Slide: Improve the performance using labeled images with different model architectures or data augmentations.</p>
<p>TA给了两个方向different model architectures或者data augmentations。</p>
<p>还是在那个范围内：Data、Structure、Optimization。</p>
<p>我们这里尝试一下data augmentations</p>
<p>（好像model architectures要使用resnet？）</p>
<p>增强处：水平反转、旋转、<strong>自动增强</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torchvision.transforms.transforms <span class="keyword">import</span> RandomHorizontalFlip</span><br><span class="line">train_tfm1 = transforms.Compose([</span><br><span class="line">    transforms.Resize((<span class="number">128</span>, <span class="number">128</span>)),</span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">train_tfm2 = transforms.Compose([</span><br><span class="line">    transforms.Resize((<span class="number">128</span>, <span class="number">128</span>)),</span><br><span class="line">    transforms.RandomHorizontalFlip(p=<span class="number">1.0</span>),</span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">train_tfm3 = transforms.Compose([</span><br><span class="line">    transforms.Resize((<span class="number">128</span>, <span class="number">128</span>)),</span><br><span class="line">    transforms.RandomRotation(<span class="number">15</span>),</span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">train_tfm4 = transforms.Compose([</span><br><span class="line">    transforms.Resize((<span class="number">128</span>, <span class="number">128</span>)),</span><br><span class="line">    transforms.AutoAugment(),</span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">test_tfm = transforms.Compose([</span><br><span class="line">    transforms.Resize((<span class="number">128</span>, <span class="number">128</span>)),</span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">])</span><br></pre></td></tr></table></figure>
<p>然后把原data和增强data拼接起来，这样就有足够的train data了。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">train_set1 = DatasetFolder(<span class="string">&quot;food-11/training/labeled&quot;</span>, loader=<span class="keyword">lambda</span> x: Image.<span class="built_in">open</span>(x), extensions=<span class="string">&quot;jpg&quot;</span>, transform=train_tfm1)</span><br><span class="line">train_set2 = DatasetFolder(<span class="string">&quot;food-11/training/labeled&quot;</span>, loader=<span class="keyword">lambda</span> x: Image.<span class="built_in">open</span>(x), extensions=<span class="string">&quot;jpg&quot;</span>, transform=train_tfm2)</span><br><span class="line">train_set3 = DatasetFolder(<span class="string">&quot;food-11/training/labeled&quot;</span>, loader=<span class="keyword">lambda</span> x: Image.<span class="built_in">open</span>(x), extensions=<span class="string">&quot;jpg&quot;</span>, transform=train_tfm3)</span><br><span class="line">train_set4 = DatasetFolder(<span class="string">&quot;food-11/training/labeled&quot;</span>, loader=<span class="keyword">lambda</span> x: Image.<span class="built_in">open</span>(x), extensions=<span class="string">&quot;jpg&quot;</span>, transform=train_tfm4)</span><br><span class="line">...</span><br><span class="line">train_set = ConcatDataset([train_set1, train_set2, train_set3, train_set4])</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<p>然后再对learning rate和n_epochs调参调参调参。。。</p>
<p>结果：双过maedium baseline</p>
<p><img src="https://testingcf.jsdelivr.net/gh/WanpengXu/myPicGo/img/202208091404766.png" alt=""></p>
<p>注1：这里还有一个使用ImageNet的mean和var做标准化的方法，但是我不太会用，加上去效果没有更好。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">preprocess = transforms.Compose([</span><br><span class="line">    transforms.Resize(<span class="number">256</span>),</span><br><span class="line">    transforms.CenterCrop(<span class="number">224</span>),</span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">    transforms.Normalize(mean=[<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>], std=[<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>]),</span><br><span class="line">])</span><br></pre></td></tr></table></figure>
<p>注2：关于各种transform方法的实例，详见pytorch教程<a target="_blank" rel="noopener" href="https://pytorch.org/vision/stable/auto_examples/plot_transforms.html#sphx-glr-auto-examples-plot-transforms-py">ILLUSTRATION OF TRANSFORMS</a></p>
<h2 id="Strong-Baseline-1"><a href="#Strong-Baseline-1" class="headerlink" title="Strong Baseline"></a>Strong Baseline</h2><p>TA’s Slide: Improve the performance with <strong>additional unlabeled images</strong>.</p>
<p>TA将本次作业数据集中的大部分labeled images变成了unlabeled images，这里也是想要让学生练习Semi-supervised Learning（自监督学习）。</p>
<p>那么我们就来试一下Semi-supervised Learning。</p>
<p>首先构建PseudoDataset</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">PseudoDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dataset, indices, labels=[]</span>):</span><br><span class="line">        self.dataset = dataset</span><br><span class="line">        self.indices = indices</span><br><span class="line">        self.targets = labels</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        subset = self.dataset[self.indices[idx]]</span><br><span class="line">        imgs, _ = subset</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(self.targets) &gt; <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> imgs, self.targets[idx]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> subset</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.indices)</span><br></pre></td></tr></table></figure>
<p>然后修改get_pseudo_labels</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_pseudo_labels</span>(<span class="params">dataset, model, threshold=<span class="number">0.65</span></span>):</span><br><span class="line">    <span class="comment"># This functions generates pseudo-labels of a dataset using given model.</span></span><br><span class="line">    <span class="comment"># It returns an instance of DatasetFolder containing images whose prediction confidences exceed a given threshold.</span></span><br><span class="line">    <span class="comment"># You are NOT allowed to use any models trained on external data for pseudo-labeling.</span></span><br><span class="line">    device = <span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Construct a data loader.</span></span><br><span class="line">    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Make sure the model is in eval mode.</span></span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    <span class="comment"># Define softmax function.</span></span><br><span class="line">    softmax = nn.Softmax(dim=-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Iterate over the dataset by batches.</span></span><br><span class="line">    pseudo_probs = []</span><br><span class="line">    pseudo_labels = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> batch <span class="keyword">in</span> tqdm(data_loader):</span><br><span class="line">        img, _ = batch</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Forward the data</span></span><br><span class="line">        <span class="comment"># Using torch.no_grad() accelerates the forward process.</span></span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            logits = model(img.to(device))  <span class="comment"># model&#x27;s output</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Obtain the probability distributions by applying softmax on logits.</span></span><br><span class="line">        probs = softmax(logits)</span><br><span class="line">        probs_max, preds = probs.<span class="built_in">max</span>(<span class="number">1</span>)      <span class="comment"># probabilities, predicts</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># ---------- TODO ----------</span></span><br><span class="line">        <span class="comment"># Filter the data and construct a new dataset.</span></span><br><span class="line">        pseudo_probs.extend(probs_max.cpu().numpy().tolist())</span><br><span class="line">        pseudo_labels.extend(preds.cpu().numpy().tolist())</span><br><span class="line"></span><br><span class="line">    pseudo_indices = [i <span class="keyword">for</span> i, v <span class="keyword">in</span> <span class="built_in">enumerate</span>(pseudo_probs) <span class="keyword">if</span> v &gt;= threshold]</span><br><span class="line">    pseudo_set = PseudoDataset(dataset, pseudo_indices, [pseudo_labels[i] <span class="keyword">for</span> i <span class="keyword">in</span> pseudo_indices])</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Pseudo images above Confidence <span class="subst">&#123;threshold:<span class="number">.2</span>f&#125;</span>: <span class="subst">&#123;<span class="built_in">len</span>(pseudo_indices)&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">    model.train()</span><br><span class="line">    <span class="keyword">return</span> pseudo_set</span><br></pre></td></tr></table></figure>
<p>在Training中加入semi-supervised learning。</p>
<p>注意两点：</p>
<ol>
<li>当验证集最好精度达到一定值时再开始semi-supervised learning，否则model对unlabel datas的label不准确。</li>
<li>为了避免semi-supervised learning影响效率，每semi_turns轮进行一次pseudo。</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(n_epochs):</span><br><span class="line">    <span class="comment"># ---------- TODO ----------</span></span><br><span class="line">    <span class="comment"># In each epoch, relabel the unlabeled dataset for semi-supervised learning.</span></span><br><span class="line">    <span class="comment"># Then you can combine the labeled dataset and pseudo-labeled dataset for the training.</span></span><br><span class="line">    <span class="keyword">if</span> do_semi <span class="keyword">and</span> best_acc &gt; <span class="number">0.7</span> <span class="keyword">and</span> epoch % semi_turns == <span class="number">0</span>:</span><br><span class="line">        <span class="comment"># Obtain pseudo-labels for unlabeled data using trained model.</span></span><br><span class="line">        pseudo_set = get_pseudo_labels(unlabeled_set, model, threshold=threshold)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Construct a new dataset and a data loader for training.</span></span><br><span class="line">        <span class="comment"># This is used in semi-supervised learning only.</span></span><br><span class="line">        concat_dataset = ConcatDataset([train_set, pseudo_set])</span><br><span class="line">        <span class="comment"># biased_sampler = BiasedSampler(concat_dataset, batch_size=batch_size, minor_ratio=0.9)</span></span><br><span class="line">        <span class="comment"># train_loader = DataLoader(concat_dataset, batch_size=batch_size, shuffle=True, sampler=biased_sampler, num_workers=2, pin_memory=True)</span></span><br><span class="line">        train_loader = DataLoader(concat_dataset, batch_size=batch_size, shuffle=<span class="literal">True</span>, num_workers=<span class="number">8</span>, pin_memory=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># ---------- Training ----------</span></span><br><span class="line">    model.train()</span><br><span class="line"></span><br><span class="line">    train_loss = []</span><br><span class="line">    train_accs = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> batch <span class="keyword">in</span> tqdm(train_loader):</span><br><span class="line">		...</span><br><span class="line">    <span class="comment"># ---------- Validation ----------</span></span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line"></span><br><span class="line">    valid_loss = []</span><br><span class="line">    valid_accs = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> batch <span class="keyword">in</span> tqdm(valid_loader):</span><br><span class="line">		...</span><br><span class="line"></span><br><span class="line">    valid_loss = <span class="built_in">sum</span>(valid_loss) / <span class="built_in">len</span>(valid_loss)</span><br><span class="line">    valid_acc = <span class="built_in">sum</span>(valid_accs) / <span class="built_in">len</span>(valid_accs)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> valid_acc &gt; best_acc:</span><br><span class="line">        best_acc = valid_acc</span><br><span class="line">        ...</span><br><span class="line">    ...</span><br></pre></td></tr></table></figure>
<p>最后我又调整了transform和CNN architecture，然而这样的效果也不尽人意</p>
<p><img src="https://testingcf.jsdelivr.net/gh/WanpengXu/myPicGo/img/202208122250508.png" alt=""></p>
<p>最后一条可行的思路是加入Sampler，也就是说新拼接的数据集的dataloader应尽量选取labeled dataset，少选取pseudo dataset。</p>
<p>但是我发现自己再进行下去完全是在做无用功，等把后面的课听完再回来做一做试试，所以目前就到此为止了。</p>
<h1 id="Course中的小知识点"><a href="#Course中的小知识点" class="headerlink" title="Course中的小知识点"></a>Course中的小知识点</h1><h2 id="如何降低loss"><a href="#如何降低loss" class="headerlink" title="如何降低loss"></a>如何降低loss</h2><p><img src="https://testingcf.jsdelivr.net/gh/WanpengXu/myPicGo/img/202207311735584.png" alt="image-20220731173535493"></p>
<h2 id="data-Augmentation"><a href="#data-Augmentation" class="headerlink" title="data Augmentation"></a>data Augmentation</h2><p><img src="https://testingcf.jsdelivr.net/gh/WanpengXu/myPicGo/img/202207311714111.png" alt=""></p>
<h2 id="cross-validation"><a href="#cross-validation" class="headerlink" title="cross validation"></a>cross validation</h2><p>避免public testing set和private testing set差距过大</p>
<p><img src="https://testingcf.jsdelivr.net/gh/WanpengXu/myPicGo/img/202207311729739.png" alt="image-20220731172904653"></p>
<h2 id="k-fold-cross-validation"><a href="#k-fold-cross-validation" class="headerlink" title="k-fold cross validation"></a>k-fold cross validation</h2><p><img src="https://testingcf.jsdelivr.net/gh/WanpengXu/myPicGo/img/202207311732252.png" alt="image-20220731173229169"></p>
<h2 id="overfitting"><a href="#overfitting" class="headerlink" title="overfitting"></a>overfitting</h2><p><img src="https://testingcf.jsdelivr.net/gh/WanpengXu/myPicGo/img/202207311742374.png" alt="image-20220731174234292"></p>
<p>极端的例子：model在训练集上100%准确（loss=0），在测试集上准确度接近0%（loss很大）</p>
<p>原因：</p>
<p><img src="https://testingcf.jsdelivr.net/gh/WanpengXu/myPicGo/img/202207311744573.png" alt="image-20220731174434486"></p>
<p>model的flexibility</p>
<p>可以通过调整层数来constrain模型，或者增加training data（数据不够？可以Data Augmentation）</p>
<p><img src="https://testingcf.jsdelivr.net/gh/WanpengXu/myPicGo/img/202207311746818.png" alt="image-20220731174653735"></p>
<p>但不能constrain过度</p>
<p><img src="https://testingcf.jsdelivr.net/gh/WanpengXu/myPicGo/img/202207311747587.png" alt="image-20220731174735504"></p>
<h2 id="mismatch"><a href="#mismatch" class="headerlink" title="mismatch"></a>mismatch</h2><p><img src="https://testingcf.jsdelivr.net/gh/WanpengXu/myPicGo/img/202207311736745.png" alt="image-20220731173609612"></p>
<p>eg.</p>
<p>Hung-yi Lee：感谢大家为了让这个模型不准，上周五花了很多力气去点了这个video，所以这一天（2021.2.26）是今年观看人数最多的一天！</p>
<p>（哈哈哈哈笑死我了）</p>
<p><img src="https://testingcf.jsdelivr.net/gh/WanpengXu/myPicGo/img/202207311735131.png" alt="image-20220731173551040"></p>
<h2 id="flat-minima-sharp-minima"><a href="#flat-minima-sharp-minima" class="headerlink" title="flat minima/sharp minima"></a>flat minima/sharp minima</h2><p><del>好minima和坏minima</del></p>
<p><img src="https://testingcf.jsdelivr.net/gh/WanpengXu/myPicGo/img/202208010056670.png" alt="image-20220801005600511"></p>
<h2 id="vanilla-GD-GD-with-momentum"><a href="#vanilla-GD-GD-with-momentum" class="headerlink" title="vanilla GD/GD with momentum"></a>vanilla GD/GD with momentum</h2><p><del>总感觉最优化理论学过，但是有点不记得了</del></p>
<p>很像共轭梯度法！</p>
<p><img src="https://testingcf.jsdelivr.net/gh/WanpengXu/myPicGo/img/202208010115865.png" alt="image-20220801011510734"></p>
<p>hylee老师讲的：</p>
<p><img src="https://testingcf.jsdelivr.net/gh/WanpengXu/myPicGo/img/202208010105321.png" alt="image-20220801010502215"></p>
<p><img src="https://testingcf.jsdelivr.net/gh/WanpengXu/myPicGo/img/202208010105713.png" alt="image-20220801010516608"></p>
<h2 id="Adaptive-Learning-Rate（AdaGrad）"><a href="#Adaptive-Learning-Rate（AdaGrad）" class="headerlink" title="Adaptive Learning Rate（AdaGrad）"></a>Adaptive Learning Rate（AdaGrad）</h2><h3 id="Define"><a href="#Define" class="headerlink" title="Define"></a>Define</h3><p><img src="https://testingcf.jsdelivr.net/gh/WanpengXu/myPicGo/img/202208010128994.png" alt="image-20220801012811831"></p>
<p><img src="https://testingcf.jsdelivr.net/gh/WanpengXu/myPicGo/img/202208010128221.png" alt="image-20220801012854116"></p>
<p>root mean square：均方根</p>
<h3 id="Scheduling"><a href="#Scheduling" class="headerlink" title="Scheduling"></a>Scheduling</h3><h4 id="Decay"><a href="#Decay" class="headerlink" title="Decay"></a>Decay</h4><p><img src="https://testingcf.jsdelivr.net/gh/WanpengXu/myPicGo/img/202208010150725.png" alt="image-20220801015052600"></p>
<h4 id="warmup"><a href="#warmup" class="headerlink" title="warmup"></a>warmup</h4><p><img src="https://testingcf.jsdelivr.net/gh/WanpengXu/myPicGo/img/202208010156758.png" alt="image-20220801015630687"></p>
<h2 id="learning-rate-adapts-dynamically（RMSProp）"><a href="#learning-rate-adapts-dynamically（RMSProp）" class="headerlink" title="learning rate adapts dynamically（RMSProp）"></a>learning rate adapts dynamically（RMSProp）</h2><p><img src="https://testingcf.jsdelivr.net/gh/WanpengXu/myPicGo/img/202208010134196.png" alt="image-20220801013420084"></p>
<p><img src="https://testingcf.jsdelivr.net/gh/WanpengXu/myPicGo/img/202208010134265.png" alt="image-20220801013435158"></p>
<p>RMSProp：RMS+支点</p>
<h2 id="RMSProp-Momentum（Adam）"><a href="#RMSProp-Momentum（Adam）" class="headerlink" title="RMSProp + Momentum（Adam）"></a>RMSProp + Momentum（Adam）</h2><p><img src="https://testingcf.jsdelivr.net/gh/WanpengXu/myPicGo/img/202208010158101.png" alt="image-20220801015847962"></p>
<h2 id="Cross-entropy"><a href="#Cross-entropy" class="headerlink" title="Cross-entropy"></a>Cross-entropy</h2><p>交叉熵</p>
<script type="math/tex; mode=display">
e=-\sum_{i}{\hat{y}_ilny_i^{\prime}}</script><p><img src="https://testingcf.jsdelivr.net/gh/WanpengXu/myPicGo/img/202208010226798.png" alt=""></p>
<p><img src="https://testingcf.jsdelivr.net/gh/WanpengXu/myPicGo/img/202208010227409.png" alt=""></p>
<p>loss function会改变optimization的难度！</p>
<h2 id="Batch-Normalization-BN"><a href="#Batch-Normalization-BN" class="headerlink" title="Batch Normalization(BN)"></a>Batch Normalization(BN)</h2><p><img src="https://testingcf.jsdelivr.net/gh/WanpengXu/myPicGo/img/202208091155595.png" alt=""></p>
<p><img src="https://testingcf.jsdelivr.net/gh/WanpengXu/myPicGo/img/202208091155676.png" alt=""></p>
<p><img src="https://testingcf.jsdelivr.net/gh/WanpengXu/myPicGo/img/202208091155969.png" alt=""></p>
<p><img src="https://testingcf.jsdelivr.net/gh/WanpengXu/myPicGo/img/202208091155460.png" alt=""></p>
<h1 id="notation"><a href="#notation" class="headerlink" title="notation"></a>notation</h1><p>$\hat{y}$：y hat</p>
<p>$e^x$：exponential x</p>
<p>$\sum$：Summation</p>
<p>$y{\prime}$：y prime</p>
<p>logit：softmax的输入</p>
<p>$\tilde{x}$：x tilde, normalization后的x</p>
<p>element-wise product：元素对应乘积</p>
<p>inference：test</p>
<!--
可以通过嵌入jupyter网页阅读器来嵌入ipynb文件，到时候上传一个ipynb到自己博客目录，再用这个读取

<iframe src="https://nbviewer.jupyter.org/github/royalosyin/Python-Practical-Application-on-Climate-Variability-Studies/blob/master/ex12-Analysis%20of%20Monthly%20GPCP%20precipitation.ipynb" width="1000" height="2000"></iframe>
-->

        </div>

        
            <section class="post-copyright">
                
                    <p class="copyright-item">
                        <span>Author:</span>
                        <span>许</span>
                    </p>
                
                
                    <p class="copyright-item">
                        <span>Permalink:</span>
                        <span><a href="https://xuwp.top/MACHINE-LEARNING-2021-SPRING-HOMEWORKS.html">https://xuwp.top/MACHINE-LEARNING-2021-SPRING-HOMEWORKS.html</a></span>
                    </p>
                
                
                    <p class="copyright-item">
                        <span>License:</span>
                        <span>Copyright (c) 2022 <a target="_blank" rel="noopener" href="http://creativecommons.org/licenses/by-nc/4.0/">CC-BY-NC-4.0</a> LICENSE</span>
                    </p>
                
                

            </section>
        
        <section class="post-tags">
            <div>
                <span>Tag(s):</span>
                <span class="tag">
                    
                    
                        <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"># 深度学习</a>
                    
                        
                </span>
            </div>
            <div>
                <a href="javascript:window.history.back();">back</a>
                <span>· </span>
                <a href="/">home</a>
            </div>
        </section>
        <section class="post-nav">
            
                <a class="prev" rel="prev" href="/Dive-into-Deep-Learning.html">Mu Li. Dive into Deep Learning</a>
            
            
            <a class="next" rel="next" href="/computer-based-Exam.html">Computer-Based Exam</a>
            
        </section>


    </article>
</div>

            </div>
            <footer id="footer" class="footer">
    <div class="copyright">
        <span>Stay hungry. Stay foolish.</span>
    </div>
</footer>

    </div>
</body>

</html>