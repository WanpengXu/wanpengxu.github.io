<!DOCTYPE html>
<html lang="default">

<head>
    <meta charset="UTF-8">
<meta name="viewport"
      content="width=device-width, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">

    <meta name="author" content="许">





<title>Hung-yi Lee Machine Learning 2021 HWs | 须臾所学之野</title>



    <link rel="icon" href="/favicon.png">




    <!-- stylesheets list from _config.yml -->
    
    <link rel="stylesheet" href="/css/style.css">
    



    <!-- scripts list from _config.yml -->
    
    <script src="/js/script.js"></script>
    
    <script src="/js/tocbot.min.js"></script>
    



    
    
        <!-- MathJax配置，可通过单美元符号书写行内公式等 -->
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
    "HTML-CSS": {
        preferredFont: "TeX",
        availableFonts: ["STIX","TeX"],
        linebreaks: { automatic:true },
        EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50)
    },
    tex2jax: {
        inlineMath: [ ["$", "$"], ["\\(","\\)"] ],
        processEscapes: true,
        ignoreClass: "tex2jax_ignore|dno",
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
        equationNumbers: { autoNumber: "AMS" },
        noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } },
        Macros: { href: "{}" }
    },
    messageStyle: "none"
    });
</script>
<!-- 给MathJax元素添加has-jax class -->
<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>
<!-- 通过连接CDN加载MathJax的js代码 -->
<script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
</script>


    


<meta name="generator" content="Hexo 5.4.0"></head>

<body>
    <script>
        // this function is used to check current theme before page loaded.
        (() => {
            const currentTheme = window.localStorage && window.localStorage.getItem('theme') || '';
            const isDark = currentTheme === 'dark';
            const pagebody = document.getElementsByTagName('body')[0]
            if (isDark) {
                pagebody.classList.add('dark-theme');
                // mobile
                document.getElementById("mobile-toggle-theme").innerText = "· Dark"
            } else {
                pagebody.classList.remove('dark-theme');
                // mobile
                document.getElementById("mobile-toggle-theme").innerText = "· Light"
            }
        })();
    </script>

    <div class="wrapper">
        <header>
    <nav class="navbar">
        <div class="container">
            <div class="navbar-header header-logo"><a href="/">Xuwp&#39;s Blog</a></div>
            <div class="menu navbar-right">
                
                    <a class="menu-item" href="/archives">Posts</a>
                
                    <a class="menu-item" href="/category">Categories</a>
                
                    <a class="menu-item" href="/tag">Tags</a>
                
                    <a class="menu-item" href="/about">About</a>
                
                    <a class="menu-item" href="/link">Links</a>
                
                <input id="switch_default" type="checkbox" class="switch_default">
                <label for="switch_default" class="toggleBtn"></label>
            </div>
        </div>
    </nav>

    
    <nav class="navbar-mobile" id="nav-mobile">
        <div class="container">
            <div class="navbar-header">
                <div>
                    <a href="/">Xuwp&#39;s Blog</a><a id="mobile-toggle-theme">·&nbsp;Light</a>
                </div>
                <div class="menu-toggle" onclick="mobileBtn()">&#9776; Menu</div>
            </div>
            <div class="menu" id="mobile-menu">
                
                    <a class="menu-item" href="/archives">Posts</a>
                
                    <a class="menu-item" href="/category">Categories</a>
                
                    <a class="menu-item" href="/tag">Tags</a>
                
                    <a class="menu-item" href="/about">About</a>
                
                    <a class="menu-item" href="/link">Links</a>
                
            </div>
        </div>
    </nav>

</header>
<script>
    var mobileBtn = function f() {
        var toggleMenu = document.getElementsByClassName("menu-toggle")[0];
        var mobileMenu = document.getElementById("mobile-menu");
        if(toggleMenu.classList.contains("active")){
           toggleMenu.classList.remove("active")
            mobileMenu.classList.remove("active")
        }else{
            toggleMenu.classList.add("active")
            mobileMenu.classList.add("active")
        }
    }
</script>
            <div class="main">
                <div class="container">
    
    
        <div class="post-toc">
    <div class="tocbot-list">
    </div>
    <div class="tocbot-list-menu">
        <a class="tocbot-toc-expand" onclick="expand_toc()">Expand all</a>
        <a onclick="go_top()">Back to top</a>
        <a onclick="go_bottom()">Go to bottom</a>
    </div>
</div>

<script>
    var tocbot_timer;
    var DEPTH_MAX = 6;    // 为 6 时展开所有
    var tocbot_default_config = {
        tocSelector: '.tocbot-list',
        contentSelector: '.post-content',
        headingSelector: 'h1, h2, h3, h4, h5',
        orderedList: false,
        scrollSmooth: true,
        onClick: extend_click,
    };

    function extend_click() {
        clearTimeout(tocbot_timer);
        tocbot_timer = setTimeout(function () {
            tocbot.refresh(obj_merge(tocbot_default_config, { hasInnerContainers: true }));
        }, 420); // 这个值是由 tocbot 源码里定义的 scrollSmoothDuration 得来的
    }

    document.ready(function () {
        tocbot.init(obj_merge(tocbot_default_config, { collapseDepth: 1 }));
    });

    function expandToc() {
        var b = document.querySelector('.tocbot-toc-expand');
        var expanded = b.getAttribute('data-expanded');
        expanded ? b.removeAttribute('data-expanded') : b.setAttribute('data-expanded', true);
        tocbot.refresh(obj_merge(tocbot_default_config, { collapseDepth: expanded ? 1 : DEPTH_MAX }));
        b.innerText = expanded ? 'Expand all' : 'Collapse all';
    }

    function go_top() {
        window.scrollTo(0, 0);
    }

    function go_bottom() {
        window.scrollTo(0, document.body.scrollHeight);
    }

    function obj_merge(target, source) {
        for (var item in source) {
            if (source.hasOwnProperty(item)) {
                target[item] = source[item];
            }
        }
        return target;
    }
</script>
    

    
    <article class="post-wrap">
        <header class="post-header">
            <h1 class="post-title">Hung-yi Lee Machine Learning 2021 HWs</h1>
            
                <div class="post-meta">
                    
                        Author: <a itemprop="author" rel="author" href="/">许</a>
                    

                    
                        <span class="post-time">
                        Date: <a href="#">July 30, 2022&nbsp;&nbsp;19:17:05</a>
                        </span>
                    
                    
                        <span class="post-category">
                    Category:
                            
                                <a href="/categories/AI/">AI</a>
                            
                        </span>
                    
                </div>
            
        </header>

        <div class="post-content">
            <p>李宏毅教授2021年机器学习课程的作业，详见<a target="_blank" rel="noopener" href="https://speech.ee.ntu.edu.tw/~hylee/ml/2021-spring.php">ML 2021 Spring</a></p>
<p>感觉NTU学生的英语都很好啊hh，所以这也是本人第一次练习使用双语书写注释及内容</p>
<h1 id="Homework-1-COVID-19-Cases-Prediction-Regression"><a href="#Homework-1-COVID-19-Cases-Prediction-Regression" class="headerlink" title="Homework 1: COVID-19 Cases Prediction (Regression)"></a><strong>Homework 1: COVID-19 Cases Prediction (Regression)</strong></h1><h2 id="Simple-Baseline"><a href="#Simple-Baseline" class="headerlink" title="Simple Baseline"></a><strong>Simple Baseline</strong></h2><p>只要跑通助教的代码就可以，经注释后的代码如下。</p>
<h3 id="Download-Data"><a href="#Download-Data" class="headerlink" title="Download Data"></a><strong>Download Data</strong></h3><p>If the Google drive links are dead, you can download data from <a target="_blank" rel="noopener" href="https://www.kaggle.com/c/ml2021spring-hw1/data">kaggle</a>, and upload data manually to the workspace.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">tr_path = <span class="string">&#x27;covid.train.csv&#x27;</span>   <span class="comment"># path to training data</span></span><br><span class="line">tt_path = <span class="string">&#x27;covid.test.csv&#x27;</span>    <span class="comment"># path to testing data</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Google Colab special command</span></span><br><span class="line">!gdown --<span class="built_in">id</span> <span class="string">&#x27;19CCyCgJrUxtvgZF53vnctJiOJ23T5mqF&#x27;</span> --output covid.train.csv</span><br><span class="line">!gdown --<span class="built_in">id</span> <span class="string">&#x27;1CE240jLm2npU-tdz81-oVKEF3T2yfT1O&#x27;</span> --output covid.test.csv</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">/usr/local/lib/python3.7/dist-packages/gdown/cli.py:131: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don&#x27;t need to pass it anymore to use a file ID.</span><br><span class="line">  category=FutureWarning,</span><br><span class="line">Downloading...</span><br><span class="line">From: https://drive.google.com/uc?id=19CCyCgJrUxtvgZF53vnctJiOJ23T5mqF</span><br><span class="line">To: /content/covid.train.csv</span><br><span class="line">100% 2.00M/2.00M [00:00&lt;00:00, 202MB/s]</span><br><span class="line">/usr/local/lib/python3.7/dist-packages/gdown/cli.py:131: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don&#x27;t need to pass it anymore to use a file ID.</span><br><span class="line">  category=FutureWarning,</span><br><span class="line">Downloading...</span><br><span class="line">From: https://drive.google.com/uc?id=1CE240jLm2npU-tdz81-oVKEF3T2yfT1O</span><br><span class="line">To: /content/covid.test.csv</span><br><span class="line">100% 651k/651k [00:00&lt;00:00, 129MB/s]</span><br></pre></td></tr></table></figure>
<h3 id="Import-Some-Packages"><a href="#Import-Some-Packages" class="headerlink" title="Import Some Packages"></a><strong>Import Some Packages</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># PyTorch</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset, DataLoader</span><br><span class="line"></span><br><span class="line"><span class="comment"># For data preprocess</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> csv</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="comment"># For plotting</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> matplotlib.pyplot <span class="keyword">import</span> figure</span><br><span class="line"></span><br><span class="line"><span class="comment"># set a random seed for reproducibility（可重复性）</span></span><br><span class="line">myseed = <span class="number">42069</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># if True, causes cuDNN to only use deterministic convolution algorithms.(确定性卷积算法)</span></span><br><span class="line"><span class="comment"># 确定性卷积算法 is, algorithms which, given the same input, and when run on the same software and hardware, always produce the same output</span></span><br><span class="line"><span class="comment"># Performance： nondeterministic algorithms &gt; deterministic algorithms (In most cases)</span></span><br><span class="line">torch.backends.cudnn.deterministic = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># if True, causes cuDNN to benchmark multiple convolution algorithms and select the fastest.</span></span><br><span class="line"><span class="comment"># 如果卷积网络结构不是动态变化的，即网络的输入 (batch size，图像的大小，输入的通道) 是固定的，设置为True。由于HW1并未涉及卷积运算，所以设置为False</span></span><br><span class="line">torch.backends.cudnn.benchmark = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># set the random seed for numpy, torch, torch.cuda</span></span><br><span class="line">np.random.seed(myseed)</span><br><span class="line">torch.manual_seed(myseed)</span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    torch.cuda.manual_seed_all(myseed)</span><br></pre></td></tr></table></figure>
<h3 id="Some-Utilities"><a href="#Some-Utilities" class="headerlink" title="Some Utilities"></a><strong>Some Utilities</strong></h3><p>You do not need to modify this part.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_device</span>():</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27; Get device (if GPU is available, use GPU) &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">&#x27;cuda&#x27;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&#x27;cpu&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_learning_curve</span>(<span class="params">loss_record, title=<span class="string">&#x27;&#x27;</span></span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27; Plot learning curve of your DNN (train &amp; dev loss) &#x27;&#x27;&#x27;</span></span><br><span class="line">    total_steps = <span class="built_in">len</span>(loss_record[<span class="string">&#x27;train&#x27;</span>])</span><br><span class="line">    x_1 = <span class="built_in">range</span>(total_steps)</span><br><span class="line">    x_2 = x_1[::<span class="built_in">len</span>(loss_record[<span class="string">&#x27;train&#x27;</span>]) // <span class="built_in">len</span>(loss_record[<span class="string">&#x27;dev&#x27;</span>])]</span><br><span class="line">    figure(figsize=(<span class="number">6</span>, <span class="number">4</span>))              <span class="comment"># Set the weight, height of the figure(in feet英尺)</span></span><br><span class="line">    plt.plot(x_1, loss_record[<span class="string">&#x27;train&#x27;</span>], c=<span class="string">&#x27;tab:red&#x27;</span>, label=<span class="string">&#x27;train&#x27;</span>)</span><br><span class="line">    plt.plot(x_2, loss_record[<span class="string">&#x27;dev&#x27;</span>], c=<span class="string">&#x27;tab:cyan&#x27;</span>, label=<span class="string">&#x27;dev&#x27;</span>)</span><br><span class="line">    plt.ylim(<span class="number">0.0</span>, <span class="number">5.</span>)                   <span class="comment"># Limit the y-axis range to 0.0~5.0</span></span><br><span class="line">    plt.xlabel(<span class="string">&#x27;Training steps&#x27;</span>)</span><br><span class="line">    plt.ylabel(<span class="string">&#x27;MSE loss&#x27;</span>)</span><br><span class="line">    plt.title(<span class="string">&#x27;Learning curve of &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(title))</span><br><span class="line">    plt.legend()                        <span class="comment"># Place a legend（图例，就是每种线代表什么） on the Axes.</span></span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_pred</span>(<span class="params">dv_set, model, device, lim=<span class="number">35.</span>, preds=<span class="literal">None</span>, targets=<span class="literal">None</span></span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27; Plot prediction of your DNN &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="keyword">if</span> preds <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">or</span> targets <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        model.<span class="built_in">eval</span>()                    <span class="comment"># Sets the module in evaluation mode.</span></span><br><span class="line">        preds, targets = [], []</span><br><span class="line">        <span class="keyword">for</span> x, y <span class="keyword">in</span> dv_set:</span><br><span class="line">            x, y = x.to(device), y.to(device)</span><br><span class="line">            <span class="keyword">with</span> torch.no_grad():       <span class="comment"># Context-manager that disabled gradient calculation.（无视梯度上下文，直接requires_grad=False，加速用）</span></span><br><span class="line">                pred = model(x)</span><br><span class="line">                preds.append(pred.detach().cpu())   <span class="comment"># Tensor.detach(): Returns a new Tensor, detached from the current graph. Tensor.cpu(): Returns a copy of this object in CPU memory.</span></span><br><span class="line">                targets.append(y.detach().cpu())</span><br><span class="line">        preds = torch.cat(preds, dim=<span class="number">0</span>).numpy()     <span class="comment"># Concatenates the given sequence of seq tensors in the given dimension. （和cpp中cat类似，这里就是list-&gt;Tensor-&gt;ndarray）</span></span><br><span class="line">        targets = torch.cat(targets, dim=<span class="number">0</span>).numpy()</span><br><span class="line"></span><br><span class="line">    figure(figsize=(<span class="number">5</span>, <span class="number">5</span>))</span><br><span class="line">    plt.scatter(targets, preds, c=<span class="string">&#x27;r&#x27;</span>, alpha=<span class="number">0.5</span>)   <span class="comment"># 散点图</span></span><br><span class="line">    plt.plot([-<span class="number">0.2</span>, lim], [-<span class="number">0.2</span>, lim], c=<span class="string">&#x27;b&#x27;</span>)</span><br><span class="line">    plt.xlim(-<span class="number">0.2</span>, lim)</span><br><span class="line">    plt.ylim(-<span class="number">0.2</span>, lim)</span><br><span class="line">    plt.xlabel(<span class="string">&#x27;ground truth value&#x27;</span>)</span><br><span class="line">    plt.ylabel(<span class="string">&#x27;predicted value&#x27;</span>)</span><br><span class="line">    plt.title(<span class="string">&#x27;Ground Truth v.s. Prediction&#x27;</span>)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>
<h3 id="Preprocess"><a href="#Preprocess" class="headerlink" title="Preprocess"></a><strong>Preprocess</strong></h3><p>We have three kinds of datasets:</p>
<ul>
<li><code>train</code>: for training</li>
<li><code>dev</code>: for validation</li>
<li><code>test</code>: for testing (w/o target value)</li>
</ul>
<h4 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a><strong>Dataset</strong></h4><p>The <code>COVID19Dataset</code> below does:</p>
<ul>
<li>read <code>.csv</code> files</li>
<li>extract features</li>
<li>split <code>covid.train.csv</code> into train/dev sets</li>
<li>normalize features</li>
</ul>
<p>Finishing <code>TODO</code> below might make you pass medium baseline.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">COVID19Dataset</span>(<span class="params">Dataset</span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27; Dataset for loading and preprocessing the COVID19 dataset &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,</span></span></span><br><span class="line"><span class="params"><span class="function">                 path,</span></span></span><br><span class="line"><span class="params"><span class="function">                 mode=<span class="string">&#x27;train&#x27;</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                 target_only=<span class="literal">False</span></span>):</span></span><br><span class="line">        self.mode = mode</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Read data into numpy arrays</span></span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(path, <span class="string">&#x27;r&#x27;</span>) <span class="keyword">as</span> fp:</span><br><span class="line">            data = <span class="built_in">list</span>(csv.reader(fp))</span><br><span class="line">            data = np.array(data[<span class="number">1</span>:])[:, <span class="number">1</span>:].astype(<span class="built_in">float</span>)    <span class="comment"># Remove the 0th row and 0th column 获取数值数据</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 默认情况，选取除最后一列的全部列（0~92）作为train data</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> target_only:</span><br><span class="line">            feats = <span class="built_in">list</span>(<span class="built_in">range</span>(<span class="number">93</span>))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># <span class="doctag">TODO:</span> Using 40 states &amp; 2 tested_positive features (indices = 57 &amp; 75)</span></span><br><span class="line">            <span class="comment"># feats = list(range(40)) + [57] + [75]</span></span><br><span class="line">            <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 最后一列数据不同，训练集有最后一列（93th），测试集没有93th column</span></span><br><span class="line">        <span class="keyword">if</span> mode == <span class="string">&#x27;test&#x27;</span>:</span><br><span class="line">            <span class="comment"># Testing data</span></span><br><span class="line">            <span class="comment"># data: 893 x 93 (40 states + day 1 (18) + day 2 (18) + day 3 (17))</span></span><br><span class="line">            <span class="comment"># 测试集不含lable，只需操作data</span></span><br><span class="line">            data = data[:, feats]       <span class="comment"># all rows，&lt;feats&gt; columns</span></span><br><span class="line">            self.data = torch.FloatTensor(data)   <span class="comment"># ndarray -&gt; Tensor(Float32)</span></span><br><span class="line">        <span class="keyword">elif</span> mode <span class="keyword">in</span> [<span class="string">&#x27;train&#x27;</span>, <span class="string">&#x27;dev&#x27;</span>]:</span><br><span class="line">            <span class="comment"># Training data (train/dev sets)</span></span><br><span class="line">            <span class="comment"># data: 2700 x 94 (40 states + day 1 (18) + day 2 (18) + day 3 (18))</span></span><br><span class="line">            target = data[:, -<span class="number">1</span>]        <span class="comment"># all rows, 94th column</span></span><br><span class="line">            data = data[:, feats]       <span class="comment"># all rows, &lt;feats(default: 0th~93th)&gt; columns</span></span><br><span class="line">            </span><br><span class="line">            <span class="comment"># Splitting training data into train &amp; dev sets</span></span><br><span class="line">            <span class="comment"># len(train sets):len(dev sets) = 9:1 （每10个中9个作为train，1个作为dev）</span></span><br><span class="line">            <span class="keyword">if</span> mode == <span class="string">&#x27;train&#x27;</span>:</span><br><span class="line">                indices = [i <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(data)) <span class="keyword">if</span> i % <span class="number">10</span> != <span class="number">0</span>]</span><br><span class="line">            <span class="keyword">elif</span> mode == <span class="string">&#x27;dev&#x27;</span>:</span><br><span class="line">                indices = [i <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(data)) <span class="keyword">if</span> i % <span class="number">10</span> == <span class="number">0</span>]</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># Convert data into PyTorch tensors</span></span><br><span class="line">            self.data = torch.FloatTensor(data[indices])</span><br><span class="line">            self.target = torch.FloatTensor(target[indices])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Normalize features (you may remove this part to see what will happen)</span></span><br><span class="line">        <span class="comment"># 归一化特征（通常添加后可以提升模型训练的效果）</span></span><br><span class="line">        <span class="comment"># （第i维数据 - 第i维数据的平均值）/（第i维数据的标准差）</span></span><br><span class="line">        self.data[:, <span class="number">40</span>:] = \</span><br><span class="line">            (self.data[:, <span class="number">40</span>:] - self.data[:, <span class="number">40</span>:].mean(dim=<span class="number">0</span>, keepdim=<span class="literal">True</span>)) \</span><br><span class="line">            / self.data[:, <span class="number">40</span>:].std(dim=<span class="number">0</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        self.dim = self.data.shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Finished reading the &#123;&#125; set of COVID19 Dataset (&#123;&#125; samples found, each dim = &#123;&#125;)&#x27;</span></span><br><span class="line">              .<span class="built_in">format</span>(mode, <span class="built_in">len</span>(self.data), self.dim))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span>(<span class="params">self, index</span>):</span></span><br><span class="line">        <span class="comment"># Returns one sample at a time</span></span><br><span class="line">        <span class="keyword">if</span> self.mode <span class="keyword">in</span> [<span class="string">&#x27;train&#x27;</span>, <span class="string">&#x27;dev&#x27;</span>]:</span><br><span class="line">            <span class="comment"># For training</span></span><br><span class="line">            <span class="keyword">return</span> self.data[index], self.target[index]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># For testing (no target)</span></span><br><span class="line">            <span class="keyword">return</span> self.data[index]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="comment"># Returns the size of the dataset</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.data)</span><br></pre></td></tr></table></figure>
<h4 id="DataLoader"><a href="#DataLoader" class="headerlink" title="DataLoader"></a><strong>DataLoader</strong></h4><p>A <code>DataLoader</code> loads data from a given <code>Dataset</code> into batches.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">prep_dataloader</span>(<span class="params">path, mode, batch_size, n_jobs=<span class="number">0</span>, target_only=<span class="literal">False</span></span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27; Generates a dataset, then is put into a dataloader. &#x27;&#x27;&#x27;</span></span><br><span class="line">    dataset = COVID19Dataset(path, mode=mode, target_only=target_only)  <span class="comment"># Construct dataset</span></span><br><span class="line">    <span class="comment"># num_workers (int, optional) – how many subprocesses to use for data loading. 0 means that the data will be loaded in the main process. (default: 0)</span></span><br><span class="line">    <span class="comment"># pin_memory (bool, optional) – If True, the data loader will copy Tensors into device/CUDA pinned memory before returning them.</span></span><br><span class="line">    dataloader = DataLoader(</span><br><span class="line">        dataset, batch_size,</span><br><span class="line">        shuffle=(mode == <span class="string">&#x27;train&#x27;</span>), drop_last=<span class="literal">False</span>,</span><br><span class="line">        num_workers=n_jobs, pin_memory=<span class="literal">True</span>)                            <span class="comment"># Construct dataloader</span></span><br><span class="line">    <span class="keyword">return</span> dataloader</span><br></pre></td></tr></table></figure>
<h3 id="Deep-Neural-Network"><a href="#Deep-Neural-Network" class="headerlink" title="Deep Neural Network"></a><strong>Deep Neural Network</strong></h3><p><code>NeuralNet</code> is an <code>nn.Module</code> designed for regression.<br>The DNN consists of 2 fully-connected layers with ReLU activation.<br>This module also included a function <code>cal_loss</code> for calculating loss.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NeuralNet</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27; A simple fully-connected deep neural network &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, input_dim</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(NeuralNet, self).__init__()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Define your neural network here</span></span><br><span class="line">        <span class="comment"># <span class="doctag">TODO:</span> How to modify this model to achieve better performance?</span></span><br><span class="line">        <span class="comment"># A sequential container.（顺序容器） Modules will be added to it in the order they are passed in the constructor. </span></span><br><span class="line">        self.net = nn.Sequential(</span><br><span class="line">            nn.Linear(input_dim, <span class="number">64</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(<span class="number">64</span>, <span class="number">1</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Mean squared error loss</span></span><br><span class="line">        <span class="comment"># reduction (string, optional) – Specifies the reduction to apply to the output: &#x27;none&#x27; | &#x27;mean&#x27; | &#x27;sum&#x27;.</span></span><br><span class="line">        <span class="comment"># &#x27;none&#x27;: no reduction will be applied, </span></span><br><span class="line">        <span class="comment"># &#x27;mean&#x27;: the sum of the output will be divided by the number of elements in the output, </span></span><br><span class="line">        <span class="comment"># &#x27;sum&#x27;: the output will be summed. (Default)</span></span><br><span class="line">        self.criterion = nn.MSELoss(reduction=<span class="string">&#x27;mean&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27; Given input of size (batch_size x input_dim), compute output of the network &#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="comment"># Returns a tensor with all the dimensions of input of size 1 removed.</span></span><br><span class="line">        <span class="keyword">return</span> self.net(x).squeeze(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">cal_loss</span>(<span class="params">self, pred, target</span>):</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27; Calculate loss &#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="comment"># <span class="doctag">TODO:</span> you may implement L1/L2 regularization here</span></span><br><span class="line">        <span class="keyword">return</span> self.criterion(pred, target)</span><br></pre></td></tr></table></figure>
<h3 id="Train-Dev-Test"><a href="#Train-Dev-Test" class="headerlink" title="Train/Dev/Test"></a><strong>Train/Dev/Test</strong></h3><h4 id="Training"><a href="#Training" class="headerlink" title="Training"></a><strong>Training</strong></h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span>(<span class="params">tr_set, dv_set, model, config, device</span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27; DNN training &#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">    n_epochs = config[<span class="string">&#x27;n_epochs&#x27;</span>]  <span class="comment"># Maximum number of epochs</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Setup optimizer</span></span><br><span class="line">    optimizer = <span class="built_in">getattr</span>(torch.optim, config[<span class="string">&#x27;optimizer&#x27;</span>])(</span><br><span class="line">        model.parameters(), **config[<span class="string">&#x27;optim_hparas&#x27;</span>])</span><br><span class="line"></span><br><span class="line">    min_mse = <span class="number">1000.</span></span><br><span class="line">    loss_record = &#123;<span class="string">&#x27;train&#x27;</span>: [], <span class="string">&#x27;dev&#x27;</span>: []&#125;      <span class="comment"># for recording training loss</span></span><br><span class="line">    early_stop_cnt = <span class="number">0</span></span><br><span class="line">    epoch = <span class="number">0</span></span><br><span class="line">    <span class="keyword">while</span> epoch &lt; n_epochs:</span><br><span class="line">        model.train()                           <span class="comment"># set model to training mode</span></span><br><span class="line">        <span class="keyword">for</span> x, y <span class="keyword">in</span> tr_set:                     <span class="comment"># iterate through the dataloader</span></span><br><span class="line">            optimizer.zero_grad()               <span class="comment"># set gradient to zero</span></span><br><span class="line">            x, y = x.to(device), y.to(device)   <span class="comment"># move data to device (cpu/cuda)</span></span><br><span class="line">            pred = model(x)                     <span class="comment"># forward pass (compute output)</span></span><br><span class="line">            mse_loss = model.cal_loss(pred, y)  <span class="comment"># compute loss</span></span><br><span class="line">            mse_loss.backward()                 <span class="comment"># compute gradient (backpropagation)</span></span><br><span class="line">            optimizer.step()                    <span class="comment"># update model with optimizer</span></span><br><span class="line">            loss_record[<span class="string">&#x27;train&#x27;</span>].append(mse_loss.detach().cpu().item())</span><br><span class="line"></span><br><span class="line">        <span class="comment"># After each epoch, test your model on the validation (development) set.</span></span><br><span class="line">        dev_mse = dev(dv_set, model, device)</span><br><span class="line">        <span class="keyword">if</span> dev_mse &lt; min_mse:</span><br><span class="line">            <span class="comment"># Save model if your model improved</span></span><br><span class="line">            min_mse = dev_mse</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&#x27;Saving model (epoch = <span class="subst">&#123;epoch + <span class="number">1</span> : 4d&#125;</span>, loss = <span class="subst">&#123;min_mse : <span class="number">.4</span>f&#125;</span>)&#x27;</span>)</span><br><span class="line">            <span class="comment"># model.state_dict(): Returns a dictionary containing a whole state of the module.</span></span><br><span class="line">            torch.save(model.state_dict(), config[<span class="string">&#x27;save_path&#x27;</span>])  <span class="comment"># Save model to specified path &lt;config[&#x27;save_path&#x27;]&gt;</span></span><br><span class="line">            early_stop_cnt = <span class="number">0</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            early_stop_cnt += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        epoch += <span class="number">1</span></span><br><span class="line">        loss_record[<span class="string">&#x27;dev&#x27;</span>].append(dev_mse)</span><br><span class="line">        <span class="keyword">if</span> early_stop_cnt &gt; config[<span class="string">&#x27;early_stop&#x27;</span>]:</span><br><span class="line">            <span class="comment"># Stop training if your model stops improving for &quot;config[&#x27;early_stop&#x27;]&quot; epochs. 早停</span></span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Finished training after &#123;&#125; epochs&#x27;</span>.<span class="built_in">format</span>(epoch))</span><br><span class="line">    <span class="keyword">return</span> min_mse, loss_record</span><br></pre></td></tr></table></figure>
<h4 id="Validation"><a href="#Validation" class="headerlink" title="Validation"></a><strong>Validation</strong></h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dev</span>(<span class="params">dv_set, model, device</span>):</span></span><br><span class="line">    model.<span class="built_in">eval</span>()                                <span class="comment"># set model to evalutation mode</span></span><br><span class="line">    total_loss = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> x, y <span class="keyword">in</span> dv_set:                         <span class="comment"># iterate through the dataloader</span></span><br><span class="line">        x, y = x.to(device), y.to(device)       <span class="comment"># move data to device (cpu/cuda)</span></span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():                   <span class="comment"># disable gradient calculation</span></span><br><span class="line">            pred = model(x)                     <span class="comment"># forward pass (compute output)</span></span><br><span class="line">            mse_loss = model.cal_loss(pred, y)  <span class="comment"># compute loss</span></span><br><span class="line">        total_loss += mse_loss.detach().cpu().item() * <span class="built_in">len</span>(x)  <span class="comment"># accumulate loss</span></span><br><span class="line">    total_loss = total_loss / <span class="built_in">len</span>(dv_set.dataset)              <span class="comment"># compute averaged loss</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> total_loss</span><br></pre></td></tr></table></figure>
<h4 id="Testing"><a href="#Testing" class="headerlink" title="Testing"></a><strong>Testing</strong></h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test</span>(<span class="params">tt_set, model, device</span>):</span></span><br><span class="line">    model.<span class="built_in">eval</span>()                                <span class="comment"># set model to evalutation mode</span></span><br><span class="line">    preds = []</span><br><span class="line">    <span class="keyword">for</span> x <span class="keyword">in</span> tt_set:                            <span class="comment"># iterate through the dataloader</span></span><br><span class="line">        x = x.to(device)                        <span class="comment"># move data to device (cpu/cuda)</span></span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():                   <span class="comment"># disable gradient calculation</span></span><br><span class="line">            pred = model(x)                     <span class="comment"># forward pass (compute output)</span></span><br><span class="line">            preds.append(pred.detach().cpu())   <span class="comment"># collect prediction 记录预测值</span></span><br><span class="line">    preds = torch.cat(preds, dim=<span class="number">0</span>).numpy()     <span class="comment"># concatenate all predictions and convert to a numpy array (list-&gt;tensor-&gt;ndarray) </span></span><br><span class="line">    <span class="keyword">return</span> preds</span><br></pre></td></tr></table></figure>
<h3 id="Setup-Hyper-parameters"><a href="#Setup-Hyper-parameters" class="headerlink" title="Setup Hyper-parameters"></a><strong>Setup Hyper-parameters</strong></h3><p><code>config</code> contains hyper-parameters for training and the path to save your model.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">device = get_device()                 <span class="comment"># get the current available device (&#x27;cpu&#x27; or &#x27;cuda&#x27;)</span></span><br><span class="line">os.makedirs(<span class="string">&#x27;models&#x27;</span>, exist_ok=<span class="literal">True</span>)  <span class="comment"># The trained model will be saved to ./models/, exist_ok：只有在目录不存在时创建目录</span></span><br><span class="line">target_only = <span class="literal">False</span>                   <span class="comment"># <span class="doctag">TODO:</span> Using 40 states &amp; 2 tested_positive features</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># <span class="doctag">TODO:</span> How to tune these hyper-parameters to improve your model&#x27;s performance?</span></span><br><span class="line">config = &#123;</span><br><span class="line">    <span class="string">&#x27;n_epochs&#x27;</span>: <span class="number">3000</span>,                <span class="comment"># maximum number of epochs</span></span><br><span class="line">    <span class="string">&#x27;batch_size&#x27;</span>: <span class="number">270</span>,               <span class="comment"># mini-batch size for dataloader</span></span><br><span class="line">    <span class="string">&#x27;optimizer&#x27;</span>: <span class="string">&#x27;SGD&#x27;</span>,              <span class="comment"># optimization algorithm (optimizer in torch.optim)</span></span><br><span class="line">    <span class="string">&#x27;optim_hparas&#x27;</span>: &#123;                <span class="comment"># hyper-parameters for the optimizer (depends on which optimizer you are using)</span></span><br><span class="line">        <span class="string">&#x27;lr&#x27;</span>: <span class="number">0.001</span>,                 <span class="comment"># learning rate of SGD</span></span><br><span class="line">        <span class="string">&#x27;momentum&#x27;</span>: <span class="number">0.9</span>              <span class="comment"># momentum for SGD</span></span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="string">&#x27;early_stop&#x27;</span>: <span class="number">200</span>,               <span class="comment"># early stopping epochs (the number epochs since your model&#x27;s last improvement)</span></span><br><span class="line">    <span class="string">&#x27;save_path&#x27;</span>: <span class="string">&#x27;models/model.pth&#x27;</span>  <span class="comment"># your model will be saved here</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="Load-data-and-model"><a href="#Load-data-and-model" class="headerlink" title="Load data and model"></a><strong>Load data and model</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tr_set = prep_dataloader(tr_path, <span class="string">&#x27;train&#x27;</span>, config[<span class="string">&#x27;batch_size&#x27;</span>], target_only=target_only)</span><br><span class="line">dv_set = prep_dataloader(tr_path, <span class="string">&#x27;dev&#x27;</span>, config[<span class="string">&#x27;batch_size&#x27;</span>], target_only=target_only)</span><br><span class="line">tt_set = prep_dataloader(tt_path, <span class="string">&#x27;test&#x27;</span>, config[<span class="string">&#x27;batch_size&#x27;</span>], target_only=target_only)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Finished reading the train set of COVID19 Dataset (2430 samples found, each dim = 93)</span><br><span class="line">Finished reading the dev set of COVID19 Dataset (270 samples found, each dim = 93)</span><br><span class="line">Finished reading the test set of COVID19 Dataset (893 samples found, each dim = 93)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model = NeuralNet(tr_set.dataset.dim).to(device)  # Construct model and move to device</span><br></pre></td></tr></table></figure>
<h3 id="Start-Training"><a href="#Start-Training" class="headerlink" title="Start Training!"></a><strong>Start Training!</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model_loss, model_loss_record = train(tr_set, dv_set, model, config, device)</span><br></pre></td></tr></table></figure>
<pre><code>Saving model (epoch =    1, loss =  78.8524)
Saving model (epoch =    2, loss =  37.6170)
Saving model (epoch =    3, loss =  26.1203)
Saving model (epoch =    4, loss =  16.1862)
Saving model (epoch =    5, loss =  9.7153)
Saving model (epoch =    6, loss =  6.3701)
Saving model (epoch =    7, loss =  5.1802)
Saving model (epoch =    8, loss =  4.4255)
Saving model (epoch =    9, loss =  3.8009)
Saving model (epoch =   10, loss =  3.3691)
Saving model (epoch =   11, loss =  3.0943)
Saving model (epoch =   12, loss =  2.8176)
Saving model (epoch =   13, loss =  2.6274)
Saving model (epoch =   14, loss =  2.4542)
Saving model (epoch =   15, loss =  2.3012)
Saving model (epoch =   16, loss =  2.1766)
Saving model (epoch =   17, loss =  2.0641)
Saving model (epoch =   18, loss =  1.9399)
Saving model (epoch =   19, loss =  1.8978)
Saving model (epoch =   20, loss =  1.7950)
Saving model (epoch =   21, loss =  1.7164)
Saving model (epoch =   22, loss =  1.6455)
Saving model (epoch =   23, loss =  1.5912)
Saving model (epoch =   24, loss =  1.5599)
Saving model (epoch =   25, loss =  1.5197)
Saving model (epoch =   26, loss =  1.4698)
Saving model (epoch =   27, loss =  1.4189)
Saving model (epoch =   28, loss =  1.3992)
Saving model (epoch =   29, loss =  1.3696)
Saving model (epoch =   30, loss =  1.3442)
Saving model (epoch =   31, loss =  1.3231)
Saving model (epoch =   32, loss =  1.2834)
Saving model (epoch =   33, loss =  1.2804)
Saving model (epoch =   34, loss =  1.2471)
Saving model (epoch =   36, loss =  1.2414)
Saving model (epoch =   37, loss =  1.2138)
Saving model (epoch =   38, loss =  1.2083)
Saving model (epoch =   41, loss =  1.1591)
Saving model (epoch =   42, loss =  1.1484)
Saving model (epoch =   44, loss =  1.1209)
Saving model (epoch =   47, loss =  1.1122)
Saving model (epoch =   48, loss =  1.0937)
Saving model (epoch =   50, loss =  1.0842)
Saving model (epoch =   53, loss =  1.0655)
Saving model (epoch =   54, loss =  1.0613)
Saving model (epoch =   57, loss =  1.0524)
Saving model (epoch =   58, loss =  1.0394)
Saving model (epoch =   60, loss =  1.0267)
Saving model (epoch =   63, loss =  1.0248)
Saving model (epoch =   66, loss =  1.0099)
Saving model (epoch =   70, loss =  0.9829)
Saving model (epoch =   72, loss =  0.9817)
Saving model (epoch =   73, loss =  0.9743)
Saving model (epoch =   75, loss =  0.9671)
Saving model (epoch =   78, loss =  0.9643)
Saving model (epoch =   79, loss =  0.9597)
Saving model (epoch =   85, loss =  0.9549)
Saving model (epoch =   86, loss =  0.9535)
Saving model (epoch =   90, loss =  0.9467)
Saving model (epoch =   92, loss =  0.9432)
Saving model (epoch =   93, loss =  0.9231)
Saving model (epoch =   95, loss =  0.9127)
Saving model (epoch =  104, loss =  0.9117)
Saving model (epoch =  107, loss =  0.8994)
Saving model (epoch =  110, loss =  0.8935)
Saving model (epoch =  116, loss =  0.8882)
Saving model (epoch =  124, loss =  0.8872)
Saving model (epoch =  128, loss =  0.8724)
Saving model (epoch =  134, loss =  0.8722)
Saving model (epoch =  139, loss =  0.8677)
Saving model (epoch =  146, loss =  0.8654)
Saving model (epoch =  156, loss =  0.8642)
Saving model (epoch =  159, loss =  0.8528)
Saving model (epoch =  167, loss =  0.8494)
Saving model (epoch =  173, loss =  0.8492)
Saving model (epoch =  176, loss =  0.8461)
Saving model (epoch =  178, loss =  0.8403)
Saving model (epoch =  182, loss =  0.8375)
Saving model (epoch =  199, loss =  0.8295)
Saving model (epoch =  212, loss =  0.8273)
Saving model (epoch =  235, loss =  0.8252)
Saving model (epoch =  238, loss =  0.8233)
Saving model (epoch =  251, loss =  0.8211)
Saving model (epoch =  253, loss =  0.8205)
Saving model (epoch =  258, loss =  0.8175)
Saving model (epoch =  284, loss =  0.8143)
Saving model (epoch =  308, loss =  0.8136)
Saving model (epoch =  312, loss =  0.8075)
Saving model (epoch =  324, loss =  0.8045)
Saving model (epoch =  400, loss =  0.8040)
Saving model (epoch =  404, loss =  0.8010)
Saving model (epoch =  466, loss =  0.7998)
Saving model (epoch =  525, loss =  0.7993)
Saving model (epoch =  561, loss =  0.7945)
Saving model (epoch =  584, loss =  0.7903)
Saving model (epoch =  667, loss =  0.7896)
Saving model (epoch =  717, loss =  0.7823)
Saving model (epoch =  776, loss =  0.7812)
Saving model (epoch =  835, loss =  0.7797)
Saving model (epoch =  866, loss =  0.7771)
Saving model (epoch =  919, loss =  0.7770)
Saving model (epoch =  933, loss =  0.7748)
Saving model (epoch =  965, loss =  0.7705)
Saving model (epoch =  1027, loss =  0.7674)
Saving model (epoch =  1119, loss =  0.7647)
Saving model (epoch =  1140, loss =  0.7643)
Saving model (epoch =  1196, loss =  0.7620)
Saving model (epoch =  1234, loss =  0.7616)
Saving model (epoch =  1243, loss =  0.7582)
Finished training after 1444 epochs
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plot_learning_curve(model_loss_record, title=<span class="string">&#x27;deep model&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p><img src="https://cdn.jsdelivr.net/gh/WanpengXu/myPicGo/img/202207301912583.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">del</span> model   <span class="comment"># delete model variable</span></span><br><span class="line">model = NeuralNet(tr_set.dataset.dim).to(device)</span><br><span class="line">ckpt = torch.load(config[<span class="string">&#x27;save_path&#x27;</span>], map_location=<span class="string">&#x27;cpu&#x27;</span>)  <span class="comment"># Load your best model</span></span><br><span class="line">model.load_state_dict(ckpt)</span><br><span class="line">plot_pred(dv_set, model, device)  <span class="comment"># Show prediction on the validation set</span></span><br></pre></td></tr></table></figure>
<p><img src="https://cdn.jsdelivr.net/gh/WanpengXu/myPicGo/img/202207301913401.png" alt="png"></p>
<h3 id="Testing-1"><a href="#Testing-1" class="headerlink" title="Testing"></a><strong>Testing</strong></h3><p>The predictions of your model on testing set will be stored at <code>pred.csv</code>.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">save_pred</span>(<span class="params">preds, file</span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27; Save predictions to specified file &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Saving results to &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(file))</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(file, <span class="string">&#x27;w&#x27;</span>) <span class="keyword">as</span> fp:</span><br><span class="line">        writer = csv.writer(fp)</span><br><span class="line">        writer.writerow([<span class="string">&#x27;id&#x27;</span>, <span class="string">&#x27;tested_positive&#x27;</span>])</span><br><span class="line">        <span class="keyword">for</span> i, p <span class="keyword">in</span> <span class="built_in">enumerate</span>(preds):</span><br><span class="line">            writer.writerow([i, p])</span><br><span class="line"></span><br><span class="line">preds = test(tt_set, model, device)  <span class="comment"># predict COVID-19 cases with your model</span></span><br><span class="line">save_pred(preds, <span class="string">&#x27;pred.csv&#x27;</span>)         <span class="comment"># save prediction file to pred.csv</span></span><br></pre></td></tr></table></figure>
<pre><code>Saving results to pred.csv
</code></pre><h2 id="Medium-Baseline"><a href="#Medium-Baseline" class="headerlink" title="Medium Baseline"></a><strong>Medium Baseline</strong></h2><p>Simple Baseline Code中标注了一些TODO，TA说只要完成这些TODO也许就可以达到Medium Baseline，TODO一共有这些：</p>
<ol>
<li>TODO: Using 40 states &amp; 2 tested_positive features (indices = 57 &amp; 75)</li>
<li>TODO: How to modify this model to achieve better performance?</li>
<li>TODO: you may implement L1/L2 regularization here</li>
<li>TODO: How to tune these hyper-parameters to improve your model’s performance?</li>
</ol>
<p>其中最重要的是TODO1，按TA所说修改即可达到Medium Baseline，即：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">feats = <span class="built_in">list</span>(<span class="built_in">range</span>(<span class="number">40</span>)) + [<span class="number">57</span>] + [<span class="number">75</span>]</span><br></pre></td></tr></table></figure>
<p>查看一下这两列数据，发现列名为<code>tested_positive</code>和<code>tested_positive.1</code>，即第1天阳性和第2天阳性，我们要预测的就是93th列——<code>tested_positive2</code></p>
<p><img src="https://cdn.jsdelivr.net/gh/WanpengXu/myPicGo/img/202207310014714.png" alt=""></p>
<h2 id="Strong-Baseline"><a href="#Strong-Baseline" class="headerlink" title="Strong Baseline"></a>Strong Baseline</h2><p>TA给了以下提示</p>
<ol>
<li>Feature selection (what other features are useful?)</li>
<li>DNN architecture (layers? dimension? activation function?)</li>
<li>Training (mini-batch? optimizer? learning rate?)</li>
<li>L2 regularization</li>
<li>There are some mistakes in the sample code, can you find them?</li>
</ol>
<p>总的来说就是从三部分优化：1. Data、2. Network Structure、3. Optimization</p>
<p>在本题中最重要的还是Feature selection。</p>
<p>关于1：</p>
<p>可以使用SelectKBest函数，也可以对tested_positive2分析相关性（方法2，还没整理）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line">data = pd.read_csv(tr_path).iloc[<span class="number">1</span>:, <span class="number">1</span>:]</span><br><span class="line">x = data.iloc[:, <span class="number">0</span>:<span class="number">93</span>]</span><br><span class="line">y = data.iloc[:, <span class="number">93</span>]</span><br><span class="line"><span class="comment"># min-max normalization 极差归一化</span></span><br><span class="line">x = (x - x.<span class="built_in">min</span>()) / (x.<span class="built_in">max</span>() - x.<span class="built_in">min</span>())</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> SelectKBest</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> f_regression</span><br><span class="line"></span><br><span class="line">best_features = SelectKBest(score_func=f_regression, k = <span class="number">5</span>)</span><br><span class="line">fit = best_features.fit(x, y)</span><br><span class="line">df_scores = pd.DataFrame(fit.scores_)</span><br><span class="line">df_columns = pd.DataFrame(x.columns)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Concat two dataframes for better visualization </span></span><br><span class="line">feature_scores = pd.concat([df_columns, df_scores], axis=<span class="number">1</span>)</span><br><span class="line">feature_scores.columns = [<span class="string">&#x27;Specs&#x27;</span>, <span class="string">&#x27;Score&#x27;</span>]         <span class="comment"># Naming the dataframe columns</span></span><br><span class="line">feature_scores.nlargest(<span class="number">15</span>, <span class="string">&#x27;Score&#x27;</span>)                <span class="comment"># Print 15 best features</span></span><br><span class="line"><span class="comment"># feature_scores.nlargest(15, &#x27;Score&#x27;).index.values</span></span><br></pre></td></tr></table></figure>
<p><img src="https://cdn.jsdelivr.net/gh/WanpengXu/myPicGo/img/202207310059387.png" alt=""></p>
<p>最后一行Score太低，去掉</p>
<p>可以得到改进：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">feats = [<span class="number">75</span>, <span class="number">57</span>, <span class="number">42</span>, <span class="number">60</span>, <span class="number">78</span>, <span class="number">43</span>, <span class="number">61</span>, <span class="number">79</span>, <span class="number">40</span>, <span class="number">58</span>, <span class="number">76</span>, <span class="number">41</span>, <span class="number">59</span>, <span class="number">77</span>]</span><br></pre></td></tr></table></figure>
<p>关于2：</p>
<p>该作业中“浅的”网络效果更好（Less is More），与后续作业不同，使用如下结构：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">self.net = nn.Sequential(</span><br><span class="line">    nn.Linear(input_dim, <span class="number">64</span>),</span><br><span class="line">    nn.BatchNorm1d(<span class="number">64</span>),       	<span class="comment"># 使用BN，加速模型训练</span></span><br><span class="line">    nn.LeakyReLU(),           	<span class="comment"># 更换activation function</span></span><br><span class="line">    nn.Dropout(p=<span class="number">0.35</span>),        	<span class="comment"># 使用Dropout，减小过拟合，注意不能在BN之前</span></span><br><span class="line">    nn.Linear(<span class="number">64</span>, <span class="number">1</span>)</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>关于3：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">config = &#123;</span><br><span class="line">    <span class="string">&#x27;n_epochs&#x27;</span>: <span class="number">10000</span>,              <span class="comment"># 因为有early_stop，所以大一点没有影响</span></span><br><span class="line">    <span class="string">&#x27;batch_size&#x27;</span>: <span class="number">200</span>,              <span class="comment"># 微调batchsize</span></span><br><span class="line">    <span class="string">&#x27;optimizer&#x27;</span>: <span class="string">&#x27;Adam&#x27;</span>,            <span class="comment"># 使用Adam优化器</span></span><br><span class="line">    <span class="string">&#x27;optim_hparas&#x27;</span>: &#123;               <span class="comment"># 完全使用默认参数</span></span><br><span class="line">        <span class="string">&#x27;lr&#x27;</span>: <span class="number">0.001</span>,                 </span><br><span class="line">        <span class="comment">#&#x27;momentum&#x27;: 0.9,             </span></span><br><span class="line">        <span class="comment">#&#x27;weight_decay&#x27;: 5e-4,</span></span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="string">&#x27;early_stop&#x27;</span>: <span class="number">500</span>,              <span class="comment"># 由于最后训练使用了所有数据，大一点影响不大</span></span><br><span class="line">    <span class="string">&#x27;save_path&#x27;</span>: <span class="string">&#x27;models/model.pth&#x27;</span>  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>关于4：</p>
<p>TA说使用L2正则化，但其实效果提升不大</p>
<p>L1正则化的损失函数</p>
<script type="math/tex; mode=display">
L = L(W)+\lambda \sum_{i=1}^{n}{|\omega_i|}</script><p>L2正则化的损失函数</p>
<script type="math/tex; mode=display">
L = L(W)+\lambda \sum_{i=1}^{n}\omega_i^2</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cal_loss</span>(<span class="params">self, pred, target</span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27; Calculate loss &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> you may implement L1/L2 regularization here</span></span><br><span class="line">    regularization_lambda = <span class="number">0.00075</span></span><br><span class="line">    regularization_loss = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> model.parameters():</span><br><span class="line">        <span class="comment"># regularization_loss += torch.sum(abs(param))    # L1 regularization_loss</span></span><br><span class="line">        regularization_loss += torch.<span class="built_in">sum</span>(param ** <span class="number">2</span>)    <span class="comment"># L2 regularization_loss</span></span><br><span class="line">    <span class="keyword">return</span> self.criterion(pred, target) + regularization_lambda * regularization_loss</span><br></pre></td></tr></table></figure>
<p>关于5：</p>
<p><img src="https://cdn.jsdelivr.net/gh/WanpengXu/myPicGo/img/202207302330850.png" alt=""></p>
<p><img src="https://cdn.jsdelivr.net/gh/WanpengXu/myPicGo/img/202207302331743.png" alt=""></p>
<p>改为</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">return</span> torch.sqrt(self.criterion(pred, target))</span><br></pre></td></tr></table></figure>
<p>最后过了public的strong baseline，private差0.00x</p>
<p><img src="https://cdn.jsdelivr.net/gh/WanpengXu/myPicGo/img/202207310211151.png" alt=""></p>
<h1 id="Homework-2-TIMIT-framewise-phoneme-classification"><a href="#Homework-2-TIMIT-framewise-phoneme-classification" class="headerlink" title="Homework 2: TIMIT framewise phoneme (classification)"></a><strong>Homework 2: TIMIT framewise phoneme (classification)</strong></h1><h2 id="Phoneme-Classification"><a href="#Phoneme-Classification" class="headerlink" title="Phoneme Classification"></a>Phoneme Classification</h2><p><img src="https://cdn.jsdelivr.net/gh/WanpengXu/myPicGo/img/202208010306532.png" alt=""></p>
<p>首先可以看到Evaluation metric就是acc，所以越高越好。</p>
<h3 id="Simple-Baseline-1"><a href="#Simple-Baseline-1" class="headerlink" title="Simple Baseline"></a>Simple Baseline</h3><p>跑通TA’s sample code就可以双过simple baseline</p>
<p><img src="https://cdn.jsdelivr.net/gh/WanpengXu/myPicGo/img/202208010253944.png" alt=""></p>
<p>ps：跑一次挺慢的，colab挂上GPU也要15min左右一次。</p>
<p>CODE:</p>
<h4 id="Download-Data-1"><a href="#Download-Data-1" class="headerlink" title="Download Data"></a>Download Data</h4><p>Download data from google drive, then unzip it.</p>
<p>You should have <code>timit_11/train_11.npy</code>, <code>timit_11/train_label_11.npy</code>, and <code>timit_11/test_11.npy</code> after running this block.<br><br><br><code>timit_11/</code></p>
<ul>
<li><code>train_11.npy</code>: training data<br></li>
<li><code>train_label_11.npy</code>: training label<br></li>
<li><code>test_11.npy</code>:  testing data<br><br></li>
</ul>
<p><strong>notes: if the google drive link is dead, you can download the data directly from Kaggle and upload it to the workspace</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">!gdown --<span class="built_in">id</span> <span class="string">&#x27;1HPkcmQmFGu-3OknddKIa5dNDsR05lIQR&#x27;</span> --output data.<span class="built_in">zip</span></span><br><span class="line">!unzip data.<span class="built_in">zip</span></span><br><span class="line">!ls</span><br></pre></td></tr></table></figure>
<p>​    /usr/local/lib/python3.7/dist-packages/gdown/cli.py:131: FutureWarning: Option <code>--id</code> was deprecated in version 4.3.1 and will be removed in 5.0. You don’t need to pass it anymore to use a file ID.<br>​      category=FutureWarning,<br>​    Downloading…<br>​    From: <a target="_blank" rel="noopener" href="https://drive.google.com/uc?id=1HPkcmQmFGu-3OknddKIa5dNDsR05lIQR">https://drive.google.com/uc?id=1HPkcmQmFGu-3OknddKIa5dNDsR05lIQR</a><br>​    To: /content/data.zip<br>​    100% 372M/372M [00:03&lt;00:00, 95.5MB/s]<br>​    Archive:  data.zip<br>​       creating: timit_11/<br>​      inflating: timit_11/train_11.npy<br>​      inflating: timit_11/test_11.npy<br>​      inflating: timit_11/train_label_11.npy<br>​    data.zip  sample_data  timit_11</p>
<h4 id="Preparing-Data"><a href="#Preparing-Data" class="headerlink" title="Preparing Data"></a>Preparing Data</h4><p>Load the training and testing data from the <code>.npy</code> file (NumPy array).</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Loading data ...&#x27;</span>)</span><br><span class="line"></span><br><span class="line">data_root=<span class="string">&#x27;./timit_11/&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># read data from .npy file using np.load</span></span><br><span class="line">train = np.load(data_root + <span class="string">&#x27;train_11.npy&#x27;</span>)</span><br><span class="line">train_label = np.load(data_root + <span class="string">&#x27;train_label_11.npy&#x27;</span>)</span><br><span class="line">test = np.load(data_root + <span class="string">&#x27;test_11.npy&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Size of training data: &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(train.shape))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Size of testing data: &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(test.shape))</span><br></pre></td></tr></table></figure>
<p>​    Loading data …<br>​    Size of training data: (1229932, 429)<br>​    Size of testing data: (451552, 429)</p>
<h4 id="Create-Dataset"><a href="#Create-Dataset" class="headerlink" title="Create Dataset"></a>Create Dataset</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TIMITDataset</span>(<span class="params">Dataset</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, X, y=<span class="literal">None</span></span>):</span></span><br><span class="line">        self.data = torch.from_numpy(X).<span class="built_in">float</span>()</span><br><span class="line">        <span class="keyword">if</span> y <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            y = y.astype(np.<span class="built_in">int</span>)</span><br><span class="line">            self.label = torch.LongTensor(y)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.label = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span>(<span class="params">self, idx</span>):</span></span><br><span class="line">        <span class="keyword">if</span> self.label <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">return</span> self.data[idx], self.label[idx]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> self.data[idx]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.data)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>Split the labeled data into a training set and a validation set, you can modify the variable <code>VAL_RATIO</code> to change the ratio of validation data.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">VAL_RATIO = <span class="number">0.2</span></span><br><span class="line"></span><br><span class="line">percent = <span class="built_in">int</span>(train.shape[<span class="number">0</span>] * (<span class="number">1</span> - VAL_RATIO))     <span class="comment"># pivot of train data and dev data</span></span><br><span class="line">train_x, train_y, val_x, val_y = train[:percent], train_label[:percent], train[percent:], train_label[percent:]</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Size of training set: &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(train_x.shape))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Size of validation set: &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(val_x.shape))</span><br><span class="line">Size of training <span class="built_in">set</span>: (<span class="number">983945</span>, <span class="number">429</span>)</span><br><span class="line">Size of validation <span class="built_in">set</span>: (<span class="number">245987</span>, <span class="number">429</span>)</span><br></pre></td></tr></table></figure>
<p>Create a data loader from the dataset, feel free to tweak the variable <code>BATCH_SIZE</code> here.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">BATCH_SIZE = <span class="number">64</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"></span><br><span class="line">train_set = TIMITDataset(train_x, train_y)</span><br><span class="line">val_set = TIMITDataset(val_x, val_y)</span><br><span class="line">train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=<span class="literal">True</span>) <span class="comment"># only shuffle the training data</span></span><br><span class="line">val_loader = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<p>​    /usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: DeprecationWarning: <code>np.int</code> is a deprecated alias for the builtin <code>int</code>. To silence this warning, use <code>int</code> by itself. Doing this will not modify any behavior and is safe. When replacing <code>np.int</code>, you may wish to use e.g. <code>np.int64</code> or <code>np.int32</code> to specify the precision. If you wish to review your current use, check the release note link for additional information.<br>​    Deprecated in NumPy 1.20; for more details and guidance: <a target="_blank" rel="noopener" href="https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations">https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations</a></p>
<p>​    </p>
<p>Cleanup the unneeded variables to save memory.<br></p>
<p><strong>notes: if you need to use these variables later, then you may remove this block or clean up unneeded variables later<br>the data size is quite huge, so be aware of memory usage in colab</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">del</span> train, train_label, train_x, train_y, val_x, val_y</span><br><span class="line"></span><br><span class="line"><span class="comment"># Garbage Collector</span></span><br><span class="line"><span class="keyword">import</span> gc</span><br><span class="line"><span class="comment"># gc.collect(generation=2): For variable of generation 0~2 of generational algorithm, delete memory for unreachable variable in them using mark-sweep algotirhm(travel algotirhm).</span></span><br><span class="line">gc.collect()        <span class="comment"># del变量后立即gc.collect，可以确保它们引用的内存被立即清除，以释放RAM</span></span><br></pre></td></tr></table></figure>
<p>​    153</p>
<h4 id="Create-Model"><a href="#Create-Model" class="headerlink" title="Create Model"></a>Create Model</h4><p>Define model architecture, you are encouraged to change and experiment with the model architecture.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Classifier</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Classifier, self).__init__()</span><br><span class="line">        self.layer1 = nn.Linear(<span class="number">429</span>, <span class="number">1024</span>)</span><br><span class="line">        self.layer2 = nn.Linear(<span class="number">1024</span>, <span class="number">512</span>)</span><br><span class="line">        self.layer3 = nn.Linear(<span class="number">512</span>, <span class="number">128</span>)</span><br><span class="line">        self.out = nn.Linear(<span class="number">128</span>, <span class="number">39</span>) </span><br><span class="line"></span><br><span class="line">        self.act_fn = nn.Sigmoid()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = self.layer1(x)</span><br><span class="line">        x = self.act_fn(x)</span><br><span class="line"></span><br><span class="line">        x = self.layer2(x)</span><br><span class="line">        x = self.act_fn(x)</span><br><span class="line"></span><br><span class="line">        x = self.layer3(x)</span><br><span class="line">        x = self.act_fn(x)</span><br><span class="line"></span><br><span class="line">        x = self.out(x)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<h4 id="Training-1"><a href="#Training-1" class="headerlink" title="Training"></a>Training</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># check device</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_device</span>():</span></span><br><span class="line">  <span class="keyword">return</span> <span class="string">&#x27;cuda&#x27;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&#x27;cpu&#x27;</span></span><br></pre></td></tr></table></figure>
<p>Fix random seeds for reproducibility.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># fix random seed</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">same_seeds</span>(<span class="params">seed</span>):</span></span><br><span class="line">    torch.manual_seed(seed)</span><br><span class="line">    <span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">        torch.cuda.manual_seed(seed)</span><br><span class="line">        torch.cuda.manual_seed_all(seed)</span><br><span class="line">    np.random.seed(seed)</span><br><span class="line">    torch.backends.cudnn.benchmark = <span class="literal">False</span></span><br><span class="line">    torch.backends.cudnn.deterministic = <span class="literal">True</span></span><br></pre></td></tr></table></figure>
<p>Feel free to change the training parameters here.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># fix random seed for reproducibility 可重复性</span></span><br><span class="line">same_seeds(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># get device </span></span><br><span class="line">device = get_device()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;DEVICE: <span class="subst">&#123;device&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># training parameters</span></span><br><span class="line">num_epoch = <span class="number">20</span>               <span class="comment"># number of training epoch</span></span><br><span class="line">learning_rate = <span class="number">0.0001</span>       <span class="comment"># learning rate</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># the path where checkpoint saved</span></span><br><span class="line">model_path = <span class="string">&#x27;./model.ckpt&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># create model, define a loss function, and optimizer</span></span><br><span class="line">model = Classifier().to(device)</span><br><span class="line">criterion = nn.CrossEntropyLoss() </span><br><span class="line">optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)</span><br></pre></td></tr></table></figure>
<p>​    DEVICE: cuda</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># start training</span></span><br><span class="line"></span><br><span class="line">best_acc = <span class="number">0.0</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epoch):</span><br><span class="line">    train_acc = <span class="number">0.0</span></span><br><span class="line">    train_loss = <span class="number">0.0</span></span><br><span class="line">    val_acc = <span class="number">0.0</span></span><br><span class="line">    val_loss = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># training</span></span><br><span class="line">    model.train()   <span class="comment"># set the model to training mode</span></span><br><span class="line">    <span class="keyword">for</span> i, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader):</span><br><span class="line">        inputs, labels = data</span><br><span class="line">        inputs, labels = inputs.to(device), labels.to(device)</span><br><span class="line">        optimizer.zero_grad() </span><br><span class="line">        outputs = model(inputs) </span><br><span class="line">        batch_loss = criterion(outputs, labels)</span><br><span class="line">        <span class="comment"># torch.max(input, dim): Returns a namedtuple (values, indices) where values is the maximum value of each row of the input tensor in the given dimension dim.</span></span><br><span class="line">        _, train_pred = torch.<span class="built_in">max</span>(outputs, <span class="number">1</span>)   <span class="comment"># get the index of the class with the highest probability</span></span><br><span class="line">        batch_loss.backward() </span><br><span class="line">        optimizer.step() </span><br><span class="line"></span><br><span class="line">        <span class="comment"># Tensor.cpu(): Returns a copy of this object in CPU memory.</span></span><br><span class="line">        <span class="comment"># Tensor.==: 对应位置相同为1，不同为0！！，等同于Tensor.eq_()方法。</span></span><br><span class="line">        <span class="comment"># Tensor.sum(): Returns the sum of all elements in the input tensor.</span></span><br><span class="line">        <span class="comment"># Tensor.item(): Returns the value of this tensor as a standard Python number.</span></span><br><span class="line">        train_acc += (train_pred.cpu() == labels.cpu()).<span class="built_in">sum</span>().item()        <span class="comment"># 2 tensor -&gt; 1 tensor -&gt; 1 tensor of 1 element -&gt; 1 number</span></span><br><span class="line">        train_loss += batch_loss.item()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># validation</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(val_set) &gt; <span class="number">0</span>:</span><br><span class="line">        model.<span class="built_in">eval</span>()    <span class="comment"># set the model to evaluation mode</span></span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            <span class="keyword">for</span> i, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(val_loader):</span><br><span class="line">                inputs, labels = data</span><br><span class="line">                inputs, labels = inputs.to(device), labels.to(device)</span><br><span class="line">                outputs = model(inputs)</span><br><span class="line">                batch_loss = criterion(outputs, labels) </span><br><span class="line">                _, val_pred = torch.<span class="built_in">max</span>(outputs, <span class="number">1</span>) </span><br><span class="line">            </span><br><span class="line">                val_acc += (val_pred.cpu() == labels.cpu()).<span class="built_in">sum</span>().item()    <span class="comment"># get the index of the class with the highest probability</span></span><br><span class="line">                val_loss += batch_loss.item()</span><br><span class="line"></span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;[&#123;:03d&#125;/&#123;:03d&#125;] Train Acc: &#123;:3.6f&#125; Loss: &#123;:3.6f&#125; | Val Acc: &#123;:3.6f&#125; loss: &#123;:3.6f&#125;&#x27;</span>.<span class="built_in">format</span>(</span><br><span class="line">                epoch + <span class="number">1</span>, num_epoch, train_acc/<span class="built_in">len</span>(train_set), train_loss/<span class="built_in">len</span>(train_loader), val_acc/<span class="built_in">len</span>(val_set), val_loss/<span class="built_in">len</span>(val_loader)</span><br><span class="line">            ))</span><br><span class="line"></span><br><span class="line">            <span class="comment"># if the model improves, save a checkpoint at this epoch</span></span><br><span class="line">            <span class="keyword">if</span> val_acc &gt; best_acc:</span><br><span class="line">                best_acc = val_acc</span><br><span class="line">                torch.save(model.state_dict(), model_path)</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&#x27;saving model with acc &#123;:.3f&#125;&#x27;</span>.<span class="built_in">format</span>(best_acc/<span class="built_in">len</span>(val_set)))</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;[&#123;:03d&#125;/&#123;:03d&#125;] Train Acc: &#123;:3.6f&#125; Loss: &#123;:3.6f&#125;&#x27;</span>.<span class="built_in">format</span>(</span><br><span class="line">            epoch + <span class="number">1</span>, num_epoch, train_acc/<span class="built_in">len</span>(train_set), train_loss/<span class="built_in">len</span>(train_loader)</span><br><span class="line">        ))</span><br><span class="line"></span><br><span class="line"><span class="comment"># if not validating, save the last epoch</span></span><br><span class="line"><span class="keyword">if</span> <span class="built_in">len</span>(val_set) == <span class="number">0</span>:</span><br><span class="line">    torch.save(model.state_dict(), model_path)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;saving model at last epoch&#x27;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>​    [001/020] Train Acc: 0.467302 Loss: 1.811661 | Val Acc: 0.567428 loss: 1.433065<br>​    saving model with acc 0.567<br>​    [002/020] Train Acc: 0.594383 Loss: 1.330666 | Val Acc: 0.628639 loss: 1.211098<br>​    saving model with acc 0.629<br>​    [003/020] Train Acc: 0.644506 Loss: 1.154064 | Val Acc: 0.660421 loss: 1.101216<br>​    saving model with acc 0.660<br>​    [004/020] Train Acc: 0.672216 Loss: 1.052246 | Val Acc: 0.676300 loss: 1.038718<br>​    saving model with acc 0.676<br>​    [005/020] Train Acc: 0.691347 Loss: 0.983104 | Val Acc: 0.685154 loss: 1.001852<br>​    saving model with acc 0.685<br>​    [006/020] Train Acc: 0.705615 Loss: 0.931955 | Val Acc: 0.689301 loss: 0.984177<br>​    saving model with acc 0.689<br>​    [007/020] Train Acc: 0.716344 Loss: 0.891687 | Val Acc: 0.694516 loss: 0.964627<br>​    saving model with acc 0.695<br>​    [008/020] Train Acc: 0.725881 Loss: 0.857907 | Val Acc: 0.697720 loss: 0.951889<br>​    saving model with acc 0.698<br>​    [009/020] Train Acc: 0.733717 Loss: 0.829495 | Val Acc: 0.696691 loss: 0.949866<br>​    [010/020] Train Acc: 0.741151 Loss: 0.803701 | Val Acc: 0.699374 loss: 0.944832<br>​    saving model with acc 0.699<br>​    [011/020] Train Acc: 0.748049 Loss: 0.781106 | Val Acc: 0.697773 loss: 0.946494<br>​    [012/020] Train Acc: 0.753793 Loss: 0.760380 | Val Acc: 0.702830 loss: 0.938236<br>​    saving model with acc 0.703<br>​    [013/020] Train Acc: 0.759404 Loss: 0.741234 | Val Acc: 0.700452 loss: 0.945627<br>​    [014/020] Train Acc: 0.764573 Loss: 0.723574 | Val Acc: 0.702159 loss: 0.942118<br>​    [015/020] Train Acc: 0.769470 Loss: 0.707325 | Val Acc: 0.704432 loss: 0.936154<br>​    saving model with acc 0.704<br>​    [016/020] Train Acc: 0.773687 Loss: 0.691314 | Val Acc: 0.701736 loss: 0.945713<br>​    [017/020] Train Acc: 0.778676 Loss: 0.676633 | Val Acc: 0.701586 loss: 0.953081<br>​    [018/020] Train Acc: 0.783106 Loss: 0.662425 | Val Acc: 0.699667 loss: 0.963290<br>​    [019/020] Train Acc: 0.786395 Loss: 0.649180 | Val Acc: 0.700082 loss: 0.957681<br>​    [020/020] Train Acc: 0.790643 Loss: 0.636623 | Val Acc: 0.699732 loss: 0.964269</p>
<h4 id="Testing-2"><a href="#Testing-2" class="headerlink" title="Testing"></a>Testing</h4><p>Create a testing dataset, and load model from the saved checkpoint.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># create testing dataset</span></span><br><span class="line">test_set = TIMITDataset(test, <span class="literal">None</span>)</span><br><span class="line">test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># create model and load weights from checkpoint</span></span><br><span class="line">model = Classifier().to(device)</span><br><span class="line">model.load_state_dict(torch.load(model_path))</span><br></pre></td></tr></table></figure>
<p>​    <All keys matched successfully></p>
<p>Make prediction.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">predict = []</span><br><span class="line">model.<span class="built_in">eval</span>()    <span class="comment"># set the model to evaluation mode</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    <span class="keyword">for</span> i, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(test_loader):</span><br><span class="line">        inputs = data</span><br><span class="line">        inputs = inputs.to(device)</span><br><span class="line">        outputs = model(inputs)</span><br><span class="line">        _, test_pred = torch.<span class="built_in">max</span>(outputs, <span class="number">1</span>)    <span class="comment"># get the index of the class with the highest probability</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> y <span class="keyword">in</span> test_pred.cpu().numpy():</span><br><span class="line">            predict.append(y)</span><br></pre></td></tr></table></figure>
<p>Write prediction to a CSV file.</p>
<p>After finish running this block, download the file <code>prediction.csv</code> from the files section on the left-hand side and submit it to Kaggle.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;prediction.csv&#x27;</span>, <span class="string">&#x27;w&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(<span class="string">&#x27;Id,Class\n&#x27;</span>)</span><br><span class="line">    <span class="keyword">for</span> i, y <span class="keyword">in</span> <span class="built_in">enumerate</span>(predict):</span><br><span class="line">        f.write(<span class="string">&#x27;&#123;&#125;,&#123;&#125;\n&#x27;</span>.<span class="built_in">format</span>(i, y))</span><br></pre></td></tr></table></figure>
<h3 id="Strong-Baseline-1"><a href="#Strong-Baseline-1" class="headerlink" title="Strong Baseline"></a>Strong Baseline</h3><p>TA在PPT里给了一些Hint。</p>
<ol>
<li>Model architecture (layers? dimension? activation function?)</li>
<li>Training (batch size? optimizer? learning rate? epoch?)</li>
<li>Tips (batch norm? dropout? regularization?)</li>
</ol>
<p>基本还是Data、Structure、Optimization。</p>
<h2 id="Hessian-Matrix"><a href="#Hessian-Matrix" class="headerlink" title="Hessian Matrix"></a>Hessian Matrix</h2><h1 id="Course中的小知识点"><a href="#Course中的小知识点" class="headerlink" title="Course中的小知识点"></a>Course中的小知识点</h1><h2 id="如何降低loss"><a href="#如何降低loss" class="headerlink" title="如何降低loss"></a>如何降低loss</h2><p><img src="https://cdn.jsdelivr.net/gh/WanpengXu/myPicGo/img/202207311735584.png" alt="image-20220731173535493"></p>
<h2 id="data-Augmentation"><a href="#data-Augmentation" class="headerlink" title="data Augmentation"></a>data Augmentation</h2><p><img src="https://cdn.jsdelivr.net/gh/WanpengXu/myPicGo/img/202207311714111.png" alt=""></p>
<h2 id="cross-validation"><a href="#cross-validation" class="headerlink" title="cross validation"></a>cross validation</h2><p>避免public testing set和private testing set差距过大</p>
<p><img src="https://cdn.jsdelivr.net/gh/WanpengXu/myPicGo/img/202207311729739.png" alt="image-20220731172904653"></p>
<h2 id="k-fold-cross-validation"><a href="#k-fold-cross-validation" class="headerlink" title="k-fold cross validation"></a>k-fold cross validation</h2><p><img src="https://cdn.jsdelivr.net/gh/WanpengXu/myPicGo/img/202207311732252.png" alt="image-20220731173229169"></p>
<h2 id="overfitting"><a href="#overfitting" class="headerlink" title="overfitting"></a>overfitting</h2><p><img src="https://cdn.jsdelivr.net/gh/WanpengXu/myPicGo/img/202207311742374.png" alt="image-20220731174234292"></p>
<p>极端的例子：model在训练集上100%准确（loss=0），在测试集上准确度接近0%（loss很大）</p>
<p>原因：</p>
<p><img src="https://cdn.jsdelivr.net/gh/WanpengXu/myPicGo/img/202207311744573.png" alt="image-20220731174434486"></p>
<p>model的flexibility</p>
<p>可以通过调整层数来constrain模型，或者增加training data（数据不够？可以Data Augmentation）</p>
<p><img src="https://cdn.jsdelivr.net/gh/WanpengXu/myPicGo/img/202207311746818.png" alt="image-20220731174653735"></p>
<p>但不能constrain过度</p>
<p><img src="https://cdn.jsdelivr.net/gh/WanpengXu/myPicGo/img/202207311747587.png" alt="image-20220731174735504"></p>
<h2 id="mismatch"><a href="#mismatch" class="headerlink" title="mismatch"></a>mismatch</h2><p><img src="https://cdn.jsdelivr.net/gh/WanpengXu/myPicGo/img/202207311736745.png" alt="image-20220731173609612"></p>
<p>eg.</p>
<p>Hung-yi Lee：感谢大家为了让这个模型不准，上周五花了很多力气去点了这个video，所以这一天（2021.2.26）是今年观看人数最多的一天！</p>
<p>（哈哈哈哈笑死我了）</p>
<p><img src="https://cdn.jsdelivr.net/gh/WanpengXu/myPicGo/img/202207311735131.png" alt="image-20220731173551040"></p>
<h2 id="flat-minima-sharp-minima"><a href="#flat-minima-sharp-minima" class="headerlink" title="flat minima/sharp minima"></a>flat minima/sharp minima</h2><p><del>好minima和坏minima</del></p>
<p><img src="https://cdn.jsdelivr.net/gh/WanpengXu/myPicGo/img/202208010056670.png" alt="image-20220801005600511"></p>
<h2 id="vanilla-GD-GD-with-momentum"><a href="#vanilla-GD-GD-with-momentum" class="headerlink" title="vanilla GD/GD with momentum"></a>vanilla GD/GD with momentum</h2><p><del>总感觉最优化理论学过，但是有点不记得了</del></p>
<p>很像共轭梯度法！</p>
<p><img src="https://cdn.jsdelivr.net/gh/WanpengXu/myPicGo/img/202208010115865.png" alt="image-20220801011510734"></p>
<p>hylee老师讲的：</p>
<p><img src="https://cdn.jsdelivr.net/gh/WanpengXu/myPicGo/img/202208010105321.png" alt="image-20220801010502215"></p>
<p><img src="https://cdn.jsdelivr.net/gh/WanpengXu/myPicGo/img/202208010105713.png" alt="image-20220801010516608"></p>
<h2 id="Adaptive-Learning-Rate（AdaGrad）"><a href="#Adaptive-Learning-Rate（AdaGrad）" class="headerlink" title="Adaptive Learning Rate（AdaGrad）"></a>Adaptive Learning Rate（AdaGrad）</h2><h3 id="Define"><a href="#Define" class="headerlink" title="Define"></a>Define</h3><p><img src="https://cdn.jsdelivr.net/gh/WanpengXu/myPicGo/img/202208010128994.png" alt="image-20220801012811831"></p>
<p><img src="https://cdn.jsdelivr.net/gh/WanpengXu/myPicGo/img/202208010128221.png" alt="image-20220801012854116"></p>
<p>root mean square：均方根</p>
<h3 id="Scheduling"><a href="#Scheduling" class="headerlink" title="Scheduling"></a>Scheduling</h3><h4 id="Decay"><a href="#Decay" class="headerlink" title="Decay"></a>Decay</h4><p><img src="https://cdn.jsdelivr.net/gh/WanpengXu/myPicGo/img/202208010150725.png" alt="image-20220801015052600"></p>
<h4 id="warmup"><a href="#warmup" class="headerlink" title="warmup"></a>warmup</h4><p><img src="https://cdn.jsdelivr.net/gh/WanpengXu/myPicGo/img/202208010156758.png" alt="image-20220801015630687"></p>
<h2 id="learning-rate-adapts-dynamically（RMSProp）"><a href="#learning-rate-adapts-dynamically（RMSProp）" class="headerlink" title="learning rate adapts dynamically（RMSProp）"></a>learning rate adapts dynamically（RMSProp）</h2><p><img src="https://cdn.jsdelivr.net/gh/WanpengXu/myPicGo/img/202208010134196.png" alt="image-20220801013420084"></p>
<p><img src="https://cdn.jsdelivr.net/gh/WanpengXu/myPicGo/img/202208010134265.png" alt="image-20220801013435158"></p>
<p>RMSProp：RMS+支点</p>
<h2 id="RMSProp-Momentum（Adam）"><a href="#RMSProp-Momentum（Adam）" class="headerlink" title="RMSProp + Momentum（Adam）"></a>RMSProp + Momentum（Adam）</h2><p><img src="https://cdn.jsdelivr.net/gh/WanpengXu/myPicGo/img/202208010158101.png" alt="image-20220801015847962"></p>
<h2 id="Cross-entropy"><a href="#Cross-entropy" class="headerlink" title="Cross-entropy"></a>Cross-entropy</h2><p>交叉熵</p>
<script type="math/tex; mode=display">
e=-\sum_{i}{\hat{y}_ilny_i^{\prime}}</script><p><img src="https://cdn.jsdelivr.net/gh/WanpengXu/myPicGo/img/202208010226798.png" alt=""></p>
<p><img src="https://cdn.jsdelivr.net/gh/WanpengXu/myPicGo/img/202208010227409.png" alt=""></p>
<p>loss function会改变optimization的难度！</p>
<h1 id="notation"><a href="#notation" class="headerlink" title="notation"></a>notation</h1><p>$\hat{y}$：y hat</p>
<p>$e^x$：exponential x</p>
<p>$\sum$：Summation</p>
<p>$y{\prime}$：y prime</p>
<p>logit：softmax的输入</p>

        </div>

        
            <section class="post-copyright">
                
                    <p class="copyright-item">
                        <span>Author:</span>
                        <span>许</span>
                    </p>
                
                
                    <p class="copyright-item">
                        <span>Permalink:</span>
                        <span><a href="http://xuwp.top/MACHINE-LEARNING-2021-SPRING-HOMEWORKS.html">http://xuwp.top/MACHINE-LEARNING-2021-SPRING-HOMEWORKS.html</a></span>
                    </p>
                
                
                    <p class="copyright-item">
                        <span>License:</span>
                        <span>Copyright (c) 2022 <a target="_blank" rel="noopener" href="http://creativecommons.org/licenses/by-nc/4.0/">CC-BY-NC-4.0</a> LICENSE</span>
                    </p>
                
                

            </section>
        
        <section class="post-tags">
            <div>
                <span>Tag(s):</span>
                <span class="tag">
                    
                    
                        <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"># 深度学习</a>
                    
                        
                </span>
            </div>
            <div>
                <a href="javascript:window.history.back();">back</a>
                <span>· </span>
                <a href="/">home</a>
            </div>
        </section>
        <section class="post-nav">
            
            
            <a class="next" rel="next" href="/computer-based-Exam.html">Computer-Based Exam</a>
            
        </section>


    </article>
</div>

            </div>
            <footer id="footer" class="footer">
    <div class="copyright">
        <span>Stay hungry. Stay foolish.</span>
    </div>
</footer>

    </div>
</body>

</html>