<!DOCTYPE html>
<html lang="default">

<head>
    <meta charset="UTF-8">
<meta name="viewport"
      content="width=device-width, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">

    <meta name="author" content="许">





<title>周志华. 机器学习初步(2022秋) | 须臾所学之野</title>



    <link rel="icon" href="/favicon.png">




    <!-- stylesheets list from _config.yml -->
    
    <link rel="stylesheet" href="/css/style.css">
    



    <!-- scripts list from _config.yml -->
    
    <script src="/js/script.js"></script>
    
    <script src="/js/tocbot.min.js"></script>
    



    
    
        <!-- MathJax配置，可通过单美元符号书写行内公式等 -->
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
    "HTML-CSS": {
        preferredFont: "TeX",
        availableFonts: ["STIX","TeX"],
        linebreaks: { automatic:true },
        EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50)
    },
    tex2jax: {
        inlineMath: [ ["$", "$"], ["\\(","\\)"] ],
        processEscapes: true,
        ignoreClass: "tex2jax_ignore|dno",
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
        equationNumbers: { autoNumber: "AMS" },
        noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } },
        Macros: { href: "{}" }
    },
    messageStyle: "none"
    });
</script>
<!-- 给MathJax元素添加has-jax class -->
<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>
<!-- 通过连接CDN加载MathJax的js代码 -->
<script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
</script>


    


<meta name="generator" content="Hexo 5.4.0"></head>

<body>
    <script>
        // this function is used to check current theme before page loaded.
        (() => {
            const currentTheme = window.localStorage && window.localStorage.getItem('theme') || '';
            const isDark = currentTheme === 'dark';
            const pagebody = document.getElementsByTagName('body')[0]
            if (isDark) {
                pagebody.classList.add('dark-theme');
                // mobile
                document.getElementById("mobile-toggle-theme").innerText = "· Dark"
            } else {
                pagebody.classList.remove('dark-theme');
                // mobile
                document.getElementById("mobile-toggle-theme").innerText = "· Light"
            }
        })();
    </script>

    <div class="wrapper">
        <header>
    <nav class="navbar">
        <div class="container">
            <div class="navbar-header header-logo"><a href="/">Xuwp&#39;s Blog</a></div>
            <div class="menu navbar-right">
                
                    <a class="menu-item" href="/archives">Posts</a>
                
                    <a class="menu-item" href="/category">Categories</a>
                
                    <a class="menu-item" href="/tag">Tags</a>
                
                    <a class="menu-item" href="/about">About</a>
                
                    <a class="menu-item" href="/link">Links</a>
                
                    <a class="menu-item" href="https://xuwp.top/cv-en/">Resume</a>
                
                <input id="switch_default" type="checkbox" class="switch_default">
                <label for="switch_default" class="toggleBtn"></label>
            </div>
        </div>
    </nav>

    
    <nav class="navbar-mobile" id="nav-mobile">
        <div class="container">
            <div class="navbar-header">
                <div>
                    <a href="/">Xuwp&#39;s Blog</a><a id="mobile-toggle-theme">·&nbsp;Light</a>
                </div>
                <div class="menu-toggle" onclick="mobileBtn()">&#9776; Menu</div>
            </div>
            <div class="menu" id="mobile-menu">
                
                    <a class="menu-item" href="/archives">Posts</a>
                
                    <a class="menu-item" href="/category">Categories</a>
                
                    <a class="menu-item" href="/tag">Tags</a>
                
                    <a class="menu-item" href="/about">About</a>
                
                    <a class="menu-item" href="/link">Links</a>
                
                    <a class="menu-item" href="https://xuwp.top/cv-en/">Resume</a>
                
            </div>
        </div>
    </nav>

</header>
<script>
    var mobileBtn = function f() {
        var toggleMenu = document.getElementsByClassName("menu-toggle")[0];
        var mobileMenu = document.getElementById("mobile-menu");
        if(toggleMenu.classList.contains("active")){
           toggleMenu.classList.remove("active")
            mobileMenu.classList.remove("active")
        }else{
            toggleMenu.classList.add("active")
            mobileMenu.classList.add("active")
        }
    }
</script>
            <div class="main">
                <div class="container">
    
    
        <div class="post-toc">
    <div class="tocbot-list">
    </div>
    <div class="tocbot-list-menu">
        <a class="tocbot-toc-expand" onclick="expand_toc()">Expand all</a>
        <a onclick="go_top()">Back to top</a>
        <a onclick="go_bottom()">Go to bottom</a>
    </div>
</div>

<script>
    var tocbot_timer;
    var DEPTH_MAX = 6;    // 为 6 时展开所有
    var tocbot_default_config = {
        tocSelector: '.tocbot-list',
        contentSelector: '.post-content',
        headingSelector: 'h1, h2, h3, h4, h5',
        orderedList: false,
        scrollSmooth: true,
        onClick: extend_click,
    };

    function extend_click() {
        clearTimeout(tocbot_timer);
        tocbot_timer = setTimeout(function () {
            tocbot.refresh(obj_merge(tocbot_default_config, { hasInnerContainers: true }));
        }, 420); // 这个值是由 tocbot 源码里定义的 scrollSmoothDuration 得来的
    }

    document.ready(function () {
        tocbot.init(obj_merge(tocbot_default_config, { collapseDepth: 1 }));
    });

    function expandToc() {
        var b = document.querySelector('.tocbot-toc-expand');
        var expanded = b.getAttribute('data-expanded');
        expanded ? b.removeAttribute('data-expanded') : b.setAttribute('data-expanded', true);
        tocbot.refresh(obj_merge(tocbot_default_config, { collapseDepth: expanded ? 1 : DEPTH_MAX }));
        b.innerText = expanded ? 'Expand all' : 'Collapse all';
    }

    function go_top() {
        window.scrollTo(0, 0);
    }

    function go_bottom() {
        window.scrollTo(0, document.body.scrollHeight);
    }

    function obj_merge(target, source) {
        for (var item in source) {
            if (source.hasOwnProperty(item)) {
                target[item] = source[item];
            }
        }
        return target;
    }
</script>
    

    
    <article class="post-wrap">
        <header class="post-header">
            <h1 class="post-title">周志华. 机器学习初步(2022秋)</h1>
            
                <div class="post-meta">
                    
                        Author: <a itemprop="author" rel="author" href="/">许</a>
                    

                    
                        <span class="post-time">
                        Date: <a href="#">October 16, 2022&nbsp;&nbsp;21:11:46</a>
                        </span>
                    
                    
                        <span class="post-category">
                    Category:
                            
                                <a href="/categories/AI/">AI</a>
                            
                        </span>
                    
                </div>
            
        </header>

        <div class="post-content">
            <blockquote>
<p>课程地址：<a target="_blank" rel="noopener" href="https://www.xuetangx.com/course/nju0802bt/14363483">https://www.xuetangx.com/course/nju0802bt/14363483</a></p>
<p>课程回放：<a target="_blank" rel="noopener" href="https://www.xuetangx.com/learn/nju0802bt/nju0802bt/14363483/video/26163005">https://www.xuetangx.com/learn/nju0802bt/nju0802bt/14363483/video/26163005</a></p>
<p>课程讨论：<a target="_blank" rel="noopener" href="https://www.xuetangx.com/learn/nju0802bt/nju0802bt/14363483/forum">https://www.xuetangx.com/learn/nju0802bt/nju0802bt/14363483/forum</a></p>
<p>教材：<a target="_blank" rel="noopener" href="https://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/MLbook2016.htm">https://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/MLbook2016.htm</a></p>
</blockquote>
<p>2022年10月10日新开设的课程，内容不多，尽快肝完。</p>
<h1 id="绪论"><a href="#绪论" class="headerlink" title="绪论"></a>绪论</h1><blockquote>
<p>科学、技术、工程、应用各解决什么问题？</p>
</blockquote>
<p>科学：是什么？为什么？</p>
<p>技术：怎么做？</p>
<p>工程：怎么做得多、快、好、省？</p>
<p>应用：怎么用？</p>
<h2 id="机器学习"><a href="#机器学习" class="headerlink" title="机器学习"></a>机器学习</h2><p>经典定义：利用经验改善系统自身的性能（T. Mitchell, 1997）</p>
<p>经验在计算机系统中以数据形式存在，因对数据分析的需求，从而产生了智能数据分析理论和方法。“智能”前缀是为了与数学家、统计学家利用数理统计方法进行数据分析相区别，其强调人工智能学者利用计算机进行数据分析。</p>
<h2 id="典型的机器学习过程"><a href="#典型的机器学习过程" class="headerlink" title="典型的机器学习过程"></a>典型的机器学习过程</h2><p><img src="https://testingcf.jsdelivr.net/gh/WanpengXu/myPicGo/img/202210170022526.png" alt=""></p>
<p>在训练数据上，使用学习算法，经训练，得到模型，输入新数据，产生输出。</p>
<p>从数据中产生的东西，可以认为是一个神经网络，甚至是一条规则，都可以宽泛地统称为“模型”。</p>
<p>David Hume(?)：适用于全局的叫模型，适用于局部的叫模式。</p>
<h2 id="计算学习理论"><a href="#计算学习理论" class="headerlink" title="计算学习理论"></a>计算学习理论</h2><p>机器学习具有坚实的理论基础：Leslie Valiant建立的计算学习理论（Computational learning theory），其中最重要的模型是PAC（Probably Approximately Correct，概率近似正确）learning model（Leslie Valiant, 1984）。</p>
<p>其基础表达式：</p>
<script type="math/tex; mode=display">
P\left(\left\vert f(\boldsymbol{x}) - y \right\vert\leq \epsilon\right) \geq 1 - \delta</script><p>在机器学习中解释为：以很高的概率得到很好的模型。</p>
<p>Q：为什么要有$\epsilon$和$\delta$，而不是$P\left(\left\vert f(\boldsymbol{x}) - y \right\vert= 0\right)= 1$呢？</p>
<p>A：从人工智能基本概念的角度，对于确定性的知识，我们可以通过确定性推理方法，在多项式时间内得到确定性结果。机器学习往往处理那些带有不确定性的知识及它们的组合，因为知识带有不确定性，所以我们的结果一定带有可信度，因此需要引入$\epsilon$。</p>
<p>从计算的角度，机器学习往往处理$NP$问题，如果模型$f(\cdot)$每次都以$P=1$的概率预测出最佳答案$y$，即在多项式时间内解决掉了问题，那么就意味着我们构造性地证明了$P=NP$，因为$P=NP$尚未被证明，因此需要引入$\delta$。</p>
<h2 id="基本术语"><a href="#基本术语" class="headerlink" title="基本术语"></a>基本术语</h2><p><img src="https://testingcf.jsdelivr.net/gh/WanpengXu/myPicGo/img/202210171647083.png" alt=""></p>
<h3 id="输入部分"><a href="#输入部分" class="headerlink" title="输入部分"></a>输入部分</h3><p>数据集：dataset，如图左侧的表</p>
<p>训练：train，建立模型的过程</p>
<p>测试：test，向模型传入数据，根据数据的label判断模型的结果正确或错误</p>
<p>示例：instance，如（青绿，蜷缩，浊响）</p>
<p>样例：example，如（青绿，蜷缩，浊响，<strong>是</strong>）</p>
<p>样本：sample，有时指样例，如（青绿，蜷缩，浊响，<strong>是</strong>）；有时指数据集，如图左侧的表</p>
<p>属性/特征：attribute/feature，如色泽</p>
<p>属性值：attribute value，如青绿</p>
<p>属性空间/样本空间/输入空间：attribute space/sample space/input space，由属性所张成的空间，如轴为青绿、蜷缩、浊响的三维空间</p>
<p>特征向量：feature vector，示例在属性空间中所形成的向量</p>
<h3 id="模型部分"><a href="#模型部分" class="headerlink" title="模型部分"></a>模型部分</h3><p>假设：hypothesis，训练得到的模型，即PAC模型中的$f(\boldsymbol{x})$</p>
<p>真相：ground-truth，即PAC模型中的$y$</p>
<p>学习器：learner，在训练数据上使用学习算法经训练产生的东西，可以宽泛地认为是“模型”，如“分类器”</p>
<h3 id="输出部分"><a href="#输出部分" class="headerlink" title="输出部分"></a>输出部分</h3><p>分类：classification，输出是离散的</p>
<p>二分类：binary classification，可能输出的基数为2</p>
<p>多分类：multiclass classification，可能输出的基数大于等于2，可以分解为若干个二分类问题</p>
<p>回归：regression，输出是连续的</p>
<p>正类：positive class，对输出进行划分，我们需要的部分定义为正类，如“好瓜”中的是</p>
<p>负类：negative class，对输出进行划分，我们不需要的部分定义为负类，如“好瓜”中的否</p>
<h3 id="学习任务"><a href="#学习任务" class="headerlink" title="学习任务"></a>学习任务</h3><p>监督学习：supervised learning，样例中含有label的学习任务，主要任务：分类、回归</p>
<p>无监督学习：unsupervised learning，样例中不含label的学习任务，主要任务：聚类（离散）、密度估计（连续）</p>
<h3 id="基本假设"><a href="#基本假设" class="headerlink" title="基本假设"></a>基本假设</h3><p>未见样本：unseen instance，不在训练数据中的样本</p>
<p>未知分布：unknown distribution，所有的数据所服从的分布</p>
<p>独立同分布：i.i.d. (independent and identically distributed)，样本服从同一分布，并且互相独立。只有满足独立同分布才能应用统计学的基本原理（大数定律），用频率逼近概率。</p>
<p>泛化：generalization，对新数据处理的能力，即PAC模型中的$\epsilon$能做到多小，当$\epsilon &lt; 0.5$（比随机猜测准）时，可以努力去优化模型的性能。</p>
<h2 id="归纳偏好"><a href="#归纳偏好" class="headerlink" title="归纳偏好"></a>归纳偏好</h2><p>归纳偏好/偏置（Inductive Bias）：机器学习算法在学习过程中对某种类型假设的偏好。</p>
<p><img src="https://testingcf.jsdelivr.net/gh/WanpengXu/myPicGo/img/202210172332601.png" alt=""></p>
<p>如图，当两种模型（类型假设）<strong>都能解释训练数据时</strong>，哪种模型更好？此时需要由算法的归纳偏好决定。因此机器学习算法的归纳偏好是否与问题本身匹配，直接决定算法取得的性能。</p>
<p>奥卡姆剃刀（Occam’s razor）准则：一个典型的归纳偏好，其最流行的表述方式为：“若非必要，勿增实体”，另外还有一种表述为“若有多个假设与观察一致，则选最简单的那个”。</p>
<p>但因“简单”这个标准比较宽泛，所以如何确定哪个假设更“简单”有很多种不同角度的实现方式。</p>
<h2 id="NFL定理"><a href="#NFL定理" class="headerlink" title="NFL定理"></a>NFL定理</h2><p><img src="https://testingcf.jsdelivr.net/gh/WanpengXu/myPicGo/img/202210172351307.png" alt=""></p>
<p>NFL定理（No Free Lunch Theoren）：一个算法$\mathfrak{L}_a$若在某些问题（左图）上比另一个算法$\mathfrak{L}_b$好，必存在另一些问题（右图），$\mathfrak{L}_b$比$\mathfrak{L}_a$好。</p>
<p>NFL定理的重要前提：所有问题出现的机会相同，或所有问题同等重要。</p>
<p>因此，机器学习中没有“更好的算法”，只有“<strong>在这个问题上</strong>更好的算法”！</p>
<h1 id="模型评估与选择"><a href="#模型评估与选择" class="headerlink" title="模型评估与选择"></a>模型评估与选择</h1><h2 id="泛化能力"><a href="#泛化能力" class="headerlink" title="泛化能力"></a>泛化能力</h2><p>错误率低、精度高、召回率高，都可以作为好模型的标准，与NFL定理类似，没有“好标准”，只有“<strong>在这个问题上</strong>的好标准”</p>
<p>泛化能力（generalization ability）：模型在未见样本上表现<strong>好</strong>。</p>
<h2 id="过拟合和欠拟合"><a href="#过拟合和欠拟合" class="headerlink" title="过拟合和欠拟合"></a>过拟合和欠拟合</h2><p>经验误差/训练误差（empirical error/training error）：学习器在训练集上的误差</p>
<p>泛化误差（generalization error）：学习器在新样本上的误差</p>
<p>因为过拟合现象的存在，经验误差与泛化误差并不是越小越好。</p>
<p>过拟合（overfitting）：学习器把训练样本自身的一些特殊性质当做了所有样本都会有的一般性质。</p>
<p>欠拟合（underfitting）：学习器尚未学好训练样本自身的性质。</p>
<p><img src="https://testingcf.jsdelivr.net/gh/WanpengXu/myPicGo/img/202210181110973.png" alt="过拟合、欠拟合的直观类比"></p>
<p>所以，对于具体机器学习算法，明白了其用什么方法缓解overfitting，这种方法在什么时候失效，就明白了这个机器学习算法的应用场景。</p>
<h2 id="三大问题"><a href="#三大问题" class="headerlink" title="三大问题"></a>三大问题</h2><p>如何进行模型选择，有三个关键问题：</p>
<ul>
<li>如何获得测试结果？</li>
<li>如何评估性能优劣？</li>
<li>如何判断实质差别？</li>
</ul>
<p>解决这三个方面的问题，有一些经典的方法/标准，分别为：</p>
<ul>
<li>评估方法：对学习器的泛化误差进行量化</li>
<li>性能度量：衡量模型泛化能力的评价标准</li>
<li>比较检验：比较模型<strong>在统计意义上</strong>的表现</li>
</ul>
<p>三者的工作方式为：对于某个学习器，使用某种评估方法，测得某个性能度量结果，比较检验这些结果。</p>
<h2 id="评估方法"><a href="#评估方法" class="headerlink" title="评估方法"></a>评估方法</h2><p>注：这一节提到的“测试集”，用“验证集”表述更好。</p>
<p>Q：怎么获得测试集？测试集应与训练集“互斥”。</p>
<p>A：对数据集进行适当的处理，从中产生训练集和测试集。</p>
<p>常见方法：留出法（hold-out）、交叉验证法（cross validation）、自助法（bootstrap）</p>
<h3 id="留出法"><a href="#留出法" class="headerlink" title="留出法"></a>留出法</h3><p>直接将数据集划分为两个互斥的集合</p>
<p><img src="https://testingcf.jsdelivr.net/gh/WanpengXu/myPicGo/img/202210181633900.png" alt=""></p>
<p>注意：</p>
<ul>
<li>保持数据分布的一致性，如分布采样/分层采样（stratified sampling）</li>
<li>多次重复划分，如100次随机划分</li>
<li>测试集不能太大、不能太小（$\frac{1}{5}$~$\frac{1}{3}$）</li>
</ul>
<p>Q：如果有一些数据，每次重复划分都在训练集里，永远没有被用作测试怎么办？</p>
<p>A：使用k折交叉验证，可以保证所有数据都成为过测试数据。</p>
<h3 id="k折交叉验证"><a href="#k折交叉验证" class="headerlink" title="k折交叉验证"></a>k折交叉验证</h3><p>将数据集（共m个样本）划分为k个大小相似的互斥子集，每次用k-1个子集的并集作为训练集，余下的那个子集作为测试集。</p>
<p>留一法（leave-one-out, LOO）：k=m时的k折交叉验证，即每1个样本都被用作其余m-1个样本的测试数据一次。</p>
<p><img src="https://testingcf.jsdelivr.net/gh/WanpengXu/myPicGo/img/202210181649476.png" alt="10折交叉验证"></p>
<p>将m个样本划分为k个子集的方式有很多，为了减小因划分方式的不同而引入的差别，通常要重复p次不同的划分方式，最终的评估结果为“p次k折交叉验证”的均值，如“10次10折交叉验证”。</p>
<p>k折交叉验证中，将k-1个子集训练出的Model，视为用全部数据集训练出的Model进行评估，即$\widetilde{M}<em>k=M</em>{k-1}$。</p>
<p>Q：有没有什么方法，既可以直接得到$M_k$，又有剩余的数据进行测试呢？</p>
<p>A：自助法</p>
<h3 id="自助法"><a href="#自助法" class="headerlink" title="自助法"></a>自助法</h3><p>自助法以自助采样法（bootstrap sampling）为基础（Efron and Tibshirani,  1993），其内容为：</p>
<p>给定数据集D（共m个样本）和数据集$D^{\prime}$（初始共0个样本），重复m次以下操作：从D中随机选中1个样本，<strong>复制</strong>进$D^{\prime}$。</p>
<p><img src="https://testingcf.jsdelivr.net/gh/WanpengXu/myPicGo/img/202210181732638.png" alt=""></p>
<p>这样就得到了训练集：具有m个样本（可能重复）的数据集$D^{\prime}$，和测试集：未出现在$D^{\prime}$中的样本集合$D-D^{\prime}$。</p>
<p>Q：某个样本在m次操作中没有被选中的概率是多少？</p>
<p>A：</p>
<script type="math/tex; mode=display">
\lim_{m\rightarrow\infty}{\left(1-\frac{1}{m}\right)^{m}=\frac{1}{e}\approx0.368}</script><p>即每个样本都有36.8%的概率不被选为训练数据，那么根据数学期望可得训练集的大小，即$E(D-D^{\prime})=0.368*\left\vert D\right\vert$</p>
<p>使用这样的测试集完成的估计，也称作“包外估计”（out-of-bag estimation）</p>
<p>注意：</p>
<ul>
<li><strong>改变了数据分布</strong></li>
</ul>
<p>所以，当数据分布不那么重要，或数据集过小时，可以考虑自助法。</p>
<h2 id="调参与验证集"><a href="#调参与验证集" class="headerlink" title="调参与验证集"></a>调参与验证集</h2><p>算法的参数：一般由人工设定，亦称“超参数”，数目常在10以内。</p>
<p>模型的参数：一般由学习确定，数目可能极多，深度学习中可达上百亿。</p>
<p>对算法的参数进行调整，即“参数调节”，简称“调参”（parameter tuning）。</p>
<p>事实上，前文中提到的，从训练集中划分出的、用于评估模型的数据，常称为“验证集”（validation set）。而真正的测试集是指在实际使用中遇到的、真的没有标签的数据。</p>
<p>当通过评估确定算法的参数后，因评估过程中仅使用训练集训练模型，所以应该用这些确定的参数对全部数据（训练集+验证集，即D）重新训练出模型，提交给用户。</p>
<h2 id="性能度量"><a href="#性能度量" class="headerlink" title="性能度量"></a>性能度量</h2><p>性能度量（performance measure）：对学习器的泛化性能进行评估。</p>
<h3 id="错误率、精度"><a href="#错误率、精度" class="headerlink" title="错误率、精度"></a>错误率、精度</h3><p>回归任务中：</p>
<p>均方误差（mean squared error）：</p>
<script type="math/tex; mode=display">
E(f ; D)=\frac{1}{m} \sum_{i=1}^m\left(f\left(\boldsymbol{x}_i\right)-y_i\right)^2</script><p>给定数据分布$\mathcal{D}$和概率密度函数$p(\cdot)$的一般形式：</p>
<script type="math/tex; mode=display">
E(f ; \mathcal{D})=
\int_{x\sim \mathcal{D}}p(\boldsymbol{x})\left(f\left(\boldsymbol{x}\right)-y\right)^2d\boldsymbol{x}</script><p>分类任务中：</p>
<p>错误率：</p>
<script type="math/tex; mode=display">
E(f ; D)=\frac{1}{m} \sum_{i=1}^m \mathbb{I}\left(f\left(\boldsymbol{x}_i\right) \neq y_i\right)</script><p>正确率：</p>
<script type="math/tex; mode=display">
\begin{align*}
acc(f ; D) &= 1 - E(f ; D) \\
&= \frac{1}{m} \sum_{i=1}^m \mathbb{I}\left(f\left(\boldsymbol{x}_i\right) = y_i\right)
\end{align*}</script><p>给定数据分布$\mathcal{D}$和概率密度函数$p(\cdot)$的一般形式：</p>
<script type="math/tex; mode=display">
E(f ; \mathcal{D})=
\int_{x\sim \mathcal{D}}p(\boldsymbol{x})\mathbb{I}\left(f\left(\boldsymbol{x}\right) \neq y\right)d\boldsymbol{x}</script><script type="math/tex; mode=display">
\begin{align*}
acc(f ; D) &= 1 - E(f ; \mathcal{D})) \\
&= \int_{x\sim \mathcal{D}}p(\boldsymbol{x})\mathbb{I}\left(f\left(\boldsymbol{x}\right) = y\right)d\boldsymbol{x}
\end{align*}</script><h3 id="查准率、查全率"><a href="#查准率、查全率" class="headerlink" title="查准率、查全率"></a>查准率、查全率</h3><p><img src="https://testingcf.jsdelivr.net/gh/WanpengXu/myPicGo/img/202210182021438.png" alt="分类结果混淆矩阵"></p>
<p>TF：预测对了或错了</p>
<p>PN：预测结果</p>
<p>例如FN（假反例）：预测错了，给预测成反例了。</p>
<p>查准率（precision）：预测结果准确的比率，如“预测出的好瓜里有多少是真的好瓜”</p>
<script type="math/tex; mode=display">
P = \frac{TP}{TP + FP}</script><p>查全率（recall）：预测结果全面的比率，如“预测出的好瓜占所有好瓜的多少”</p>
<script type="math/tex; mode=display">
R = \frac{TP}{TP + FN}</script><h3 id="F1-、-F-beta"><a href="#F1-、-F-beta" class="headerlink" title="$F1$、$F_\beta$"></a>$F1$、$F_\beta$</h3><p>为了同时考虑查准率与查全率，同时又<strong>不忽视较小值</strong>，我们可以对二者进行调和平均，得到F1度量。</p>
<script type="math/tex; mode=display">
\begin{align*}
\frac{1}{F1} &= \frac{1}{2} \cdot \left(\frac{1}{P} + \frac{1}{R}\right) \\
&= \frac{\left\vert D \right\vert + TP - TN}{2 \times TP}
\end{align*}</script><p>即</p>
<script type="math/tex; mode=display">
\begin{align*}
F1 &= \frac{2 \times P \times R}{P + R}\\
&=\frac{2 \times TP}{\left\vert D \right\vert + TP - TN}
\end{align*}</script><p>如果对P和R有偏好，可以对二者进行<strong>加权</strong>调和平均，得到$F_\beta$度量。</p>
<script type="math/tex; mode=display">
\frac{1}{F_\beta}=\frac{1}{1+\beta^2} \cdot\left(\frac{1}{P}+\frac{\beta^2}{R}\right)</script><script type="math/tex; mode=display">
F_\beta=\frac{\left(1+\beta^2\right) \times P \times R}{\left(\beta^2 \times P\right)+R}</script><p>注. 这里的权重$\beta$加在R上，P的权重为1。所以当$\beta &lt; 1$时，P更重要；当$\beta &gt; 1$时，R更重要。</p>
<h2 id="比较检验"><a href="#比较检验" class="headerlink" title="比较检验"></a>比较检验</h2><p>学习器间的性能比较是一件复杂的事情，因为：</p>
<ul>
<li>测试性能不等于泛化性能（验证集不等于测试集）</li>
<li>测试性能随测试集变化</li>
<li>很多机器学习算法本身有一定的随机性</li>
</ul>
<p>我们可以使用统计假设检验（hypothesis test）<strong>在统计意义上</strong>比较学习器的泛化性能。</p>
<p>单个学习器假设检验：二项检验（binomial test）、t检验（t-test）</p>
<p>两个学习器假设检验：交叉验证t检验、McNemar检验</p>
<p>多个学习器假设检验：Friedman检验、Nemenyi后续检验</p>
<p><img src="https://testingcf.jsdelivr.net/gh/WanpengXu/myPicGo/img/202210182132310.png" alt=""></p>
<p>阈值上升，则预测为正类的样本（P）可能变少，预测为负类的样本（N）可能变多。因R的分子含TP，可能变小，分母恒为真实情况正例数，所以R下降或不变。根据P-R曲线（这不是个经验曲线吗？一定准确？），P上升。</p>
<h1 id="线性模型"><a href="#线性模型" class="headerlink" title="线性模型"></a>线性模型</h1><p>人类很难直接思考非线性问题，而在思考线性问题时可以有几何上的直观印象，基于这种直观印象，再加上一些技巧，便可能得到复杂的非线性问题的解决方案，所以线性模型很重要。</p>
<p>回归（regression）：该词源于生物学，意思是平均而言，子代的表现型比其父代更接近该物种的基因型应该表现出来的样子。所以回归就是根据样本的“表现型”确定其“基因型”。在统计学上，指确定变量之间的关系。</p>
<h2 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h2><p>线性回归（linear regression）：将变量间的关系建模为线性方程（线性模型）的回归分析。</p>
<p>线性回归在分类问题中，建模出一个线性方程，用于将不同样本“分开”。</p>
<p>线性回归在回归问题中，建模出一个线性方程，用于将样本“串起来”。</p>
<p><img src="https://testingcf.jsdelivr.net/gh/WanpengXu/myPicGo/img/202210191525198.png" alt=""></p>
<p>以上图为例，即确定</p>
<script type="math/tex; mode=display">
f(x_i)=w x_i + b\\
s.t.\quad  f(x_i) \simeq y_i</script><p>注. $\simeq$是<code>\simeq</code>，“similar equal”，近似相等。</p>
<p>更一般地，该线性模型的标量形式为：</p>
<script type="math/tex; mode=display">
\begin{aligned}
&f(\boldsymbol{x})=w_1 x_1+w_2 x_2+\ldots+w_d x_d+b \\
\end{aligned}</script><p>向量形式为：</p>
<script type="math/tex; mode=display">
f(\boldsymbol{x})=\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}+b</script><p>Q：若某一组输入的属性是离散的，如三值长度属性：{“长”，“中”，“短”}或三值敲声属性：{“浊响”，“清脆”，“沉闷”}，如何回归分析？</p>
<p>A：可以对输入属性进行处理，若属性值间存在“序”（order）关系，则可将其连续化，成为连续值，如长度属性可转化为${1.0, 0.5, 0.0}$。若属性值间不存在序关系，则可以将其one-hot编码，成为one-hot向量，如敲声属性可转化为${(1, 0, 0), (0, 1, 0), (0, 0, 1)}$</p>
<p>注. pandas中通过get_dummies（转哑变量）实现one-hot编码。</p>
<p>Q：线性回归怎么得到线性模型的呢？</p>
<p>A：通过使均方误差最小化，解出$w^<em>$和$b^</em>$</p>
<script type="math/tex; mode=display">
\begin{aligned}
\left(w^*, b^*\right) &=\underset{(w, b)}{\arg \min } \sum_{i=1}^m\left(f\left(x_i\right)-y_i\right)^2 \\
&=\underset{(w, b)}{\arg \min } \sum_{i=1}^m\left(y_i-w x_i-b\right)^2
\end{aligned}</script><p>注. </p>
<p>$\left(x_1, x_2, …, x_n\right)$代表行向量</p>
<script type="math/tex; mode=display">
\left[x_1, x_2, ..., x_n\right]</script><p>$\left(x_1; x_2; …; x_n\right)$代表列向量</p>
<script type="math/tex; mode=display">
\begin{bmatrix}
x_1 \\
x_2 \\
\vdots \\
x_n
\end{bmatrix}</script><h2 id="最小二乘解"><a href="#最小二乘解" class="headerlink" title="最小二乘解"></a>最小二乘解</h2><p>上述通过使均方误差最小化得到的解，也叫最小二乘/最小平方（least square method）解。</p>
<p>最小二乘解是一种解析解/闭式解（Closed-form expression），其解法如下：</p>
<p>分别对$w$和$b$求导</p>
<script type="math/tex; mode=display">
\begin{aligned}
&\frac{\partial E_{(w, b)}}{\partial w}=2\left(w \sum_{i=1}^m x_i^2-\sum_{i=1}^m\left(y_i-b\right) x_i\right) \\
&\frac{\partial E_{(w, b)}}{\partial b}=2\left(m b-\sum_{i=1}^m\left(y_i-w x_i\right)\right)
\end{aligned}</script><p>令导数为 0, 得到解析解:</p>
<script type="math/tex; mode=display">
\begin{aligned}
w&=\frac{\sum\limits_{i=1}^m y_i\left(x_i-\bar{x}\right)}{\sum\limits_{i=1}^m x_i^2-\frac{1}{m}\left(\sum\limits_{i=1}^m x_i\right)^2} \\
b&=\frac{1}{m} \sum_{i=1}^m\left(y_i-w x_i\right)
\end{aligned}</script><p>因为<script type="math/tex">E_{(w, b)}</script>是一个关于$w$和$b$的凸函数，所以其关于$w$和$b$的导数为0时，解得的就是最优解<script type="math/tex">\left(w^*, b^*\right)</script>，对应全局极小点<script type="math/tex">\left(w^*, b^*; E_{min}\right)</script>。</p>
<h2 id="多元线性回归"><a href="#多元线性回归" class="headerlink" title="多元线性回归"></a>多元线性回归</h2><p>如第一节中所述，线性回归的一般形式是</p>
<script type="math/tex; mode=display">
f(\boldsymbol{x_i})=\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x_i} + b\\
s.t.\quad  f(\boldsymbol{x_i}) \simeq y_i</script><p>其中$\boldsymbol{x<em>i}=\left(x</em>{i1}; x<em>{i2}; …; x</em>{id}\right)$，样本共有$d$个属性。</p>
<p>这便是“多元线性回归”（multivariate linear regression）。</p>
<p>类似地，我们仍可以利用最小二乘法得到其最优参数。但为了方便解，我们首先对一般形式进行调整。</p>
<p>令</p>
<script type="math/tex; mode=display">
\cases{
\begin{align*}
    \hat{\boldsymbol{w}} &= (\boldsymbol{w}; b) \\
    \hat{\boldsymbol{x}}_i &= (\boldsymbol{x_i}; 1)
\end{align*}
}</script><p>即</p>
<script type="math/tex; mode=display">
\hat{\boldsymbol{w}}^{\mathrm{T}} \hat{\boldsymbol{x}}_i = \boldsymbol{w}^{\mathrm{T}} \boldsymbol{x_i} + b</script><p>那么，数据集即为</p>
<script type="math/tex; mode=display">
\mathbf{X}=\left(
\begin{array}{ccccc}
x_{11} & x_{12} & \cdots & x_{1 d} & 1 \\
x_{21} & x_{22} & \cdots & x_{2 d} & 1 \\
\vdots & \vdots & \ddots & \vdots & \vdots \\
x_{m 1} & x_{m 2} & \cdots & x_{m d} & 1
\end{array}
\right)
=\left(
\begin{array}{cc}
\boldsymbol{x}_1^{\mathrm{T}} & 1 \\
\boldsymbol{x}_2^{\mathrm{T}} & 1 \\
\vdots & \vdots \\
\boldsymbol{x}_m^{\mathrm{T}} & 1
\end{array}\right)
\quad
\boldsymbol{y}=\begin{pmatrix}
y_1 \\
y_2 \\
\vdots \\
y_m
\end{pmatrix}</script><p>类似地，令均方误差最小</p>
<script type="math/tex; mode=display">
\begin{aligned}
&\hat{\boldsymbol{w}}^*=\underset{\hat{\boldsymbol{w}}}{\arg \min }(\boldsymbol{y}-\mathbf{X} \hat{\boldsymbol{w}})^{\mathrm{T}}(\boldsymbol{y}-\mathbf{X} \hat{\boldsymbol{w}})\\
\end{aligned}</script><p>注. 需要从一般形式推导至向量形式</p>
<p>设</p>
<script type="math/tex; mode=display">
E_{\hat{\boldsymbol{w}}}=(\boldsymbol{y}-\mathbf{X} \hat{\boldsymbol{w}})^{\mathrm{T}}(\boldsymbol{y}-\mathbf{X} \hat{\boldsymbol{w}})</script><p> 对$\hat{\boldsymbol{w}} $求导，得</p>
<script type="math/tex; mode=display">
\frac{\partial E_{\hat{\boldsymbol{w}}}}{\partial \hat{\boldsymbol{w}}}=2 \mathbf{X}^{\mathrm{T}}(\mathbf{X} \hat{\boldsymbol{w}}-\boldsymbol{y})</script><p>令其为零即可得$\hat{\boldsymbol{w}}$</p>
<p>但，若想解出$\hat{\boldsymbol{w}}$，需要对$\mathbf{X}$求逆！</p>
<p>所以，若：</p>
<ul>
<li>$\mathbf{X}^{\mathrm{T}}\mathbf{X}$为满秩矩阵或正定矩阵，则可以解得唯一$\hat{\boldsymbol{w}}^* = \left(\mathbf{X}^{\mathrm{T}}\mathbf{X}\right)^{-1}\mathbf{X}^{\mathrm{T}}\boldsymbol{y}$</li>
<li>$\mathbf{X}^{\mathrm{T}}\mathbf{X}$不为满秩矩阵，则$\hat{\boldsymbol{w}}$不唯一，此时由算法的归纳偏好选择一个解</li>
</ul>
<h2 id="广义线性模型"><a href="#广义线性模型" class="headerlink" title="广义线性模型"></a>广义线性模型</h2><p>在$f(\boldsymbol{x})$预测出$y$后，我们还可以预测$y$的“衍生物”（以$y$为自变量的其他函数），即</p>
<script type="math/tex; mode=display">
g(y) = \boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}+b</script><p>其中$g(\cdot)$称为“联系函数”（link function），其单调可微、连续且充分光滑。</p>
<p>其等价于</p>
<script type="math/tex; mode=display">
y = g^{-1}(\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}+b)</script><p>即我们用线性模型，逼近/预测了标记的“衍生物”，这样得到的模型称为“广义线性模型”（generalized linear model）。</p>
<p>当$g(\cdot) = ln(\cdot)$时，即得到“对数线性模型”（log-linear model）</p>
<script type="math/tex; mode=display">
ln(y) = \boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}+b</script><script type="math/tex; mode=display">
y = \exp(\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}+b)</script><p>首先预处理标记，然后以线性回归求解线性模型的参数，最后得到标记真实的模型——指数模型。</p>
<p><img src="https://testingcf.jsdelivr.net/gh/WanpengXu/myPicGo/img/202210192108509.png" alt=""></p>
<p>注. 广义线性模型的参数估计常通过加权最小二乘法或极大似然法进行。</p>
<h2 id="对数几率回归"><a href="#对数几率回归" class="headerlink" title="对数几率回归"></a>对数几率回归</h2><p>使用线性模型和广义线性模型进行回归分析，可以解决回归任务。</p>
<p>那么对于分类任务，事实上是在更进一步地，将线性模型的连续预测值$z$映射到离散的真实标记$y$。</p>
<p>以二分类任务为例，设其输出标记$y \in \left{0, 1\right}$，其线性回归模型及其预测值$z=\boldsymbol{w}^{\mathrm{T}}\boldsymbol{x}+b$。</p>
<p>那么为了将$z$映射到$y$，有一个最理想的函数“单位阶跃函数”（unit-step function）</p>
<script type="math/tex; mode=display">
y=\left\{
\begin{array}{cc}
0, & z<0 \\
0.5, & z=0 \\
1, & z>0
\end{array}
\right.</script><p>其在预测值大于零时判为正例，小于零时判为负例，等于零（临界值）时任意判别，取两标记0和1的平均值0.5。</p>
<p>但单位阶跃函数性质很差，不连续且不可微，所以需要寻找能在一定程度上近似它的、性质好的“替代函数”（surrogate function）。</p>
<p>我们常用“对数几率函数”（logistic function）替代单位阶跃函数，其表达式为：</p>
<script type="math/tex; mode=display">
y = \frac{1}{1 + \exp{\left(-z\right)}}</script><p>二者的函数图像为</p>
<p><img src="https://testingcf.jsdelivr.net/gh/WanpengXu/myPicGo/img/202210201222973.png" alt=""></p>
<p>对数几率（对率）函数是一种代表性的Sigmoid函数（形似“S”的函数），其单调可微、任意阶可导，并且能将$z$映射到<strong>接近0或1</strong>的$y$，且在$z=0$附近很“陡”。</p>
<p>注. “对数几率”logistic一词源自统计学造词术语logit，其原本的单词为log odds，odds是“事件发生的概率与事件不发生的概率之比”，可译作“几率”或“发生比”。</p>
<p>将线性模型代入对数几率函数（将对率函数视作$y=g^{-1}(\cdot)$），得到广义线性模型$y=g^{-1}(\boldsymbol{w}^{\mathrm{T}}\boldsymbol{x}+b)$</p>
<script type="math/tex; mode=display">
y = \frac{1}{1 + \exp{\left(-\left(\boldsymbol{w}^{\mathrm{T}}\boldsymbol{x}+b\right)\right)}}</script><p>将其转化为线性回归的形式，即</p>
<script type="math/tex; mode=display">
\ln\frac{y}{1-y}=\boldsymbol{w}^{\mathrm{T}}\boldsymbol{x}+b</script><p>$y$是学习器将样本视作正例的可能性，$1-y$是学习器将样本视作负例的可能性，所以$\frac{y}{1-y}$便是odds，$\ln\frac{y}{1-y}$便是log odds，即logit。所以该模型称作“对数几率模型”。</p>
<p>虽然对数几率模型是用线性回归方法确定的，也被叫做对数几率回归（logistic regression），但其实际上用于分类任务。</p>
<p>注. 统计回归方法只用于确定一个模型，模型适用于分类任务还是回归任务，是由模型自身决定的。</p>
<p>优点：</p>
<ul>
<li>无需事先假设数据分布（直接对原始的“类别”可能性建模，即将$\mathbb{R}$压缩到(0, 1)）</li>
<li>可得到“类别”的<strong>近似概率</strong>预测</li>
<li>可直接应用现有数值优化方法求取最优解（模型/目标函数<strong>的对数似然函数</strong>是任意阶可导的<strong>凸函数</strong>）</li>
</ul>
<h2 id="对率回归求解-极大似然解？"><a href="#对率回归求解-极大似然解？" class="headerlink" title="对率回归求解/极大似然解？"></a>对率回归求解/极大似然解？</h2><p>Note. $P(A\mid B)$, is usually read as “the conditional probability of <em>A</em> <strong>given</strong> <em>B</em>“.</p>
<p>事实上，$y = \frac{1}{1 + \exp{\left(-\left(\boldsymbol{w}^{\mathrm{T}}\boldsymbol{x}+b\right)\right)}}$是非凸的，所以不能用最小二乘法确定模型。</p>

        </div>

        
            <section class="post-copyright">
                
                    <p class="copyright-item">
                        <span>Author:</span>
                        <span>许</span>
                    </p>
                
                
                    <p class="copyright-item">
                        <span>Permalink:</span>
                        <span><a href="http://xuwp.top/Zhihua-Zhou-Machine-Learning-Prelim.html">http://xuwp.top/Zhihua-Zhou-Machine-Learning-Prelim.html</a></span>
                    </p>
                
                
                    <p class="copyright-item">
                        <span>License:</span>
                        <span>Copyright (c) 2022 <a target="_blank" rel="noopener" href="http://creativecommons.org/licenses/by-nc/4.0/">CC-BY-NC-4.0</a> LICENSE</span>
                    </p>
                
                

            </section>
        
        <section class="post-tags">
            <div>
                <span>Tag(s):</span>
                <span class="tag">
                    
                    
                        <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"># 机器学习</a>
                    
                        
                </span>
            </div>
            <div>
                <a href="javascript:window.history.back();">back</a>
                <span>· </span>
                <a href="/">home</a>
            </div>
        </section>
        <section class="post-nav">
            
            
            <a class="next" rel="next" href="/Dive-into-Deep-Learning.html">Mu Li. Dive into Deep Learning</a>
            
        </section>


    </article>
</div>

            </div>
            <footer id="footer" class="footer">
    <div class="copyright">
        <span>Stay hungry. Stay foolish.</span>
    </div>
</footer>

    </div>
</body>

</html>